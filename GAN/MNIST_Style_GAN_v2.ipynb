{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST Style GAN v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "192FrUAFYlZpLMqHznPAwZVj8Tedv10wc",
      "authorship_tag": "ABX9TyPB6QCONHbWXKiN9RV+ULTY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nk555/AI-Projects/blob/master/GAN/MNIST_Style_GAN_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ35YNJNGSQ0"
      },
      "source": [
        "# MNIST learns your handwriting\n",
        "\n",
        "This is a small project on using a GAN to generate numbers that look as someone else's handwriting when not trained on all numbers written by this person. For example say we had someone write the number 273 and we now want to write 481 in their own handwriting.\n",
        "\n",
        "The main inspiration for this project is a paper I read recently called STAR GAN v2. In this paper they try to recognize diferent styles and features in images and transfer those into a different image. For example they are able to use image of different animals like dogs or tigers and making them look like a cat. Furthermore at the time of writing this it is currently a state-of-the-art method for this style translation tasks.\n",
        "\n",
        "Some of the results can be seen at the end of this notebook. Unfortunately it seems not that many features were captured and mostly it was only the thickness of the numbers that was preserved. A reason this happens might be that the size of the images is small being 28x28. However, some ways to allow for more variation might be by exteding the number of layers being used, by having higher dimensional spaces for the latent and style spaces, or by giving a higher weight to the style diversification loss (look at section loss functions to see more about this).\n",
        "\n",
        "The main purpose of this notebook is to make a small showcase of the architecture used in a simple design so that the ideas are simple to follow. This notebook will also contain some explanations and comments on the architecture of the neural network so that it might be easier to follow.\n",
        "\n",
        "Note: another small thing I did in this project is to 'translate' STAR GAN code from pytorch to tensorflow. Redoing all of the work was useful to understand everything done on their code and having an option in tensorflow might be useful for some people.\n",
        "\n",
        "For a small tutorial on how to write a simple GAN architecture: https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/\n",
        "\n",
        "Link to STAR GAN v2: https://app.wandb.ai/stacey/stargan/reports/Cute-Animals-and-Post-Modern-Style-Transfer%3A-StarGAN-v2-for-Multi-Domain-Image-Synthesis---VmlldzoxNzcwODQ\n",
        "\n",
        "Further Reading on style domain techniques for image generation:\n",
        "\n",
        "Link to STAR GAN paper: https://arxiv.org/pdf/1912.01865.pdf\n",
        "\n",
        "Link to Multimodal Unsupervised Image-to-Image Translation: https://arxiv.org/pdf/1804.04732.pdf\n",
        "\n",
        "Link to Improving Style-Content Disentanglement Paper: https://arxiv.org/pdf/2007.04964.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIuW2ka_GmY7"
      },
      "source": [
        "# Intitializing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JufRLrnvGles"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow_addons.layers import InstanceNormalization\n",
        "import numpy as np\n",
        "import tensorflow.keras.layers as layers\n",
        "import time\n",
        "import torchvision.utils as vutils\n",
        "from keras.datasets.mnist import load_data\n",
        "import sys\n",
        "import os\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVkpYJ2JMMQS"
      },
      "source": [
        "# Layers\n",
        "\n",
        "There are a few layers that were custom made. More importantly it is udeful to make this custom layers for the layers that try to incorporate style. This is as the inputs themselves are custom as you are inputing an image and a vector representing the style.\n",
        "\n",
        "ResBlk is short for Residual Block, where it is predicting the residual (the difference between the original and the prediction)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOEAadTNO8zL"
      },
      "source": [
        "class ResBlk(tf.keras.Model):\n",
        "  def __init__(self, dim_in, dim_out, actv=layers.LeakyReLU(),\n",
        "                 normalize=False, downsample=False):\n",
        "    super(ResBlk, self).__init__()\n",
        "    self.actv = actv\n",
        "    self.normalize = normalize\n",
        "    self.downsample = downsample\n",
        "    self.learned_sc = dim_in != dim_out\n",
        "    self._build_weights(dim_in, dim_out)\n",
        "\n",
        "  def _build_weights(self, dim_in, dim_out):\n",
        "    self.conv1 = layers.Conv2D(dim_in, 3, padding='same')\n",
        "    self.conv2 = layers.Conv2D(dim_out, 3, padding='same')\n",
        "    if self.normalize:\n",
        "      self.norm1 = InstanceNormalization()\n",
        "      self.norm2 = InstanceNormalization()\n",
        "    if self.learned_sc:\n",
        "      self.conv1x1 = layers.Conv2D(dim_out, 1)\n",
        "\n",
        "  def _shortcut(self, x):\n",
        "    if self.learned_sc:\n",
        "        x = self.conv1x1(x)\n",
        "    if self.downsample:\n",
        "        x = layers.AveragePooling2D(pool_size=(2,2), padding='same')(x)\n",
        "    return x\n",
        "  \n",
        "  def _residual(self, x):\n",
        "    if len(tf.shape(x))>4:\n",
        "      x=tf.reshape(x,tf.shape(x)[1:])\n",
        "    if self.normalize:\n",
        "      x = self.norm1(x)\n",
        "    x = self.actv(x)\n",
        "    x = self.conv1(x)\n",
        "    if self.downsample:\n",
        "      x = layers.AveragePooling2D(pool_size=(2,2), padding='same')(x)\n",
        "    if self.normalize:\n",
        "      x = self.norm2(x)\n",
        "    x = self.actv(x)\n",
        "    x = self.conv2(x)\n",
        "    return x\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self._shortcut(x) + self._residual(x)\n",
        "    return x / 2**(1/2) # unit variance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKp6hfiJnQqe"
      },
      "source": [
        "AdaIN stands for Adaptive Instance Normalization. It is a type of normalization that allows to 'mix' two inputs. In this case we use the style vector to mix with our input x which is the image or part of the process of constructing this image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RONGU0PsMLWS"
      },
      "source": [
        "class AdaIn(tf.keras.Model):\n",
        "  def __init__(self, style_dim, num_features):\n",
        "    super(AdaIn,self).__init__()\n",
        "    self.norm = InstanceNormalization()\n",
        "    self.lin = layers.Dense(num_features*2)\n",
        "\n",
        "  def call(self, x, s):\n",
        "    h=self.lin(s)\n",
        "    h=tf.reshape(h, [1, tf.shape(h)[0], 1, tf.shape(h)[1]])\n",
        "    gamma,beta=tf.split(h, 2, axis=3)\n",
        "    return (1+gamma)*self.norm(x)+beta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRsH0NEsbzwV"
      },
      "source": [
        "class AdainResBlk(tf.keras.Model):\n",
        "  def __init__(self, dim_in, dim_out, style_dim=16,\n",
        "                 actv=layers.LeakyReLU(), upsample=False):\n",
        "    super(AdainResBlk, self).__init__()\n",
        "    self.actv = actv\n",
        "    self.upsample = upsample\n",
        "    self.learned_sc = dim_in != dim_out\n",
        "    self._build_weights(dim_in, dim_out, style_dim)\n",
        "\n",
        "  def _build_weights(self, dim_in, dim_out, style_dim=16):\n",
        "    self.conv1 = layers.Conv2D(dim_out, 3, padding='same')\n",
        "    self.conv2 = layers.Conv2D(dim_out, 3, padding='same')\n",
        "    self.norm1 = AdaIn(style_dim, dim_in)\n",
        "    self.norm2 = AdaIn(style_dim, dim_out)\n",
        "    if self.learned_sc:\n",
        "      self.conv1x1 = layers.Conv2D(dim_out, 1)\n",
        "\n",
        "  def _shortcut(self, x):\n",
        "    if self.upsample:\n",
        "        x = layers.UpSampling2D(size=(2,2), interpolation='nearest')(x)\n",
        "    if self.learned_sc:\n",
        "        x = self.conv1x1(x)\n",
        "    return x\n",
        "\n",
        "  def _residual(self, x, s):\n",
        "    x = self.norm1(x, s)\n",
        "    x = self.actv(x)\n",
        "    if self.upsample:\n",
        "      x = layers.UpSampling2D(size=(2,2), interpolation='nearest')(x)\n",
        "    x = self.conv1(x)\n",
        "    x = self.norm2(x, s)\n",
        "    x = self.actv(x)\n",
        "    x = self.conv2(x)\n",
        "    return x\n",
        "\n",
        "  def call(self, x, s):\n",
        "    x = self._shortcut(x) + self._residual(x,s)\n",
        "    return x / 2**(1/2) # unit variance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZO-FgSqG82j"
      },
      "source": [
        "# Generator Class\n",
        "\n",
        "In the generator we have two steps one for encoding the image into lower level information and one to decode back to the image. In this particular architecture the decoding uses the style to build back the image as it is an important part of the process. The decoding does not do this as we have the style encoder as an architecture that deals with this issue of generating a style vector for a particular image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-o2ysprG_uF"
      },
      "source": [
        "class Generator(tf.keras.Model):\n",
        "  def __init__(self, img_size=28, style_dim=24, dim_in=8, max_conv_dim=128, repeat_num=2):\n",
        "    super(Generator, self).__init__()\n",
        "    self.img_size=img_size\n",
        "    self.from_bw=layers.Conv2D(dim_in, 3, padding='same', input_shape=(1,img_size,img_size,1))\n",
        "    self.encode=[]\n",
        "    self.decode=[]\n",
        "    self.to_bw=tf.keras.Sequential([InstanceNormalization(), layers.LeakyReLU(), layers.Conv2D(1, 1, padding='same')])\n",
        "\n",
        "    for _ in range(repeat_num):\n",
        "      dim_out = min(dim_in*2, max_conv_dim)\n",
        "      self.encode.append(ResBlk(dim_in, dim_out, normalize=True, downsample=True))\n",
        "      self.decode.insert(0, AdainResBlk(dim_out, dim_in, style_dim, upsample=True))\n",
        "      dim_in = dim_out\n",
        "\n",
        "    # bottleneck blocks\n",
        "    for _ in range(2):\n",
        "      self.encode.append(ResBlk(dim_out, dim_out, normalize=True))\n",
        "      self.decode.insert(0, AdainResBlk(dim_out, dim_out, style_dim))\n",
        "\n",
        "  def call(self, x, s):\n",
        "    x = self.from_bw(x)\n",
        "    cache = {}\n",
        "    for block in self.encode:\n",
        "      x = block(x)\n",
        "    for block in self.decode:\n",
        "      x = block(x, s)\n",
        "    return self.to_bw(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo5w30M3xZId"
      },
      "source": [
        "# Mapping Network\n",
        "\n",
        "The Mapping Network and the Style encoder are the parts of this architecture that make a difference in allowing style to be analyzed and put into our images. The mapping network will take as an input a latent code (represents images as a vector in a high dimensional space) and the domain in this case the domain is the number we are representing. And the style encoder will take as inputs an image and a domain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_jdQfzHxcwE"
      },
      "source": [
        "class MappingNetwork(tf.keras.Model):\n",
        "  def __init__(self, latent_dim=16, style_dim=24, num_domains=10):\n",
        "    super(MappingNetwork,self).__init__()\n",
        "    map_layers = [layers.Dense(128)]\n",
        "    map_layers += [layers.ReLU()]\n",
        "    for _ in range(2):\n",
        "      map_layers += [layers.Dense(128)]\n",
        "      map_layers += [layers.ReLU()]\n",
        "    self.shared = tf.keras.Sequential(layers=map_layers)\n",
        "\n",
        "    self.unshared = []\n",
        "    for _ in range(num_domains):\n",
        "      self.unshared += [tf.keras.Sequential(layers=[layers.Dense(128),\n",
        "                                            layers.ReLU(),\n",
        "                                            layers.Dense(128),\n",
        "                                            layers.ReLU(),\n",
        "                                            layers.Dense(128),\n",
        "                                            layers.ReLU(),\n",
        "                                            layers.Dense(style_dim)])]\n",
        "\n",
        "  def call(self, z, y):\n",
        "      h = self.shared(z)\n",
        "      out = []\n",
        "      for layer in self.unshared:\n",
        "          out += [layer(h)]\n",
        "      out = tf.stack(out, axis=1)  # (batch, num_domains, style_dim)\n",
        "      s = tf.gather(out, y, axis=1)  # (batch, style_dim)\n",
        "      return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-qp6RU5xPaP"
      },
      "source": [
        "# Style Encoder\n",
        "\n",
        "An important thing to notice from the style encoder is that it takes as an input an image and outputs a style vector. Looking at the dimensions of these we notice we need to flatten out the image through the layers. This can usually be done in two ways. By flattening a 2 dimensional input to a 1 dimensional output a flatten layer, or as it was done hear by using enough pooling layers so that we downsample the size of our 2 dimensional input until it is one dimensional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7CrR0wNxVqZ"
      },
      "source": [
        "class StyleEncoder(tf.keras.Model):\n",
        "  def __init__(self, img_size=28, style_dim=24, dim_in=16, num_domains=10, max_conv_dim=128, repeat_num=5):\n",
        "    super(StyleEncoder,self).__init__()\n",
        "    blocks = [layers.Conv2D(dim_in, 3, padding='same')]\n",
        "\n",
        "    for _ in range(repeat_num):           #repetition 1 sends to (b,14,14,d) 2 to (b,7,7,d) 3 to (b,4,4,d) 4 to (b,2,2,d) 5 to (b,1,1,d)\n",
        "      dim_out = min(dim_in*2, max_conv_dim)\n",
        "      blocks += [ResBlk(dim_in, dim_out, downsample=True)]\n",
        "      dim_in = dim_out\n",
        "\n",
        "    blocks += [layers.LeakyReLU()]\n",
        "    blocks += [layers.Conv2D(dim_out, 4, padding='same')]\n",
        "    blocks += [layers.LeakyReLU()]\n",
        "    self.shared = tf.keras.Sequential(layers=blocks)\n",
        "\n",
        "    self.unshared = []\n",
        "    for _ in range(num_domains):\n",
        "      self.unshared += [layers.Dense(style_dim)]\n",
        "\n",
        "  def call(self, x, y):\n",
        "    h = self.shared(x)\n",
        "    h = tf.reshape(h,[tf.shape(h)[0], tf.shape(h)[3]])\n",
        "    out = []\n",
        "    for layer in self.unshared:\n",
        "      out += [layer(h)]\n",
        "    out = tf.stack(out, axis=1)  # (batch, num_domains, style_dim)\n",
        "    s = tf.gather(out, y, axis=1)  # (batch, style_dim)\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bFCyoa6v5Bu"
      },
      "source": [
        "# Discriminator Class\n",
        "\n",
        "Similarly to the Style encoder the input of the discriminator is an image and we need to downsample it until it is one dimensional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7auvm08v7xw"
      },
      "source": [
        "class Discriminator(tf.keras.Model):\n",
        "  def __init__(self, img_size=28, dim_in=16, num_domains=10, max_conv_dim=128, repeat_num=5):\n",
        "    super(Discriminator, self).__init__()\n",
        "    blocks = [layers.Conv2D(dim_in, 3, padding='same')]\n",
        "\n",
        "    for _ in range(repeat_num):       #repetition 1 sends to (b,14,14,d) 2 to (b,7,7,d) 3 to (b,4,4,d) 4 to (b,2,2,d) 5 to (b,1,1,d)\n",
        "      dim_out = min(dim_in*2, max_conv_dim)\n",
        "      blocks += [ResBlk(dim_in, dim_out, downsample=True)]\n",
        "      dim_in = dim_out\n",
        "\n",
        "    blocks += [layers.LeakyReLU()]\n",
        "    blocks += [layers.Conv2D(dim_out, 4, padding='same')]\n",
        "    blocks += [layers.LeakyReLU()]\n",
        "    blocks += [layers.Conv2D(num_domains, 1, padding='same')]\n",
        "    self.main = tf.keras.Sequential(layers=blocks)\n",
        "\n",
        "\n",
        "  def call(self, x, y):\n",
        "    out = self.main(x)\n",
        "    out = tf.reshape(out, (tf.shape(out)[0], tf.shape(out)[3]))  # (batch, num_domains)\n",
        "    out = tf.gather(out, y, axis=1)  # (batch)\n",
        "    out = tf.reshape(out, [1])\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSCfJ2rOhRTA"
      },
      "source": [
        "# Loss Functions\n",
        "\n",
        "The loss functions used are an important part of this model as it describes our goal when training and how to perform gradient descent. The discriminator loss function is the regular adversarial loss L_adv used in a GAN architecture. But furthermore we have three loss functions added.\n",
        "\n",
        "For this loss functions if you want to see the mathematical formula I recommend looking at STAR GAN 2's paper. However I will explain what the loss tries to measure and a quick description of how it does so.\n",
        "\n",
        "L_sty is a style reconstruction loss. This tries to capture how well the style was captured on our output. It is computed as an expected value of the distance between the target style vector and the style vector that our style encoder predicts for the generated image.\n",
        "\n",
        "L_ds is a style diversification loss. It tries to capture that the images produced are different to promote a variety of images produced. It is computed as the expected value of the distance between the images (l_1 norm) generated when using two different styles and the same sources. \n",
        "\n",
        "L_cyc is a characteristic preserving loss. The cyc comes from cyclic as we measusre the distance between the original image and the image generated by using an image generated by this image and the style our style encoder provides as an input. (Notice we use the image generated by the image generated, so that we use the generator two times.)\n",
        "\n",
        "In the end the total loss function is expressed as\n",
        "\n",
        "L_adv + lambda_sty * L_sty + lambda_ds * L_ds + lambda_cyc * L_cyc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htzhU_8chtcd"
      },
      "source": [
        "def moving_average(model, model_test, beta=0.999):\n",
        "    for i in range(len(model.weights)):\n",
        "      model_test.weights[i] = (1-beta)*model.weights[i] + beta*model_test.weights[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il6WmjVLh1mB"
      },
      "source": [
        "def adv_loss(logits, target):\n",
        "    assert target in [1, 0]\n",
        "    targets = tf.fill(tf.shape(logits), target)\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(targets, logits)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hX4lAX7h2ZD"
      },
      "source": [
        "def r1_reg(d_out, x_in, g):\n",
        "    # zero-centered gradient penalty for real images\n",
        "    batch_size = tf.shape(x_in)[0]\n",
        "    grad_dout=g.gradient(d_out, x_in)\n",
        "    #grad_dout = tf.gradients(ys=d_out, xs=x_in)\n",
        "    grad_dout2 = tf.square(grad_dout)\n",
        "    grad_dout2 = tf.reshape(grad_dout2,[batch_size, tf.shape(grad_dout2)[1]*tf.shape(grad_dout2)[2]])\n",
        "    reg = 0.5 * tf.math.reduce_mean(tf.math.reduce_sum(grad_dout2, axis=1))\n",
        "    return reg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOzUelojhUWL"
      },
      "source": [
        "def compute_d_loss(nets, args, x_real, y_org, y_trg, z_trg=None, x_ref=None):\n",
        "  assert (z_trg is None) != (x_ref is None)\n",
        "  # with real images\n",
        "  with tf.GradientTape() as g:\n",
        "    g.watch(x_real)\n",
        "    out = nets['discriminator'](x_real, y_org)\n",
        "    loss_real = adv_loss(out, 1)\n",
        "    loss_reg = r1_reg(out, x_real, g)\n",
        "\n",
        "    # with fake images\n",
        "  if z_trg is not None:\n",
        "    s_trg = nets['mapping_network'](z_trg, y_trg)\n",
        "  else:  # x_ref is not None\n",
        "    s_trg = nets['style_encoder'](x_ref, y_trg)\n",
        "\n",
        "  x_fake = nets['generator'](x_real, s_trg)\n",
        "  out = nets['discriminator'](x_fake, y_trg)\n",
        "  loss_fake = adv_loss(out, 0)\n",
        "\n",
        "  loss = loss_real + loss_fake + args['lambda_reg'] * loss_reg\n",
        "  return loss, {'real': loss_real, 'fake':loss_fake, 'reg':loss_reg}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpK3H_v0h97Q"
      },
      "source": [
        "def compute_g_loss(nets, args, x_real, y_org, y_trg, z_trgs=None, x_refs=None):\n",
        "  assert (z_trgs is None) != (x_refs is None)\n",
        "  if z_trgs is not None:\n",
        "    z_trg, z_trg2 = z_trgs\n",
        "  if x_refs is not None:\n",
        "    x_ref, x_ref2 = x_refs\n",
        "\n",
        "  # adversarial loss\n",
        "  if z_trgs is not None:\n",
        "    s_trg = nets['mapping_network'](z_trg, y_trg)\n",
        "  else:\n",
        "    s_trg = nets['style_encoder'](x_ref, y_trg)\n",
        "\n",
        "  x_fake = nets['generator'](x_real, s_trg)\n",
        "  out = nets['discriminator'](x_fake, y_trg)\n",
        "  loss_adv = adv_loss(out, 1)\n",
        "\n",
        "  # style reconstruction loss\n",
        "  s_pred = nets['style_encoder'](x_fake, y_trg)\n",
        "  loss_sty = tf.math.reduce_mean(tf.abs(s_pred - s_trg))\n",
        "\n",
        "  # diversity sensitive loss\n",
        "  if z_trgs is not None:\n",
        "    s_trg2 = nets['mapping_network'](z_trg2, y_trg)\n",
        "  else:\n",
        "    s_trg2 = nets['style_encoder'](x_ref2, y_trg)\n",
        "  x_fake2 = nets['generator'](x_real, s_trg2)\n",
        "  loss_ds = tf.math.reduce_mean(tf.abs(x_fake - x_fake2))\n",
        "\n",
        "  # cycle-consistency loss\n",
        "  s_org = nets['style_encoder'](x_real, y_org)\n",
        "  x_rec = nets['generator'](x_fake, s_org)\n",
        "  loss_cyc = tf.math.reduce_mean(tf.abs(x_rec - x_real))\n",
        "\n",
        "  loss = loss_adv + args['lambda_sty'] * loss_sty \\\n",
        "        - args['lambda_ds'] * loss_ds + args['lambda_cyc'] * loss_cyc\n",
        "  return loss, {'adv':loss_adv, 'sty':loss_sty, 'ds':loss_ds, 'cyc':loss_cyc}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_PposMX_NEd"
      },
      "source": [
        "# The Model\n",
        "\n",
        "Here we introduce the class Solver which is the most important class as this will represent our whole model. It will initiate all of our neural networks as well as train our network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBIuDj7t_XjA"
      },
      "source": [
        "class Solver(tf.keras.Model):\n",
        "  def __init__(self, args):\n",
        "    super(Solver, self).__init__()\n",
        "    self.args = args\n",
        "    self.step=0\n",
        "\n",
        "    self.nets, self.nets_ema = self.build_model(self.args)\n",
        "    # below setattrs are to make networks be children of Solver, e.g., for self.to(self.device)\n",
        "    for name in self.nets.keys():\n",
        "      setattr(self, name, self.nets[name])\n",
        "    for name in self.nets_ema.keys():\n",
        "      setattr(self, name + '_ema', self.nets_ema[name])\n",
        "\n",
        "    if args['mode'] == 'train':\n",
        "      self.optims = {}\n",
        "      for net in self.nets.keys():\n",
        "        self.optims[net] = tf.keras.optimizers.Adam(learning_rate= args['f_lr'] if net == 'mapping_network' else args['lr'], \n",
        "                                                    beta_1=args['beta1'], beta_2=args['beta2'], \n",
        "                                                    epsilon=args['weight_decay'])\n",
        "\n",
        "        self.ckptios = [tf.train.Checkpoint(model=net) for net in self.nets.values()]\n",
        "        self.ckptios += [tf.train.Checkpoint(model=net_ema) for net_ema in self.nets_ema.values()]\n",
        "        self.ckptios += [tf.train.Checkpoint(optimizer=optim) for optim in self.optims.values()]\n",
        "    else:\n",
        "      self.ckptios = [tf.train.Checkpoint(model=net_ema) for net_ema in self.nets_ema.values()]\n",
        "\n",
        "    #for name in self.nets.keys():\n",
        "      # Do not initialize the FAN parameters\n",
        "     # print('Initializing %s...' % name)\n",
        "      #self.nets[name].apply(initializer=tf.keras.initializers.HeNormal)\n",
        "\n",
        "  def build_model(self, args):\n",
        "    generator = Generator(args['img_size'], args['style_dim'])\n",
        "    mapping_network = MappingNetwork(args['latent_dim'], args['style_dim'], args['num_domains'])\n",
        "    style_encoder = StyleEncoder(args['img_size'], args['style_dim'], args['num_domains'])\n",
        "    discriminator = Discriminator(args['img_size'], args['num_domains'])\n",
        "    generator_ema = Generator(args['img_size'], args['style_dim'])\n",
        "    mapping_network_ema = MappingNetwork(args['latent_dim'], args['style_dim'], args['num_domains'])\n",
        "    style_encoder_ema = StyleEncoder(args['img_size'], args['style_dim'], args['num_domains'])\n",
        "\n",
        "    nets = {'generator':generator, 'mapping_network':mapping_network,\n",
        "            'style_encoder':style_encoder, 'discriminator':discriminator}\n",
        "    nets_ema = {'generator':generator_ema, 'mapping_network':mapping_network_ema,\n",
        "                'style_encoder':style_encoder_ema}\n",
        "\n",
        "    nets['discriminator'](inputs[0]['x_src'],inputs[0]['y_src'])\n",
        "    s_trg = nets['mapping_network'](inputs[0]['z_trg'],inputs[0]['y_src'])\n",
        "    nets['generator'](inputs[0]['x_src'],s_trg)\n",
        "    nets['style_encoder'](inputs[0]['x_src'], inputs[0]['y_src'])\n",
        "    s_trg = nets_ema['mapping_network'](inputs[0]['z_trg'],inputs[0]['y_src'])\n",
        "    nets_ema['generator'](inputs[0]['x_src'],s_trg)\n",
        "    nets_ema['style_encoder'](inputs[0]['x_src'], inputs[0]['y_src'])\n",
        "\n",
        "    return nets, nets_ema\n",
        "\n",
        "  def save(self):\n",
        "    for net in solv.nets.keys():\n",
        "      solv.nets[net].save_weights('drive/My Drive/MNIST_GAN_2/saved_model/'+net+'step'+str(self.step)+'.h5')\n",
        "    for net in solv.nets_ema.keys():\n",
        "      solv.nets[net].save_weights('drive/My Drive/MNIST_GAN_2/saved_model/'+net+'step'+str(self.step)+'_ema.h5')\n",
        "    \n",
        "    \n",
        "    #for ckptio in self.ckptios:\n",
        "    #  ckptio.save(step)\n",
        "\n",
        "  def load(self, step):\n",
        "    self.step= step\n",
        "    for net in solv.nets.keys():\n",
        "      solv.nets[net].load_weights('drive/My Drive/MNIST_GAN_2/saved_model/'+net+'step'+str(step)+'.h5')\n",
        "    for net in solv.nets_ema.keys():\n",
        "      solv.nets[net].load_weights('drive/My Drive/MNIST_GAN_2/saved_model/'+net+'step'+str(step)+'_ema.h5')\n",
        "    \n",
        "    #for ckptio in self.ckptios:\n",
        "    #  ckptio.load(step)\n",
        "\n",
        "#  def _reset_grad(self):\n",
        "#    for optim in self.optims.values():\n",
        "#      optim.zero_grad()\n",
        "\n",
        "  def train(self, inputs, validations):\n",
        "    \"\"\"\n",
        "    inputs is a list of dictionaries that contains a source image, a reference image, domain and latent code information used to train the network\n",
        "    validation is a list that contains validation images\n",
        "    \"\"\"\n",
        "\n",
        "    args = self.args\n",
        "    nets = self.nets\n",
        "    nets_ema = self.nets_ema\n",
        "    optims = self.optims\n",
        "\n",
        "    inputs_val=validations[0]\n",
        "\n",
        "    # resume training if necessary\n",
        "    if args['resume_iter'] > 0:\n",
        "      self.load(args['resume_iter'])\n",
        "\n",
        "    # remember the initial value of ds weight\n",
        "    initial_lambda_ds = args['lambda_ds']\n",
        "\n",
        "    print('Start training...')\n",
        "    start_time = time.time()\n",
        "    for i in range(args['resume_iter'], args['total_iters']):\n",
        "      self.step+=1\n",
        "      # fetch images and labels\n",
        "      input= inputs[i-args['resume_iter']]\n",
        "\n",
        "      x_real, y_org = input['x_src'], input['y_src']\n",
        "      x_ref, x_ref2, y_trg = input['x_ref'], input['x_ref2'], input['y_ref']\n",
        "      z_trg, z_trg2 = input['z_trg'], input['z_trg2']\n",
        "\n",
        "      #print(1.5)\n",
        "\n",
        "      # train the discriminator\n",
        "      with tf.GradientTape() as g:\n",
        "        g.watch(nets['discriminator'].weights)\n",
        "\n",
        "        d_loss, d_losses_latent = compute_d_loss(\n",
        "                nets, args, x_real, y_org, y_trg, z_trg=z_trg)\n",
        "        #self._reset_grad()\n",
        "        #d_loss.backward()\n",
        "        grad=g.gradient(d_loss, nets['discriminator'].weights)\n",
        "        #optims['discriminator'].get_gradients(d_loss, nets['discriminator'].weights)\n",
        "        optims['discriminator'].apply_gradients(zip(grad, nets['discriminator'].weights))\n",
        "\n",
        "      #print(2)\n",
        "\n",
        "      with tf.GradientTape() as g:\n",
        "        g.watch(nets['discriminator'].weights)\n",
        "        d_loss, d_losses_ref = compute_d_loss(\n",
        "                nets, args, x_real, y_org, y_trg, x_ref=x_ref)\n",
        "        #self._reset_grad()\n",
        "        #d_loss.backward()\n",
        "        grad=g.gradient(d_loss, nets['discriminator'].weights)\n",
        "        optims['discriminator'].apply_gradients(zip(grad, nets['discriminator'].weights))\n",
        "\n",
        "      #print(3)\n",
        "\n",
        "      # train the generator\n",
        "      with tf.GradientTape(persistent=True) as g:\n",
        "        g.watch(nets['generator'].weights)\n",
        "        g.watch(nets['mapping_network'].weights)\n",
        "        g.watch(nets['style_encoder'].weights)\n",
        "        g_loss, g_losses_latent = compute_g_loss(\n",
        "                nets, args, x_real, y_org, y_trg, z_trgs=[z_trg, z_trg2])\n",
        "        #self._reset_grad()\n",
        "        #g_loss.backward()\n",
        "        grad=g.gradient(g_loss, nets['generator'].weights)\n",
        "        optims['generator'].apply_gradients(zip(grad, nets['generator'].weights))\n",
        "        grad=g.gradient(g_loss, nets['mapping_network'].weights)\n",
        "        optims['mapping_network'].apply_gradients(zip(grad, nets['mapping_network'].weights))\n",
        "        grad=g.gradient(g_loss, nets['style_encoder'].weights)\n",
        "        optims['style_encoder'].apply_gradients(zip(grad, nets['style_encoder'].weights))\n",
        "        del g\n",
        "\n",
        "      #print(4)\n",
        "      with tf.GradientTape(persistent=True) as g:\n",
        "        g.watch(nets['generator'].weights)\n",
        "        g_loss, g_losses_ref = compute_g_loss(\n",
        "                nets, args, x_real, y_org, y_trg, x_refs=[x_ref, x_ref2])\n",
        "        #self._reset_grad()\n",
        "        #g_loss.backward()\n",
        "        grad=g.gradient(g_loss, nets['generator'].weights)\n",
        "        optims['generator'].apply_gradients(zip(grad, nets['generator'].weights))\n",
        "\n",
        "      #print(5)\n",
        "\n",
        "      # compute moving average of network parameters\n",
        "      moving_average(nets['generator'], nets_ema['generator'], beta=0.999)\n",
        "      moving_average(nets['mapping_network'], nets_ema['mapping_network'], beta=0.999)\n",
        "      moving_average(nets['style_encoder'], nets_ema['style_encoder'], beta=0.999)\n",
        "\n",
        "      #print(6)\n",
        "\n",
        "      # decay weight for diversity sensitive loss\n",
        "      if args['lambda_ds'] > 0:\n",
        "        args['lambda_ds'] -= (initial_lambda_ds / args['ds_iter'])\n",
        "\n",
        "      # print out log info\n",
        "      if (i+1) % args['print_every'] == 0:\n",
        "        elapsed = time.time() - start_time\n",
        "        elapsed = str(datetime.timedelta(seconds=elapsed))[:-7]\n",
        "        log = \"Elapsed time [%s], Iteration [%i/%i], \" % (elapsed, i+1, args['total_iters'])\n",
        "        all_losses = {}\n",
        "        for loss, prefix in [(d_losses_latent,'D/latent_'), (d_losses_ref,'D/ref_'), \n",
        "                             (g_losses_latent,'G/latent_'), (g_losses_ref,'G/ref_')]:\n",
        "          for key, value in loss.items():\n",
        "            all_losses[prefix + key] = value\n",
        "        all_losses['G/lambda_ds'] = args['lambda_ds']\n",
        "        for key, value in all_losses.items():\n",
        "          if key!= 'G/lambda_ds':\n",
        "            print(log+key, value.numpy())\n",
        "          else:\n",
        "            print(log+key, value)\n",
        "\n",
        "      # generate images for debugging\n",
        "      #if (i+1) % args['sample_every'] == 0:\n",
        "      #  os.makedirs(args['sample_dir'], exist_ok=True)\n",
        "      #  debug_image(nets_ema, args, inputs=inputs_val, step=i+1)\n",
        "\n",
        "      # save model checkpoints\n",
        "      if (i+1) % args['save_every'] == 0:\n",
        "        for net in solv.nets.keys():\n",
        "          solv.nets[net].save_weights('drive/My Drive/MNIST_GAN_2/saved_model/'+net+'step'+str(self.step)+'.h5')\n",
        "        for net in solv.nets_ema.keys():\n",
        "          solv.nets[net].save_weights('drive/My Drive/MNIST_GAN_2/saved_model/'+net+'step'+str(self.step)+'_ema.h5')\n",
        "      \n",
        "      \n",
        "      #  self._save_checkpoint(step=i+1)\n",
        "\n",
        "  def sample(self, src, ref):\n",
        "    \"\"\"\n",
        "    src     source image that we want to modify\n",
        "    ref     pair of reference image and domain\n",
        "\n",
        "    generates an image that changes source image into the style of the reference image \n",
        "    \"\"\"\n",
        "    args = self.args\n",
        "    nets_ema = self.nets_ema\n",
        "    os.makedirs(args['result_dir'], exist_ok=True)\n",
        "    self._load_checkpoint(args['resume_iter'])\n",
        "\n",
        "    fname = ospj(args['result_dir'], 'reference.jpg')\n",
        "    print('Working on {}...'.format(fname))\n",
        "    translate_using_reference(nets_ema, args, src, ref[0], ref[1], fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1nqH_NanX_r"
      },
      "source": [
        "# Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wkp2TOCOnbmu"
      },
      "source": [
        "(trainX, trainy), (valX, valy) = load_data()\n",
        "\n",
        "trainX=tf.reshape(trainX, (60000,1,28,28,1))\n",
        "valX=tf.reshape(valX, (10000,1,28,28,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfGnysBhnfeG",
        "outputId": "29a6789d-1bd5-45e6-b3ae-2a36efd2e2a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "\n",
        "inputs=[]\n",
        "latent_dim=8\n",
        "for i in range(10000):\n",
        "  i=i\n",
        "  if i % 2000==1999:\n",
        "    print(i+1)\n",
        "  input={}\n",
        "  input['x_src']=tf.cast(trainX[i],tf.float32)\n",
        "  input['y_src']=int(trainy[i])\n",
        "  n=np.random.randint(0,60000)\n",
        "  input['x_ref']=tf.cast(trainX[n],tf.float32)\n",
        "  input['x_ref2']=tf.cast(trainX[np.random.randint(0,60000)],tf.float32)\n",
        "  input['y_ref']=int(trainy[n])\n",
        "  input['z_trg']=tf.random.normal((1,latent_dim))\n",
        "  input['z_trg2']=tf.random.normal((1,latent_dim))\n",
        "  inputs.append(input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n",
            "4000\n",
            "6000\n",
            "8000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lQnUhqyZfMQ"
      },
      "source": [
        "# Parameters\n",
        "\n",
        "This dictionary contains the different parameters we use to run the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5bhGZY2hkGi"
      },
      "source": [
        "args={'img_size':28,\n",
        "      'style_dim':24,\n",
        "      'latent_dim':16,\n",
        "      'num_domains':10,\n",
        "      'lambda_reg':1, \n",
        "      'lambda_ds':1,\n",
        "      'lambda_sty':10,\n",
        "      'lambda_cyc':.1,\n",
        "      'hidden_dim':128,\n",
        "      'resume_iter':0,\n",
        "      'ds_iter':10000,  \n",
        "      'total_iters':10000,\n",
        "      'batch_size':8,\n",
        "      'val_batch_size':32,  \n",
        "      'lr':1e-4,\n",
        "      'f_lr':1e-6,\n",
        "      'beta1':0,\n",
        "      'beta2':0.99,\n",
        "      'weight_decay':1e-4,\n",
        "      'num_outs_per_domain':4,\n",
        "      'mode': 'train',  #train,sample,eval\n",
        "      'seed':0,\n",
        "      'train_img_dir':'GAN/data/train',\n",
        "      'val_img_dir': 'GAN/data/val',\n",
        "      'sample_dir':'GAN/res/samples',\n",
        "      'checkpoint_dir':'GAN/res/checkpoints',\n",
        "      'eval_dir':'GAN/res/eval',\n",
        "      'result_dir':'GAN/res/results',\n",
        "      'src_dir':'GAN/data/src', \n",
        "      'ref_dir':'GAN/data/ref',\n",
        "      'print_every': 500,\n",
        "      'sample_every':200,\n",
        "      'save_every':1000,\n",
        "      'eval_every':1000 }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRxr0xdqajQx"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edoX0YFH5Zdb"
      },
      "source": [
        "solv=Solver(args)\n",
        "solv.build_model(args)\n",
        "solv.load(60000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7Lk-dm-L7sg"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwL2FjeoL9MY",
        "outputId": "74692085-c9a7-4116-ee8f-93e29bafd46a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  solv.train(inputs, inputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "Elapsed time [0:25:25], Iteration [500/10000], D/latent_real 3.8035455e-07\n",
            "Elapsed time [0:25:25], Iteration [500/10000], D/latent_fake 0.00094635325\n",
            "Elapsed time [0:25:25], Iteration [500/10000], D/latent_reg 0.0007748243\n",
            "Elapsed time [0:25:25], Iteration [500/10000], D/ref_real 4.19652e-07\n",
            "Elapsed time [0:25:25], Iteration [500/10000], D/ref_fake 0.0014167315\n",
            "Elapsed time [0:25:25], Iteration [500/10000], D/ref_reg 0.0008041278\n",
            "Elapsed time [0:25:25], Iteration [500/10000], G/latent_adv 7.603031\n",
            "Elapsed time [0:25:25], Iteration [500/10000], G/latent_sty 0.018901378\n",
            "Elapsed time [0:25:25], Iteration [500/10000], G/latent_ds 53.252697\n",
            "Elapsed time [0:25:25], Iteration [500/10000], G/latent_cyc 30.847721\n",
            "Elapsed time [0:25:25], Iteration [500/10000], G/ref_adv 7.709834\n",
            "Elapsed time [0:25:25], Iteration [500/10000], G/ref_sty 0.02280374\n",
            "Elapsed time [0:25:25], Iteration [500/10000], G/ref_ds 14.743326\n",
            "Elapsed time [0:25:25], Iteration [500/10000], G/ref_cyc 28.888575\n",
            "Elapsed time [0:25:25], Iteration [500/10000], G/lambda_ds 0.9500000000000055\n",
            "Elapsed time [0:50:53], Iteration [1000/10000], D/latent_real 7.7350414e-08\n",
            "Elapsed time [0:50:53], Iteration [1000/10000], D/latent_fake 0.0015410566\n",
            "Elapsed time [0:50:53], Iteration [1000/10000], D/latent_reg 0.0007872412\n",
            "Elapsed time [0:50:53], Iteration [1000/10000], D/ref_real 7.947615e-08\n",
            "Elapsed time [0:50:53], Iteration [1000/10000], D/ref_fake 3.8739453e-10\n",
            "Elapsed time [0:50:53], Iteration [1000/10000], D/ref_reg 0.0007866467\n",
            "Elapsed time [0:50:53], Iteration [1000/10000], G/latent_adv 6.588364\n",
            "Elapsed time [0:50:53], Iteration [1000/10000], G/latent_sty 0.022705676\n",
            "Elapsed time [0:50:53], Iteration [1000/10000], G/latent_ds 16.470871\n",
            "Elapsed time [0:50:53], Iteration [1000/10000], G/latent_cyc 34.649124\n",
            "Elapsed time [0:50:53], Iteration [1000/10000], G/ref_adv 21.71409\n",
            "Elapsed time [0:50:53], Iteration [1000/10000], G/ref_sty 44.76697\n",
            "Elapsed time [0:50:53], Iteration [1000/10000], G/ref_ds 0.81211525\n",
            "Elapsed time [0:50:53], Iteration [1000/10000], G/ref_cyc 34.96733\n",
            "Elapsed time [0:50:53], Iteration [1000/10000], G/lambda_ds 0.900000000000011\n",
            "Elapsed time [1:16:21], Iteration [1500/10000], D/latent_real 1.24858425e-05\n",
            "Elapsed time [1:16:21], Iteration [1500/10000], D/latent_fake 5.9629114e-11\n",
            "Elapsed time [1:16:21], Iteration [1500/10000], D/latent_reg 0.00019563411\n",
            "Elapsed time [1:16:21], Iteration [1500/10000], D/ref_real 1.2505483e-05\n",
            "Elapsed time [1:16:21], Iteration [1500/10000], D/ref_fake 2.1104589e-13\n",
            "Elapsed time [1:16:21], Iteration [1500/10000], D/ref_reg 0.00019553732\n",
            "Elapsed time [1:16:21], Iteration [1500/10000], G/latent_adv 23.542889\n",
            "Elapsed time [1:16:21], Iteration [1500/10000], G/latent_sty 0.037070256\n",
            "Elapsed time [1:16:21], Iteration [1500/10000], G/latent_ds 88.45014\n",
            "Elapsed time [1:16:21], Iteration [1500/10000], G/latent_cyc 35.58559\n",
            "Elapsed time [1:16:21], Iteration [1500/10000], G/ref_adv 29.250957\n",
            "Elapsed time [1:16:21], Iteration [1500/10000], G/ref_sty 12.663005\n",
            "Elapsed time [1:16:21], Iteration [1500/10000], G/ref_ds 69.80585\n",
            "Elapsed time [1:16:21], Iteration [1500/10000], G/ref_cyc 34.501183\n",
            "Elapsed time [1:16:21], Iteration [1500/10000], G/lambda_ds 0.8500000000000165\n",
            "Elapsed time [1:41:50], Iteration [2000/10000], D/latent_real 4.97012e-07\n",
            "Elapsed time [1:41:50], Iteration [2000/10000], D/latent_fake 7.8069725e-08\n",
            "Elapsed time [1:41:50], Iteration [2000/10000], D/latent_reg 0.0017582542\n",
            "Elapsed time [1:41:50], Iteration [2000/10000], D/ref_real 5.089129e-07\n",
            "Elapsed time [1:41:50], Iteration [2000/10000], D/ref_fake 3.9939685e-10\n",
            "Elapsed time [1:41:50], Iteration [2000/10000], D/ref_reg 0.001732865\n",
            "Elapsed time [1:41:50], Iteration [2000/10000], G/latent_adv 16.337694\n",
            "Elapsed time [1:41:50], Iteration [2000/10000], G/latent_sty 0.024729565\n",
            "Elapsed time [1:41:50], Iteration [2000/10000], G/latent_ds 84.317\n",
            "Elapsed time [1:41:50], Iteration [2000/10000], G/latent_cyc 59.586277\n",
            "Elapsed time [1:41:50], Iteration [2000/10000], G/ref_adv 21.596052\n",
            "Elapsed time [1:41:50], Iteration [2000/10000], G/ref_sty 73.802925\n",
            "Elapsed time [1:41:50], Iteration [2000/10000], G/ref_ds 72.336044\n",
            "Elapsed time [1:41:50], Iteration [2000/10000], G/ref_cyc 57.130764\n",
            "Elapsed time [1:41:50], Iteration [2000/10000], G/lambda_ds 0.800000000000022\n",
            "Elapsed time [2:07:23], Iteration [2500/10000], D/latent_real 0.0173225\n",
            "Elapsed time [2:07:23], Iteration [2500/10000], D/latent_fake 9.424545e-06\n",
            "Elapsed time [2:07:23], Iteration [2500/10000], D/latent_reg 0.016297052\n",
            "Elapsed time [2:07:23], Iteration [2500/10000], D/ref_real 0.004332891\n",
            "Elapsed time [2:07:23], Iteration [2500/10000], D/ref_fake 4.2522784e-06\n",
            "Elapsed time [2:07:23], Iteration [2500/10000], D/ref_reg 0.0042403387\n",
            "Elapsed time [2:07:23], Iteration [2500/10000], G/latent_adv 10.998817\n",
            "Elapsed time [2:07:23], Iteration [2500/10000], G/latent_sty 0.019953068\n",
            "Elapsed time [2:07:23], Iteration [2500/10000], G/latent_ds 46.09984\n",
            "Elapsed time [2:07:23], Iteration [2500/10000], G/latent_cyc 21.726707\n",
            "Elapsed time [2:07:23], Iteration [2500/10000], G/ref_adv 10.968796\n",
            "Elapsed time [2:07:23], Iteration [2500/10000], G/ref_sty 0.037745275\n",
            "Elapsed time [2:07:23], Iteration [2500/10000], G/ref_ds 73.81894\n",
            "Elapsed time [2:07:23], Iteration [2500/10000], G/ref_cyc 24.85559\n",
            "Elapsed time [2:07:23], Iteration [2500/10000], G/lambda_ds 0.7500000000000275\n",
            "Elapsed time [2:32:54], Iteration [3000/10000], D/latent_real 6.613375e-05\n",
            "Elapsed time [2:32:54], Iteration [3000/10000], D/latent_fake 6.903717e-08\n",
            "Elapsed time [2:32:54], Iteration [3000/10000], D/latent_reg 0.00015479299\n",
            "Elapsed time [2:32:54], Iteration [3000/10000], D/ref_real 6.597582e-05\n",
            "Elapsed time [2:32:54], Iteration [3000/10000], D/ref_fake 0.001151727\n",
            "Elapsed time [2:32:54], Iteration [3000/10000], D/ref_reg 0.0001548348\n",
            "Elapsed time [2:32:54], Iteration [3000/10000], G/latent_adv 16.77787\n",
            "Elapsed time [2:32:54], Iteration [3000/10000], G/latent_sty 0.028202035\n",
            "Elapsed time [2:32:54], Iteration [3000/10000], G/latent_ds 91.3079\n",
            "Elapsed time [2:32:54], Iteration [3000/10000], G/latent_cyc 39.233337\n",
            "Elapsed time [2:32:54], Iteration [3000/10000], G/ref_adv 7.9122443\n",
            "Elapsed time [2:32:54], Iteration [3000/10000], G/ref_sty 0.011915297\n",
            "Elapsed time [2:32:54], Iteration [3000/10000], G/ref_ds 76.62266\n",
            "Elapsed time [2:32:54], Iteration [3000/10000], G/ref_cyc 40.150024\n",
            "Elapsed time [2:32:54], Iteration [3000/10000], G/lambda_ds 0.700000000000033\n",
            "Elapsed time [2:58:20], Iteration [3500/10000], D/latent_real 5.3080607e-06\n",
            "Elapsed time [2:58:20], Iteration [3500/10000], D/latent_fake 3.8684348e-07\n",
            "Elapsed time [2:58:20], Iteration [3500/10000], D/latent_reg 0.0003957442\n",
            "Elapsed time [2:58:20], Iteration [3500/10000], D/ref_real 5.328536e-06\n",
            "Elapsed time [2:58:20], Iteration [3500/10000], D/ref_fake 1.05703536e-10\n",
            "Elapsed time [2:58:20], Iteration [3500/10000], D/ref_reg 0.00039511797\n",
            "Elapsed time [2:58:20], Iteration [3500/10000], G/latent_adv 14.771749\n",
            "Elapsed time [2:58:20], Iteration [3500/10000], G/latent_sty 0.026847294\n",
            "Elapsed time [2:58:20], Iteration [3500/10000], G/latent_ds 88.832466\n",
            "Elapsed time [2:58:20], Iteration [3500/10000], G/latent_cyc 31.29968\n",
            "Elapsed time [2:58:20], Iteration [3500/10000], G/ref_adv 23.421618\n",
            "Elapsed time [2:58:20], Iteration [3500/10000], G/ref_sty 5.7295127\n",
            "Elapsed time [2:58:20], Iteration [3500/10000], G/ref_ds 48.516193\n",
            "Elapsed time [2:58:20], Iteration [3500/10000], G/ref_cyc 30.839699\n",
            "Elapsed time [2:58:20], Iteration [3500/10000], G/lambda_ds 0.6500000000000385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMzzM5poyVg2"
      },
      "source": [
        "# Results\n",
        "\n",
        "In this first cell we show an image where the rows represent a source image and the columns the style they are trying to mimic. We can see in this case that that the image still highly resembles the source image but has obtained some characteristics depending on the style of our reference. In most cases this style is mostly about the thickness of the lines, but it does vary slightly in other ways."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDyfNQeAyW42"
      },
      "source": [
        "import matplotlib.pyplot as pyplot\n",
        "for i in range(4):\n",
        "\tpyplot.subplot(5,5,2+i)\n",
        "\tpyplot.axis('off')\n",
        "\tpyplot.imshow(np.reshape(inputs[i]['x_ref'],[28,28]), cmap='gray_r')\n",
        "for i in range(4):\n",
        "\tpyplot.subplot(5, 5, 5*(i+1) + 1)\n",
        "\tpyplot.axis('off')\n",
        "\tpyplot.imshow(np.reshape(inputs[i]['x_src'], [28,28]), cmap='gray_r')\n",
        "\tfor j in range(4):\n",
        "\t\tpyplot.subplot(5, 5, 5*(i+1) + j +2)\n",
        "\t\tpyplot.axis('off')\n",
        "\t\tpyplot.imshow(np.reshape(solv.nets['generator'](inputs[i]['x_src'],solv.nets['style_encoder'](inputs[j]['x_ref'],inputs[j]['y_ref'])).numpy(), [28,28]), cmap='gray_r')\n",
        "pyplot.show()\n",
        "\n",
        "#left is source and top is the target trying to mimic its font"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOcIcfWXcTnE"
      },
      "source": [
        "Below we generate random styles and see the output it generates. We notice that it is quite likely the images are distorted in this case, compared to when using the style of an already existing image it seems it would usually have a good quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rxva38XUbRI_"
      },
      "source": [
        "for i in range(5):\n",
        "\tpyplot.subplot(5,5,1+i)\n",
        "\tpyplot.axis('off')\n",
        "\tpyplot.imshow(np.reshape(solv.nets['generator'](inputs[0]['x_src'],tf.random.normal((1,24))).numpy(), [28,28]), cmap='gray_r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsgqgBHVHsSj"
      },
      "source": [
        "Here we can see the process of how the image transforms into the target. In these small images there is not too much that is changing but we can still appreciate the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24EUFLjWFfOc"
      },
      "source": [
        "s1=solv.nets['style_encoder'](inputs[3]['x_src'],inputs[3]['y_src'])\n",
        "s2=solv.nets['style_encoder'](inputs[3]['x_ref'],inputs[3]['y_ref'])\n",
        "for i in range(5):\n",
        "  pyplot.subplot(5,5,1+i)\n",
        "  pyplot.axis('off')\n",
        "  s=(1-i/5)*s1+i/5*s2\n",
        "  pyplot.imshow(np.reshape(solv.nets['generator'](inputs[3]['x_src'],s).numpy(), [28,28]), cmap='gray_r')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}