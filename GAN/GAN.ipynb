{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1eprarfZJJy7eEQlrN88a9zdQcID0bB-Y",
      "authorship_tag": "ABX9TyP0HlBwIsXwmnm6JzlDoZSh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nk555/AI-Projects/blob/master/GAN/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BCDLa2EXCSD",
        "colab_type": "text"
      },
      "source": [
        "# MNIST learns your handwriting\n",
        "\n",
        "This is a small test of using a GAN to generate numbers that look as someone's handwriting when not trained on all numbers. For example we saw someone write the number 273 and we now predict how 481 looks like. \n",
        "\n",
        "Main inspiration for doing this is a paper I read recently on Star GAN v2. In this paper they try to recognize diferent latent spaces on the data used and making predictions. For example they are able to use image of different animals like dogs or tigers and making them look like a cat. \n",
        "\n",
        "For a small tutorial on how to write a GAN I looked at: https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/\n",
        "\n",
        "Link to Star GAN v2: https://app.wandb.ai/stacey/stargan/reports/Cute-Animals-and-Post-Modern-Style-Transfer%3A-StarGAN-v2-for-Multi-Domain-Image-Synthesis---VmlldzoxNzcwODQ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stY43f8UtP3W",
        "colab_type": "text"
      },
      "source": [
        "# Initializing and Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH-SuWd1GUhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import math\n",
        "from collections import namedtuple\n",
        "from copy import deepcopy\n",
        "from functools import partial\n",
        "import argparse\n",
        "\n",
        "from munch import Munch\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "from skimage.filters import gaussian\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from collections import OrderedDict\n",
        "\n",
        "from torchvision import models\n",
        "from scipy import linalg\n",
        "from core.data_loader import get_eval_loader"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhuFSFKaHfsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_real_samples():\n",
        "\t(trainX, _), (_, _) = load_data()\n",
        "\t# expand to 3d for using CNN\n",
        "\tX = expand_dims(trainX, axis=-1)\n",
        "\tX = X.astype('float32')\n",
        "\t# normalize to [0,1]\n",
        "\tX = X / 255.0\n",
        "\treturn X"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6E-mpzNyuDX",
        "colab_type": "text"
      },
      "source": [
        "# Helper Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGNtw-mmyMlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResBlk(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, actv=nn.LeakyReLU(0.2),\n",
        "                 normalize=False, downsample=False):\n",
        "        super().__init__()\n",
        "        self.actv = actv\n",
        "        self.normalize = normalize\n",
        "        self.downsample = downsample\n",
        "        self.learned_sc = dim_in != dim_out\n",
        "        self._build_weights(dim_in, dim_out)\n",
        "\n",
        "    def _build_weights(self, dim_in, dim_out):\n",
        "        self.conv1 = nn.Conv2d(dim_in, dim_in, 3, 1, 1)\n",
        "        self.conv2 = nn.Conv2d(dim_in, dim_out, 3, 1, 1)\n",
        "        if self.normalize:\n",
        "            self.norm1 = nn.InstanceNorm2d(dim_in, affine=True)\n",
        "            self.norm2 = nn.InstanceNorm2d(dim_in, affine=True)\n",
        "        if self.learned_sc:\n",
        "            self.conv1x1 = nn.Conv2d(dim_in, dim_out, 1, 1, 0, bias=False)\n",
        "\n",
        "    def _shortcut(self, x):\n",
        "        if self.learned_sc:\n",
        "            x = self.conv1x1(x)\n",
        "        if self.downsample:\n",
        "            x = F.avg_pool2d(x, 2)\n",
        "        return x\n",
        "\n",
        "    def _residual(self, x):\n",
        "        if self.normalize:\n",
        "            x = self.norm1(x)\n",
        "        x = self.actv(x)\n",
        "        x = self.conv1(x)\n",
        "        if self.downsample:\n",
        "            x = F.avg_pool2d(x, 2)\n",
        "        if self.normalize:\n",
        "            x = self.norm2(x)\n",
        "        x = self.actv(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._shortcut(x) + self._residual(x)\n",
        "        return x / math.sqrt(2)  # unit variance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Tx-G0a7y0f4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdaIN(nn.Module):\n",
        "    def __init__(self, style_dim, num_features):\n",
        "        super().__init__()\n",
        "        self.norm = nn.InstanceNorm2d(num_features, affine=False)\n",
        "        self.fc = nn.Linear(style_dim, num_features*2)\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        h = self.fc(s)\n",
        "        h = h.view(h.size(0), h.size(1), 1, 1)\n",
        "        gamma, beta = torch.chunk(h, chunks=2, dim=1)\n",
        "        return (1 + gamma) * self.norm(x) + beta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gfcz8qoy85-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdainResBlk(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, style_dim=64, w_hpf=0,\n",
        "                 actv=nn.LeakyReLU(0.2), upsample=False):\n",
        "        super().__init__()\n",
        "        self.w_hpf = w_hpf\n",
        "        self.actv = actv\n",
        "        self.upsample = upsample\n",
        "        self.learned_sc = dim_in != dim_out\n",
        "        self._build_weights(dim_in, dim_out, style_dim)\n",
        "\n",
        "    def _build_weights(self, dim_in, dim_out, style_dim=64):\n",
        "        self.conv1 = nn.Conv2d(dim_in, dim_out, 3, 1, 1)\n",
        "        self.conv2 = nn.Conv2d(dim_out, dim_out, 3, 1, 1)\n",
        "        self.norm1 = AdaIN(style_dim, dim_in)\n",
        "        self.norm2 = AdaIN(style_dim, dim_out)\n",
        "        if self.learned_sc:\n",
        "            self.conv1x1 = nn.Conv2d(dim_in, dim_out, 1, 1, 0, bias=False)\n",
        "\n",
        "    def _shortcut(self, x):\n",
        "        if self.upsample:\n",
        "            x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        if self.learned_sc:\n",
        "            x = self.conv1x1(x)\n",
        "        return x\n",
        "\n",
        "    def _residual(self, x, s):\n",
        "        x = self.norm1(x, s)\n",
        "        x = self.actv(x)\n",
        "        if self.upsample:\n",
        "            x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm2(x, s)\n",
        "        x = self.actv(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        out = self._residual(x, s)\n",
        "        if self.w_hpf == 0:\n",
        "            out = (out + self._shortcut(x)) / math.sqrt(2)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVxwvuzAB0nM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InceptionV3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        inception = models.inception_v3(pretrained=True)\n",
        "        self.block1 = nn.Sequential(\n",
        "            inception.Conv2d_1a_3x3, inception.Conv2d_2a_3x3,\n",
        "            inception.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2))\n",
        "        self.block2 = nn.Sequential(\n",
        "            inception.Conv2d_3b_1x1, inception.Conv2d_4a_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2))\n",
        "        self.block3 = nn.Sequential(\n",
        "            inception.Mixed_5b, inception.Mixed_5c,\n",
        "            inception.Mixed_5d, inception.Mixed_6a,\n",
        "            inception.Mixed_6b, inception.Mixed_6c,\n",
        "            inception.Mixed_6d, inception.Mixed_6e)\n",
        "        self.block4 = nn.Sequential(\n",
        "            inception.Mixed_7a, inception.Mixed_7b,\n",
        "            inception.Mixed_7c,\n",
        "            nn.AdaptiveAvgPool2d(output_size=(1, 1)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        return x.view(x.size(0), -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyeldk4bCbWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(x, eps=1e-10):\n",
        "    return x * torch.rsqrt(torch.sum(x**2, dim=1, keepdim=True) + eps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIOxklTECfgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = models.alexnet(pretrained=True).features\n",
        "        self.channels = []\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                self.channels.append(layer.out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmaps = []\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "            if isinstance(layer, nn.ReLU):\n",
        "                fmaps.append(x)\n",
        "        return fmaps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pgr_FZKCiry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Conv1x1(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels=1):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC8N3_H7Cluk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LPIPS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.alexnet = AlexNet()\n",
        "        self.lpips_weights = nn.ModuleList()\n",
        "        for channels in self.alexnet.channels:\n",
        "            self.lpips_weights.append(Conv1x1(channels, 1))\n",
        "        self._load_lpips_weights()\n",
        "        # imagenet normalization for range [-1, 1]\n",
        "        self.mu = torch.tensor([-0.03, -0.088, -0.188]).view(1, 3, 1, 1).cuda()\n",
        "        self.sigma = torch.tensor([0.458, 0.448, 0.450]).view(1, 3, 1, 1).cuda()\n",
        "\n",
        "    def _load_lpips_weights(self):\n",
        "        own_state_dict = self.state_dict()\n",
        "        if torch.cuda.is_available():\n",
        "            state_dict = torch.load('metrics/lpips_weights.ckpt')\n",
        "        else:\n",
        "            state_dict = torch.load('metrics/lpips_weights.ckpt',\n",
        "                                    map_location=torch.device('cpu'))\n",
        "        for name, param in state_dict.items():\n",
        "            if name in own_state_dict:\n",
        "                own_state_dict[name].copy_(param)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = (x - self.mu) / self.sigma\n",
        "        y = (y - self.mu) / self.sigma\n",
        "        x_fmaps = self.alexnet(x)\n",
        "        y_fmaps = self.alexnet(y)\n",
        "        lpips_value = 0\n",
        "        for x_fmap, y_fmap, conv1x1 in zip(x_fmaps, y_fmaps, self.lpips_weights):\n",
        "            x_fmap = normalize(x_fmap)\n",
        "            y_fmap = normalize(y_fmap)\n",
        "            lpips_value += torch.mean(conv1x1((x_fmap - y_fmap)**2))\n",
        "        return lpips_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wD5dNFdCpVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@torch.no_grad()\n",
        "def calculate_lpips_given_images(group_of_images):\n",
        "    # group_of_images = [torch.randn(N, C, H, W) for _ in range(10)]\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    lpips = LPIPS().eval().to(device)\n",
        "    lpips_values = []\n",
        "    num_rand_outputs = len(group_of_images)\n",
        "\n",
        "    # calculate the average of pairwise distances among all random outputs\n",
        "    for i in range(num_rand_outputs-1):\n",
        "        for j in range(i+1, num_rand_outputs):\n",
        "            lpips_values.append(lpips(group_of_images[i], group_of_images[j]))\n",
        "    lpips_value = torch.mean(torch.stack(lpips_values, dim=0))\n",
        "    return lpips_value.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPSL7qYb-Lbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HighPass(nn.Module):\n",
        "    def __init__(self, w_hpf, device):\n",
        "        super(HighPass, self).__init__()\n",
        "        self.filter = torch.tensor([[-1, -1, -1],\n",
        "                                    [-1, 8., -1],\n",
        "                                    [-1, -1, -1]]).to(device) / w_hpf\n",
        "\n",
        "    def forward(self, x):\n",
        "        filter = self.filter.unsqueeze(0).unsqueeze(1).repeat(x.size(1), 1, 1, 1)\n",
        "        return F.conv2d(x, filter, padding=1, groups=x.size(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "269f1a0stvIp",
        "colab_type": "text"
      },
      "source": [
        "# Discriminator Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtXYGkA9kcg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_size=256, num_domains=2, max_conv_dim=512):\n",
        "        super().__init__()\n",
        "        dim_in = 2**14 // img_size\n",
        "        blocks = []\n",
        "        blocks += [nn.Conv2d(3, dim_in, 3, 1, 1)]\n",
        "\n",
        "        repeat_num = int(np.log2(img_size)) - 2\n",
        "        for _ in range(repeat_num):\n",
        "            dim_out = min(dim_in*2, max_conv_dim)\n",
        "            blocks += [ResBlk(dim_in, dim_out, downsample=True)]\n",
        "            dim_in = dim_out\n",
        "\n",
        "        blocks += [nn.LeakyReLU(0.2)]\n",
        "        blocks += [nn.Conv2d(dim_out, dim_out, 4, 1, 0)]\n",
        "        blocks += [nn.LeakyReLU(0.2)]\n",
        "        blocks += [nn.Conv2d(dim_out, num_domains, 1, 1, 0)]\n",
        "        self.main = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = self.main(x)\n",
        "        out = out.view(out.size(0), -1)  # (batch, num_domains)\n",
        "        idx = torch.LongTensor(range(y.size(0))).to(y.device)\n",
        "        out = out[idx, y]  # (batch)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnX_yIittyT-",
        "colab_type": "text"
      },
      "source": [
        "# Style Encoder Class (Latent Space)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQZPUtQSsTPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MappingNetwork(nn.Module):\n",
        "    def __init__(self, latent_dim=16, style_dim=64, num_domains=2):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        layers += [nn.Linear(latent_dim, 512)]\n",
        "        layers += [nn.ReLU()]\n",
        "        for _ in range(3):\n",
        "            layers += [nn.Linear(512, 512)]\n",
        "            layers += [nn.ReLU()]\n",
        "        self.shared = nn.Sequential(*layers)\n",
        "\n",
        "        self.unshared = nn.ModuleList()\n",
        "        for _ in range(num_domains):\n",
        "            self.unshared += [nn.Sequential(nn.Linear(512, 512),\n",
        "                                            nn.ReLU(),\n",
        "                                            nn.Linear(512, 512),\n",
        "                                            nn.ReLU(),\n",
        "                                            nn.Linear(512, 512),\n",
        "                                            nn.ReLU(),\n",
        "                                            nn.Linear(512, style_dim))]\n",
        "\n",
        "    def forward(self, z, y):\n",
        "        h = self.shared(z)\n",
        "        out = []\n",
        "        for layer in self.unshared:\n",
        "            out += [layer(h)]\n",
        "        out = torch.stack(out, dim=1)  # (batch, num_domains, style_dim)\n",
        "        idx = torch.LongTensor(range(y.size(0))).to(y.device)\n",
        "        s = out[idx, y]  # (batch, style_dim)\n",
        "        return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm4Kwa_vzjL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StyleEncoder(nn.Module):\n",
        "    def __init__(self, img_size=256, style_dim=64, num_domains=2, max_conv_dim=512):\n",
        "        super().__init__()\n",
        "        dim_in = 2**14 // img_size\n",
        "        blocks = []\n",
        "        blocks += [nn.Conv2d(3, dim_in, 3, 1, 1)]\n",
        "\n",
        "        repeat_num = int(np.log2(img_size)) - 2\n",
        "        for _ in range(repeat_num):\n",
        "            dim_out = min(dim_in*2, max_conv_dim)\n",
        "            blocks += [ResBlk(dim_in, dim_out, downsample=True)]\n",
        "            dim_in = dim_out\n",
        "\n",
        "        blocks += [nn.LeakyReLU(0.2)]\n",
        "        blocks += [nn.Conv2d(dim_out, dim_out, 4, 1, 0)]\n",
        "        blocks += [nn.LeakyReLU(0.2)]\n",
        "        self.shared = nn.Sequential(*blocks)\n",
        "\n",
        "        self.unshared = nn.ModuleList()\n",
        "        for _ in range(num_domains):\n",
        "            self.unshared += [nn.Linear(dim_out, style_dim)]\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        h = self.shared(x)\n",
        "        h = h.view(h.size(0), -1)\n",
        "        out = []\n",
        "        for layer in self.unshared:\n",
        "            out += [layer(h)]\n",
        "        out = torch.stack(out, dim=1)  # (batch, num_domains, style_dim)\n",
        "        idx = torch.LongTensor(range(y.size(0))).to(y.device)\n",
        "        s = out[idx, y]  # (batch, style_dim)\n",
        "        return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOLX45nxA-W5",
        "colab_type": "text"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68yPcb9CBIEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@torch.no_grad()\n",
        "def calculate_metrics(nets, args, step, mode):\n",
        "    print('Calculating evaluation metrics...')\n",
        "    assert mode in ['latent', 'reference']\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    domains = os.listdir(args.val_img_dir)\n",
        "    domains.sort()\n",
        "    num_domains = len(domains)\n",
        "    print('Number of domains: %d' % num_domains)\n",
        "\n",
        "    lpips_dict = OrderedDict()\n",
        "    for trg_idx, trg_domain in enumerate(domains):\n",
        "        src_domains = [x for x in domains if x != trg_domain]\n",
        "\n",
        "        if mode == 'reference':\n",
        "            path_ref = os.path.join(args.val_img_dir, trg_domain)\n",
        "            loader_ref = get_eval_loader(root=path_ref,\n",
        "                                         img_size=args.img_size,\n",
        "                                         batch_size=args.val_batch_size,\n",
        "                                         imagenet_normalize=False,\n",
        "                                         drop_last=True)\n",
        "\n",
        "        for src_idx, src_domain in enumerate(src_domains):\n",
        "            path_src = os.path.join(args.val_img_dir, src_domain)\n",
        "            loader_src = get_eval_loader(root=path_src,\n",
        "                                         img_size=args.img_size,\n",
        "                                         batch_size=args.val_batch_size,\n",
        "                                         imagenet_normalize=False)\n",
        "\n",
        "            task = '%s2%s' % (src_domain, trg_domain)\n",
        "            path_fake = os.path.join(args.eval_dir, task)\n",
        "            shutil.rmtree(path_fake, ignore_errors=True)\n",
        "            os.makedirs(path_fake)\n",
        "\n",
        "            lpips_values = []\n",
        "            print('Generating images and calculating LPIPS for %s...' % task)\n",
        "            for i, x_src in enumerate(tqdm(loader_src, total=len(loader_src))):\n",
        "                N = x_src.size(0)\n",
        "                x_src = x_src.to(device)\n",
        "                y_trg = torch.tensor([trg_idx] * N).to(device)\n",
        "                masks = nets.fan.get_heatmap(x_src) if args.w_hpf > 0 else None\n",
        "\n",
        "                # generate 10 outputs from the same input\n",
        "                group_of_images = []\n",
        "                for j in range(args.num_outs_per_domain):\n",
        "                    if mode == 'latent':\n",
        "                        z_trg = torch.randn(N, args.latent_dim).to(device)\n",
        "                        s_trg = nets.mapping_network(z_trg, y_trg)\n",
        "                    else:\n",
        "                        try:\n",
        "                            x_ref = next(iter_ref).to(device)\n",
        "                        except:\n",
        "                            iter_ref = iter(loader_ref)\n",
        "                            x_ref = next(iter_ref).to(device)\n",
        "\n",
        "                        if x_ref.size(0) > N:\n",
        "                            x_ref = x_ref[:N]\n",
        "                        s_trg = nets.style_encoder(x_ref, y_trg)\n",
        "\n",
        "                    x_fake = nets.generator(x_src, s_trg, masks=masks)\n",
        "                    group_of_images.append(x_fake)\n",
        "\n",
        "                    # save generated images to calculate FID later\n",
        "                    for k in range(N):\n",
        "                        filename = os.path.join(\n",
        "                            path_fake,\n",
        "                            '%.4i_%.2i.png' % (i*args.val_batch_size+(k+1), j+1))\n",
        "                        utils.save_image(x_fake[k], ncol=1, filename=filename)\n",
        "\n",
        "                lpips_value = calculate_lpips_given_images(group_of_images)\n",
        "                lpips_values.append(lpips_value)\n",
        "\n",
        "            # calculate LPIPS for each task (e.g. cat2dog, dog2cat)\n",
        "            lpips_mean = np.array(lpips_values).mean()\n",
        "            lpips_dict['LPIPS_%s/%s' % (mode, task)] = lpips_mean\n",
        "\n",
        "        # delete dataloaders\n",
        "        del loader_src\n",
        "        if mode == 'reference':\n",
        "            del loader_ref\n",
        "            del iter_ref\n",
        "\n",
        "    # calculate the average LPIPS for all tasks\n",
        "    lpips_mean = 0\n",
        "    for _, value in lpips_dict.items():\n",
        "        lpips_mean += value / len(lpips_dict)\n",
        "    lpips_dict['LPIPS_%s/mean' % mode] = lpips_mean\n",
        "\n",
        "    # report LPIPS values\n",
        "    filename = os.path.join(args.eval_dir, 'LPIPS_%.5i_%s.json' % (step, mode))\n",
        "    utils.save_json(lpips_dict, filename)\n",
        "\n",
        "    # calculate and report fid values\n",
        "    calculate_fid_for_all_tasks(args, domains, step=step, mode=mode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hncK6ei_BblN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_fid_for_all_tasks(args, domains, step, mode):\n",
        "    print('Calculating FID for all tasks...')\n",
        "    fid_values = OrderedDict()\n",
        "    for trg_domain in domains:\n",
        "        src_domains = [x for x in domains if x != trg_domain]\n",
        "\n",
        "        for src_domain in src_domains:\n",
        "            task = '%s2%s' % (src_domain, trg_domain)\n",
        "            path_real = os.path.join(args.train_img_dir, trg_domain)\n",
        "            path_fake = os.path.join(args.eval_dir, task)\n",
        "            print('Calculating FID for %s...' % task)\n",
        "            fid_value = calculate_fid_given_paths(\n",
        "                paths=[path_real, path_fake],\n",
        "                img_size=args.img_size,\n",
        "                batch_size=args.val_batch_size)\n",
        "            fid_values['FID_%s/%s' % (mode, task)] = fid_value\n",
        "\n",
        "    # calculate the average FID for all tasks\n",
        "    fid_mean = 0\n",
        "    for _, value in fid_values.items():\n",
        "        fid_mean += value / len(fid_values)\n",
        "    fid_values['FID_%s/mean' % mode] = fid_mean\n",
        "\n",
        "    # report FID values\n",
        "    filename = os.path.join(args.eval_dir, 'FID_%.5i_%s.json' % (step, mode))\n",
        "    utils.save_json(fid_values, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGkXuBMPBtGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def frechet_distance(mu, cov, mu2, cov2):\n",
        "    cc, _ = linalg.sqrtm(np.dot(cov, cov2), disp=False)\n",
        "    dist = np.sum((mu -mu2)**2) + np.trace(cov + cov2 - 2*cc)\n",
        "    return np.real(dist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoG6imvnB7ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@torch.no_grad()\n",
        "def calculate_fid_given_paths(paths, img_size=256, batch_size=50):\n",
        "    print('Calculating FID given paths %s and %s...' % (paths[0], paths[1]))\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    inception = InceptionV3().eval().to(device)\n",
        "    loaders = [get_eval_loader(path, img_size, batch_size) for path in paths]\n",
        "\n",
        "    mu, cov = [], []\n",
        "    for loader in loaders:\n",
        "        actvs = []\n",
        "        for x in tqdm(loader, total=len(loader)):\n",
        "            actv = inception(x.to(device))\n",
        "            actvs.append(actv)\n",
        "        actvs = torch.cat(actvs, dim=0).cpu().detach().numpy()\n",
        "        mu.append(np.mean(actvs, axis=0))\n",
        "        cov.append(np.cov(actvs, rowvar=False))\n",
        "    fid_value = frechet_distance(mu[0], cov[0], mu[1], cov[1])\n",
        "    return fid_value\n",
        "\n",
        "#Code from metrics/fid\n",
        "if __name__ == '__main__':\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--paths', type=str, nargs=2, help='paths to real and fake images')\n",
        "  parser.add_argument('--img_size', type=int, default=256, help='image resolution')\n",
        "  parser.add_argument('--batch_size', type=int, default=64, help='batch size to use')\n",
        "  args = parser.parse_args()\n",
        "  fid_value = calculate_fid_given_paths(args.paths, args.img_size, args.batch_size)\n",
        "  print('FID: ', fid_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uoa6mxTat9yG",
        "colab_type": "text"
      },
      "source": [
        "# Generator Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPDjTlC-tEfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, img_size=256, style_dim=64, max_conv_dim=512, w_hpf=1):\n",
        "        super().__init__()\n",
        "        dim_in = 2**14 // img_size\n",
        "        self.img_size = img_size\n",
        "        self.from_rgb = nn.Conv2d(3, dim_in, 3, 1, 1)\n",
        "        self.encode = nn.ModuleList()\n",
        "        self.decode = nn.ModuleList()\n",
        "        self.to_rgb = nn.Sequential(\n",
        "            nn.InstanceNorm2d(dim_in, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(dim_in, 3, 1, 1, 0))\n",
        "\n",
        "        # down/up-sampling blocks\n",
        "        repeat_num = int(np.log2(img_size)) - 4\n",
        "        if w_hpf > 0:\n",
        "            repeat_num += 1\n",
        "        for _ in range(repeat_num):\n",
        "            dim_out = min(dim_in*2, max_conv_dim)\n",
        "            self.encode.append(\n",
        "                ResBlk(dim_in, dim_out, normalize=True, downsample=True))\n",
        "            self.decode.insert(\n",
        "                0, AdainResBlk(dim_out, dim_in, style_dim,\n",
        "                               w_hpf=w_hpf, upsample=True))  # stack-like\n",
        "            dim_in = dim_out\n",
        "\n",
        "        # bottleneck blocks\n",
        "        for _ in range(2):\n",
        "            self.encode.append(\n",
        "                ResBlk(dim_out, dim_out, normalize=True))\n",
        "            self.decode.insert(\n",
        "                0, AdainResBlk(dim_out, dim_out, style_dim, w_hpf=w_hpf))\n",
        "\n",
        "        if w_hpf > 0:\n",
        "            device = torch.device(\n",
        "                'cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            self.hpf = HighPass(w_hpf, device)\n",
        "\n",
        "    def forward(self, x, s, masks=None):\n",
        "        x = self.from_rgb(x)\n",
        "        cache = {}\n",
        "        for block in self.encode:\n",
        "            if (masks is not None) and (x.size(2) in [32, 64, 128]):\n",
        "                cache[x.size(2)] = x\n",
        "            x = block(x)\n",
        "        for block in self.decode:\n",
        "            x = block(x, s)\n",
        "            if (masks is not None) and (x.size(2) in [32, 64, 128]):\n",
        "                mask = masks[0] if x.size(2) in [32] else masks[1]\n",
        "                mask = F.interpolate(mask, size=x.size(2), mode='bilinear')\n",
        "                x = x + self.hpf(mask * cache[x.size(2)])\n",
        "        return self.to_rgb(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m1uERTl1Uw3",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxWYCkTM1Tue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Solver(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.nets, self.nets_ema = build_model(args)\n",
        "        # below setattrs are to make networks be children of Solver, e.g., for self.to(self.device)\n",
        "        for name, module in self.nets.items():\n",
        "            utils.print_network(module, name)\n",
        "            setattr(self, name, module)\n",
        "        for name, module in self.nets_ema.items():\n",
        "            setattr(self, name + '_ema', module)\n",
        "\n",
        "        if args.mode == 'train':\n",
        "            self.optims = Munch()\n",
        "            for net in self.nets.keys():\n",
        "                if net == 'fan':\n",
        "                    continue\n",
        "                self.optims[net] = torch.optim.Adam(\n",
        "                    params=self.nets[net].parameters(),\n",
        "                    lr=args.f_lr if net == 'mapping_network' else args.lr,\n",
        "                    betas=[args.beta1, args.beta2],\n",
        "                    weight_decay=args.weight_decay)\n",
        "\n",
        "            self.ckptios = [\n",
        "                CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_nets.ckpt'), **self.nets),\n",
        "                CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_nets_ema.ckpt'), **self.nets_ema),\n",
        "                CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_optims.ckpt'), **self.optims)]\n",
        "        else:\n",
        "            self.ckptios = [CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_nets_ema.ckpt'), **self.nets_ema)]\n",
        "\n",
        "        self.to(self.device)\n",
        "        for name, network in self.named_children():\n",
        "            # Do not initialize the FAN parameters\n",
        "            if ('ema' not in name) and ('fan' not in name):\n",
        "                print('Initializing %s...' % name)\n",
        "                network.apply(utils.he_init)\n",
        "\n",
        "    def _save_checkpoint(self, step):\n",
        "        for ckptio in self.ckptios:\n",
        "            ckptio.save(step)\n",
        "\n",
        "    def _load_checkpoint(self, step):\n",
        "        for ckptio in self.ckptios:\n",
        "            ckptio.load(step)\n",
        "\n",
        "    def _reset_grad(self):\n",
        "        for optim in self.optims.values():\n",
        "            optim.zero_grad()\n",
        "\n",
        "    def train(self, loaders):\n",
        "        args = self.args\n",
        "        nets = self.nets\n",
        "        nets_ema = self.nets_ema\n",
        "        optims = self.optims\n",
        "\n",
        "        # fetch random validation images for debugging\n",
        "        fetcher = InputFetcher(loaders.src, loaders.ref, args.latent_dim, 'train')\n",
        "        fetcher_val = InputFetcher(loaders.val, None, args.latent_dim, 'val')\n",
        "        inputs_val = next(fetcher_val)\n",
        "\n",
        "        # resume training if necessary\n",
        "        if args.resume_iter > 0:\n",
        "            self._load_checkpoint(args.resume_iter)\n",
        "\n",
        "        # remember the initial value of ds weight\n",
        "        initial_lambda_ds = args.lambda_ds\n",
        "\n",
        "        print('Start training...')\n",
        "        start_time = time.time()\n",
        "        for i in range(args.resume_iter, args.total_iters):\n",
        "            # fetch images and labels\n",
        "            inputs = next(fetcher)\n",
        "            x_real, y_org = inputs.x_src, inputs.y_src\n",
        "            x_ref, x_ref2, y_trg = inputs.x_ref, inputs.x_ref2, inputs.y_ref\n",
        "            z_trg, z_trg2 = inputs.z_trg, inputs.z_trg2\n",
        "\n",
        "            masks = nets.fan.get_heatmap(x_real) if args.w_hpf > 0 else None\n",
        "\n",
        "            # train the discriminator\n",
        "            d_loss, d_losses_latent = compute_d_loss(\n",
        "                nets, args, x_real, y_org, y_trg, z_trg=z_trg, masks=masks)\n",
        "            self._reset_grad()\n",
        "            d_loss.backward()\n",
        "            optims.discriminator.step()\n",
        "\n",
        "            d_loss, d_losses_ref = compute_d_loss(\n",
        "                nets, args, x_real, y_org, y_trg, x_ref=x_ref, masks=masks)\n",
        "            self._reset_grad()\n",
        "            d_loss.backward()\n",
        "            optims.discriminator.step()\n",
        "\n",
        "            # train the generator\n",
        "            g_loss, g_losses_latent = compute_g_loss(\n",
        "                nets, args, x_real, y_org, y_trg, z_trgs=[z_trg, z_trg2], masks=masks)\n",
        "            self._reset_grad()\n",
        "            g_loss.backward()\n",
        "            optims.generator.step()\n",
        "            optims.mapping_network.step()\n",
        "            optims.style_encoder.step()\n",
        "\n",
        "            g_loss, g_losses_ref = compute_g_loss(\n",
        "                nets, args, x_real, y_org, y_trg, x_refs=[x_ref, x_ref2], masks=masks)\n",
        "            self._reset_grad()\n",
        "            g_loss.backward()\n",
        "            optims.generator.step()\n",
        "\n",
        "            # compute moving average of network parameters\n",
        "            moving_average(nets.generator, nets_ema.generator, beta=0.999)\n",
        "            moving_average(nets.mapping_network, nets_ema.mapping_network, beta=0.999)\n",
        "            moving_average(nets.style_encoder, nets_ema.style_encoder, beta=0.999)\n",
        "\n",
        "            # decay weight for diversity sensitive loss\n",
        "            if args.lambda_ds > 0:\n",
        "                args.lambda_ds -= (initial_lambda_ds / args.ds_iter)\n",
        "\n",
        "            # print out log info\n",
        "            if (i+1) % args.print_every == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                elapsed = str(datetime.timedelta(seconds=elapsed))[:-7]\n",
        "                log = \"Elapsed time [%s], Iteration [%i/%i], \" % (elapsed, i+1, args.total_iters)\n",
        "                all_losses = dict()\n",
        "                for loss, prefix in zip([d_losses_latent, d_losses_ref, g_losses_latent, g_losses_ref],\n",
        "                                        ['D/latent_', 'D/ref_', 'G/latent_', 'G/ref_']):\n",
        "                    for key, value in loss.items():\n",
        "                        all_losses[prefix + key] = value\n",
        "                all_losses['G/lambda_ds'] = args.lambda_ds\n",
        "                log += ' '.join(['%s: [%.4f]' % (key, value) for key, value in all_losses.items()])\n",
        "                print(log)\n",
        "\n",
        "            # generate images for debugging\n",
        "            if (i+1) % args.sample_every == 0:\n",
        "                os.makedirs(args.sample_dir, exist_ok=True)\n",
        "                utils.debug_image(nets_ema, args, inputs=inputs_val, step=i+1)\n",
        "\n",
        "            # save model checkpoints\n",
        "            if (i+1) % args.save_every == 0:\n",
        "                self._save_checkpoint(step=i+1)\n",
        "\n",
        "            # compute FID and LPIPS if necessary\n",
        "            if (i+1) % args.eval_every == 0:\n",
        "                calculate_metrics(nets_ema, args, i+1, mode='latent')\n",
        "                calculate_metrics(nets_ema, args, i+1, mode='reference')\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, loaders):\n",
        "        args = self.args\n",
        "        nets_ema = self.nets_ema\n",
        "        os.makedirs(args.result_dir, exist_ok=True)\n",
        "        self._load_checkpoint(args.resume_iter)\n",
        "\n",
        "        src = next(InputFetcher(loaders.src, None, args.latent_dim, 'test'))\n",
        "        ref = next(InputFetcher(loaders.ref, None, args.latent_dim, 'test'))\n",
        "\n",
        "        fname = ospj(args.result_dir, 'reference.jpg')\n",
        "        print('Working on {}...'.format(fname))\n",
        "        utils.translate_using_reference(nets_ema, args, src.x, ref.x, ref.y, fname)\n",
        "\n",
        "        fname = ospj(args.result_dir, 'video_ref.mp4')\n",
        "        print('Working on {}...'.format(fname))\n",
        "        utils.video_ref(nets_ema, args, src.x, ref.x, ref.y, fname)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self):\n",
        "        args = self.args\n",
        "        nets_ema = self.nets_ema\n",
        "        resume_iter = args.resume_iter\n",
        "        self._load_checkpoint(args.resume_iter)\n",
        "        calculate_metrics(nets_ema, args, step=resume_iter, mode='latent')\n",
        "        calculate_metrics(nets_ema, args, step=resume_iter, mode='reference')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_SI_lre1vnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_d_loss(nets, args, x_real, y_org, y_trg, z_trg=None, x_ref=None, masks=None):\n",
        "    assert (z_trg is None) != (x_ref is None)\n",
        "    # with real images\n",
        "    x_real.requires_grad_()\n",
        "    out = nets.discriminator(x_real, y_org)\n",
        "    loss_real = adv_loss(out, 1)\n",
        "    loss_reg = r1_reg(out, x_real)\n",
        "\n",
        "    # with fake images\n",
        "    with torch.no_grad():\n",
        "        if z_trg is not None:\n",
        "            s_trg = nets.mapping_network(z_trg, y_trg)\n",
        "        else:  # x_ref is not None\n",
        "            s_trg = nets.style_encoder(x_ref, y_trg)\n",
        "\n",
        "        x_fake = nets.generator(x_real, s_trg, masks=masks)\n",
        "    out = nets.discriminator(x_fake, y_trg)\n",
        "    loss_fake = adv_loss(out, 0)\n",
        "\n",
        "    loss = loss_real + loss_fake + args.lambda_reg * loss_reg\n",
        "    return loss, Munch(real=loss_real.item(),\n",
        "                       fake=loss_fake.item(),\n",
        "                       reg=loss_reg.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGq6uInu1sYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_g_loss(nets, args, x_real, y_org, y_trg, z_trgs=None, x_refs=None, masks=None):\n",
        "    assert (z_trgs is None) != (x_refs is None)\n",
        "    if z_trgs is not None:\n",
        "        z_trg, z_trg2 = z_trgs\n",
        "    if x_refs is not None:\n",
        "        x_ref, x_ref2 = x_refs\n",
        "\n",
        "    # adversarial loss\n",
        "    if z_trgs is not None:\n",
        "        s_trg = nets.mapping_network(z_trg, y_trg)\n",
        "    else:\n",
        "        s_trg = nets.style_encoder(x_ref, y_trg)\n",
        "\n",
        "    x_fake = nets.generator(x_real, s_trg, masks=masks)\n",
        "    out = nets.discriminator(x_fake, y_trg)\n",
        "    loss_adv = adv_loss(out, 1)\n",
        "\n",
        "    # style reconstruction loss\n",
        "    s_pred = nets.style_encoder(x_fake, y_trg)\n",
        "    loss_sty = torch.mean(torch.abs(s_pred - s_trg))\n",
        "\n",
        "    # diversity sensitive loss\n",
        "    if z_trgs is not None:\n",
        "        s_trg2 = nets.mapping_network(z_trg2, y_trg)\n",
        "    else:\n",
        "        s_trg2 = nets.style_encoder(x_ref2, y_trg)\n",
        "    x_fake2 = nets.generator(x_real, s_trg2, masks=masks)\n",
        "    x_fake2 = x_fake2.detach()\n",
        "    loss_ds = torch.mean(torch.abs(x_fake - x_fake2))\n",
        "\n",
        "    # cycle-consistency loss\n",
        "    masks = nets.fan.get_heatmap(x_fake) if args.w_hpf > 0 else None\n",
        "    s_org = nets.style_encoder(x_real, y_org)\n",
        "    x_rec = nets.generator(x_fake, s_org, masks=masks)\n",
        "    loss_cyc = torch.mean(torch.abs(x_rec - x_real))\n",
        "\n",
        "    loss = loss_adv + args.lambda_sty * loss_sty \\\n",
        "        - args.lambda_ds * loss_ds + args.lambda_cyc * loss_cyc\n",
        "    return loss, Munch(adv=loss_adv.item(),\n",
        "                       sty=loss_sty.item(),\n",
        "                       ds=loss_ds.item(),\n",
        "                       cyc=loss_cyc.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3xMe__p1jEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def moving_average(model, model_test, beta=0.999):\n",
        "    for param, param_test in zip(model.parameters(), model_test.parameters()):\n",
        "        param_test.data = torch.lerp(param.data, param_test.data, beta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu6M89-Z1mNn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def adv_loss(logits, target):\n",
        "    assert target in [1, 0]\n",
        "    targets = torch.full_like(logits, fill_value=target)\n",
        "    loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8jSdbUL1ok1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def r1_reg(d_out, x_in):\n",
        "    # zero-centered gradient penalty for real images\n",
        "    batch_size = x_in.size(0)\n",
        "    grad_dout = torch.autograd.grad(\n",
        "        outputs=d_out.sum(), inputs=x_in,\n",
        "        create_graph=True, retain_graph=True, only_inputs=True\n",
        "    )[0]\n",
        "    grad_dout2 = grad_dout.pow(2)\n",
        "    assert(grad_dout2.size() == x_in.size())\n",
        "    reg = 0.5 * grad_dout2.view(batch_size, -1).sum(1).mean(0)\n",
        "    return reg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW4GAxZX0Jg5",
        "colab_type": "text"
      },
      "source": [
        "# Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSbDCEvi0M9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(args):\n",
        "    generator = Generator(args.img_size, args.style_dim, w_hpf=args.w_hpf)\n",
        "    mapping_network = MappingNetwork(args.latent_dim, args.style_dim, args.num_domains)\n",
        "    style_encoder = StyleEncoder(args.img_size, args.style_dim, args.num_domains)\n",
        "    discriminator = Discriminator(args.img_size, args.num_domains)\n",
        "    generator_ema = copy.deepcopy(generator)\n",
        "    mapping_network_ema = copy.deepcopy(mapping_network)\n",
        "    style_encoder_ema = copy.deepcopy(style_encoder)\n",
        "\n",
        "    nets = Munch(generator=generator,\n",
        "                 mapping_network=mapping_network,\n",
        "                 style_encoder=style_encoder,\n",
        "                 discriminator=discriminator)\n",
        "    nets_ema = Munch(generator=generator_ema,\n",
        "                     mapping_network=mapping_network_ema,\n",
        "                     style_encoder=style_encoder_ema)\n",
        "\n",
        "    if args.w_hpf > 0:\n",
        "        fan = FAN(fname_pretrained=args.wing_path).eval()\n",
        "        nets.fan = fan\n",
        "        nets_ema.fan = fan\n",
        "\n",
        "    return nets, nets_ema"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3oi71uV_DkJ",
        "colab_type": "text"
      },
      "source": [
        "# Predicting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oktld0_V_GBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_preds_fromhm(hm):\n",
        "    max, idx = torch.max(\n",
        "        hm.view(hm.size(0), hm.size(1), hm.size(2) * hm.size(3)), 2)\n",
        "    idx += 1\n",
        "    preds = idx.view(idx.size(0), idx.size(1), 1).repeat(1, 1, 2).float()\n",
        "    preds[..., 0].apply_(lambda x: (x - 1) % hm.size(3) + 1)\n",
        "    preds[..., 1].add_(-1).div_(hm.size(2)).floor_().add_(1)\n",
        "\n",
        "    for i in range(preds.size(0)):\n",
        "        for j in range(preds.size(1)):\n",
        "            hm_ = hm[i, j, :]\n",
        "            pX, pY = int(preds[i, j, 0]) - 1, int(preds[i, j, 1]) - 1\n",
        "            if pX > 0 and pX < 63 and pY > 0 and pY < 63:\n",
        "                diff = torch.FloatTensor(\n",
        "                    [hm_[pY, pX + 1] - hm_[pY, pX - 1],\n",
        "                     hm_[pY + 1, pX] - hm_[pY - 1, pX]])\n",
        "                preds[i, j].add_(diff.sign_().mul_(.25))\n",
        "\n",
        "    preds.add_(-0.5)\n",
        "    return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nu7JhCvuCP-",
        "colab_type": "text"
      },
      "source": [
        "# Extras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smFIHI3lws14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HighPass(nn.Module):\n",
        "    def __init__(self, w_hpf, device):\n",
        "        super(HighPass, self).__init__()\n",
        "        self.filter = keras.tensor([[-1, -1, -1],\n",
        "                                    [-1, 8., -1],\n",
        "                                    [-1, -1, -1]]).to(device) / w_hpf\n",
        "\n",
        "    def forward(self, x):\n",
        "        filter = self.filter.unsqueeze(0).unsqueeze(1).repeat(x.size(1), 1, 1, 1)\n",
        "        return keras.Functional.conv2d(x, filter, padding=1, groups=x.size(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrSfbzrHHyIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_real_samples(dataset, n_samples):\n",
        "\t# choose random instances\n",
        "\tix = randint(0, dataset.shape[0], n_samples)\n",
        "\t# retrieve selected images\n",
        "\tX = dataset[ix]\n",
        "\t# generate 'real' class labels (1)\n",
        "\ty = ones((n_samples, 1))\n",
        "\treturn X, y"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Lp-F7QZH2yk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_fake_samples(n_samples):\n",
        "\t# generate uniform random numbers in [0,1]\n",
        "\tX = rand(28 * 28 * n_samples)\n",
        "\t# reshape into a batch of grayscale images\n",
        "\tX = X.reshape((n_samples, 28, 28, 1))\n",
        "\t# generate 'fake' class labels (0)\n",
        "\ty = zeros((n_samples, 1))\n",
        "\treturn X, y"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7X675UKIWv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_discriminator(model, dataset, n_iter=100, n_batch=256):\n",
        "\thalf_batch = int(n_batch / 2)\n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(n_iter):\n",
        "\t\t# get randomly selected 'real' samples\n",
        "\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "\t\t# update discriminator on real samples\n",
        "\t\t_, real_acc = model.train_on_batch(X_real, y_real)\n",
        "\t\t# generate 'fake' examples\n",
        "\t\tX_fake, y_fake = generate_fake_samples(half_batch)\n",
        "\t\t# update discriminator on fake samples\n",
        "\t\t_, fake_acc = model.train_on_batch(X_fake, y_fake)\n",
        "\t\t# summarize performance\n",
        "\t\tprint('>%d real=%.0f%% fake=%.0f%%' % (i+1, real_acc*100, fake_acc*100))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf1gD1WBIu7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_generator(latent_dim):\n",
        "\tmodel = Sequential()\n",
        "\t# foundation for 7x7 image\n",
        "\tn_nodes = 128 * 7 * 7\n",
        "\tmodel.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Reshape((7, 7, 128)))\n",
        "\t# upsample to 14x14\n",
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t# upsample to 28x28\n",
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Conv2D(1, (7,7), activation='sigmoid', padding='same'))\n",
        "\treturn model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reAhKr-UI2p2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn x_input"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1LJws48I4-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
        "\t# generate points in latent space\n",
        "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
        "\t# predict outputs\n",
        "\tX = g_model.predict(x_input)\n",
        "\t# create 'fake' class labels (0)\n",
        "\ty = zeros((n_samples, 1))\n",
        "\treturn X, y"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-jkwEERJOh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_gan(g_model, d_model):\n",
        "\t# make weights in the discriminator not trainable\n",
        "\td_model.trainable = False\n",
        "\t# connect them\n",
        "\tmodel = Sequential()\n",
        "\t# add generator\n",
        "\tmodel.add(g_model)\n",
        "\t# add the discriminator\n",
        "\tmodel.add(d_model)\n",
        "\t# compile model\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\treturn model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGuZDfmjJdnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_gan(gan_model, latent_dim, n_epochs=100, n_batch=256):\n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(n_epochs):\n",
        "\t\t# prepare points in latent space as input for the generator\n",
        "\t\tx_gan = generate_latent_points(latent_dim, n_batch)\n",
        "\t\t# create inverted labels for the fake samples\n",
        "\t\ty_gan = ones((n_batch, 1))\n",
        "\t\t# update the generator via the discriminator's error\n",
        "\t\tgan_model.train_on_batch(x_gan, y_gan)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soe9r3JJJrKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=100):\n",
        "\t# prepare real samples\n",
        "\tX_real, y_real = generate_real_samples(dataset, n_samples)\n",
        "\t# evaluate discriminator on real examples\n",
        "\t_, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
        "\t# prepare fake examples\n",
        "\tx_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "\t# evaluate discriminator on fake examples\n",
        "\t_, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
        "\t# summarize discriminator performance\n",
        "\tprint('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
        "\tsave_plot(x_fake, epoch)\n",
        "\t# save the generator model tile file\n",
        "\tfilename = 'generator_model_%03d.h5' % (epoch + 1)\n",
        "\tg_model.save(filename)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFYiEn23JnNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=256):\n",
        "\tbat_per_epo = int(dataset.shape[0] / n_batch)\n",
        "\thalf_batch = int(n_batch / 2)\n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(n_epochs):\n",
        "\t\t# enumerate batches over the training set\n",
        "\t\tfor j in range(bat_per_epo):\n",
        "\t\t\t# get randomly selected 'real' samples\n",
        "\t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "\t\t\t# generate 'fake' examples\n",
        "\t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "\t\t\t# create training set for the discriminator\n",
        "\t\t\tX, y = vstack((X_real, X_fake)), vstack((y_real, y_fake))\n",
        "\t\t\t# update discriminator model weights\n",
        "\t\t\td_loss, _ = d_model.train_on_batch(X, y)\n",
        "\t\t\t# prepare points in latent space as input for the generator\n",
        "\t\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n",
        "\t\t\t# create inverted labels for the fake samples\n",
        "\t\t\ty_gan = ones((n_batch, 1))\n",
        "\t\t\t# update the generator via the discriminator's error\n",
        "\t\t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "\t\t\t# summarize loss on this batch\n",
        "\t\t\tprint('>%d, %d/%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_loss, g_loss))\n",
        "\t\t\tif (i+1) % 10 == 0:\n",
        "\t\t\t\tsummarize_performance(i, g_model, d_model, dataset, latent_dim)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXMo9-GAKGl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_plot(examples, epoch, n=10):\n",
        "\t# plot images\n",
        "\tfor i in range(n * n):\n",
        "\t\t# define subplot\n",
        "\t\tpyplot.subplot(n, n, 1 + i)\n",
        "\t\t# turn off axis\n",
        "\t\tpyplot.axis('off')\n",
        "\t\t# plot raw pixel data\n",
        "\t\tpyplot.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
        "\t# save plot to file\n",
        "\tfilename = 'generated_plot_e%03d.png' % (epoch+1)\n",
        "\tpyplot.savefig(filename)\n",
        "\tpyplot.close()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5TtRS3GI7E7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "edee60d1-355b-4500-ba98-88e2b383626d"
      },
      "source": [
        "latent_dim = 100\n",
        "# create the discriminator\n",
        "d_model = define_discriminator()\n",
        "# create the generator\n",
        "g_model = define_generator(latent_dim)\n",
        "# create the gan\n",
        "gan_model = define_gan(g_model, d_model)\n",
        "# load image data\n",
        "dataset = load_real_samples()\n",
        "# train model\n",
        "train(g_model, d_model, gan_model, dataset, latent_dim)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n",
            ">81, 150/234, d=0.689, g=0.689\n",
            ">81, 151/234, d=0.684, g=0.681\n",
            ">81, 152/234, d=0.696, g=0.707\n",
            ">81, 153/234, d=0.700, g=0.726\n",
            ">81, 154/234, d=0.689, g=0.737\n",
            ">81, 155/234, d=0.689, g=0.720\n",
            ">81, 156/234, d=0.688, g=0.710\n",
            ">81, 157/234, d=0.690, g=0.679\n",
            ">81, 158/234, d=0.691, g=0.682\n",
            ">81, 159/234, d=0.689, g=0.684\n",
            ">81, 160/234, d=0.695, g=0.696\n",
            ">81, 161/234, d=0.694, g=0.718\n",
            ">81, 162/234, d=0.691, g=0.717\n",
            ">81, 163/234, d=0.691, g=0.693\n",
            ">81, 164/234, d=0.693, g=0.679\n",
            ">81, 165/234, d=0.694, g=0.707\n",
            ">81, 166/234, d=0.696, g=0.717\n",
            ">81, 167/234, d=0.689, g=0.719\n",
            ">81, 168/234, d=0.689, g=0.709\n",
            ">81, 169/234, d=0.692, g=0.691\n",
            ">81, 170/234, d=0.691, g=0.687\n",
            ">81, 171/234, d=0.692, g=0.689\n",
            ">81, 172/234, d=0.690, g=0.700\n",
            ">81, 173/234, d=0.685, g=0.713\n",
            ">81, 174/234, d=0.689, g=0.715\n",
            ">81, 175/234, d=0.692, g=0.697\n",
            ">81, 176/234, d=0.694, g=0.680\n",
            ">81, 177/234, d=0.702, g=0.703\n",
            ">81, 178/234, d=0.694, g=0.705\n",
            ">81, 179/234, d=0.682, g=0.707\n",
            ">81, 180/234, d=0.691, g=0.707\n",
            ">81, 181/234, d=0.690, g=0.694\n",
            ">81, 182/234, d=0.692, g=0.686\n",
            ">81, 183/234, d=0.687, g=0.693\n",
            ">81, 184/234, d=0.687, g=0.707\n",
            ">81, 185/234, d=0.699, g=0.739\n",
            ">81, 186/234, d=0.687, g=0.710\n",
            ">81, 187/234, d=0.697, g=0.702\n",
            ">81, 188/234, d=0.692, g=0.667\n",
            ">81, 189/234, d=0.690, g=0.675\n",
            ">81, 190/234, d=0.692, g=0.697\n",
            ">81, 191/234, d=0.688, g=0.718\n",
            ">81, 192/234, d=0.695, g=0.708\n",
            ">81, 193/234, d=0.694, g=0.689\n",
            ">81, 194/234, d=0.689, g=0.689\n",
            ">81, 195/234, d=0.693, g=0.700\n",
            ">81, 196/234, d=0.689, g=0.708\n",
            ">81, 197/234, d=0.682, g=0.702\n",
            ">81, 198/234, d=0.686, g=0.693\n",
            ">81, 199/234, d=0.691, g=0.698\n",
            ">81, 200/234, d=0.695, g=0.695\n",
            ">81, 201/234, d=0.687, g=0.698\n",
            ">81, 202/234, d=0.690, g=0.698\n",
            ">81, 203/234, d=0.690, g=0.696\n",
            ">81, 204/234, d=0.695, g=0.702\n",
            ">81, 205/234, d=0.690, g=0.708\n",
            ">81, 206/234, d=0.687, g=0.702\n",
            ">81, 207/234, d=0.690, g=0.701\n",
            ">81, 208/234, d=0.696, g=0.704\n",
            ">81, 209/234, d=0.687, g=0.716\n",
            ">81, 210/234, d=0.693, g=0.697\n",
            ">81, 211/234, d=0.692, g=0.685\n",
            ">81, 212/234, d=0.687, g=0.685\n",
            ">81, 213/234, d=0.696, g=0.707\n",
            ">81, 214/234, d=0.690, g=0.715\n",
            ">81, 215/234, d=0.690, g=0.711\n",
            ">81, 216/234, d=0.693, g=0.691\n",
            ">81, 217/234, d=0.692, g=0.684\n",
            ">81, 218/234, d=0.691, g=0.697\n",
            ">81, 219/234, d=0.693, g=0.700\n",
            ">81, 220/234, d=0.694, g=0.699\n",
            ">81, 221/234, d=0.688, g=0.684\n",
            ">81, 222/234, d=0.692, g=0.695\n",
            ">81, 223/234, d=0.684, g=0.712\n",
            ">81, 224/234, d=0.693, g=0.728\n",
            ">81, 225/234, d=0.693, g=0.716\n",
            ">81, 226/234, d=0.691, g=0.709\n",
            ">81, 227/234, d=0.680, g=0.693\n",
            ">81, 228/234, d=0.690, g=0.701\n",
            ">81, 229/234, d=0.682, g=0.695\n",
            ">81, 230/234, d=0.688, g=0.692\n",
            ">81, 231/234, d=0.699, g=0.696\n",
            ">81, 232/234, d=0.692, g=0.712\n",
            ">81, 233/234, d=0.691, g=0.715\n",
            ">81, 234/234, d=0.698, g=0.708\n",
            ">82, 1/234, d=0.694, g=0.696\n",
            ">82, 2/234, d=0.698, g=0.681\n",
            ">82, 3/234, d=0.693, g=0.681\n",
            ">82, 4/234, d=0.694, g=0.715\n",
            ">82, 5/234, d=0.693, g=0.719\n",
            ">82, 6/234, d=0.690, g=0.707\n",
            ">82, 7/234, d=0.689, g=0.691\n",
            ">82, 8/234, d=0.686, g=0.684\n",
            ">82, 9/234, d=0.696, g=0.680\n",
            ">82, 10/234, d=0.692, g=0.691\n",
            ">82, 11/234, d=0.690, g=0.722\n",
            ">82, 12/234, d=0.689, g=0.731\n",
            ">82, 13/234, d=0.699, g=0.705\n",
            ">82, 14/234, d=0.688, g=0.688\n",
            ">82, 15/234, d=0.689, g=0.688\n",
            ">82, 16/234, d=0.691, g=0.708\n",
            ">82, 17/234, d=0.693, g=0.714\n",
            ">82, 18/234, d=0.688, g=0.728\n",
            ">82, 19/234, d=0.688, g=0.701\n",
            ">82, 20/234, d=0.692, g=0.685\n",
            ">82, 21/234, d=0.686, g=0.672\n",
            ">82, 22/234, d=0.700, g=0.695\n",
            ">82, 23/234, d=0.690, g=0.713\n",
            ">82, 24/234, d=0.691, g=0.724\n",
            ">82, 25/234, d=0.698, g=0.696\n",
            ">82, 26/234, d=0.692, g=0.678\n",
            ">82, 27/234, d=0.692, g=0.695\n",
            ">82, 28/234, d=0.690, g=0.699\n",
            ">82, 29/234, d=0.690, g=0.693\n",
            ">82, 30/234, d=0.695, g=0.687\n",
            ">82, 31/234, d=0.690, g=0.693\n",
            ">82, 32/234, d=0.692, g=0.719\n",
            ">82, 33/234, d=0.687, g=0.719\n",
            ">82, 34/234, d=0.688, g=0.690\n",
            ">82, 35/234, d=0.695, g=0.668\n",
            ">82, 36/234, d=0.690, g=0.689\n",
            ">82, 37/234, d=0.696, g=0.701\n",
            ">82, 38/234, d=0.690, g=0.711\n",
            ">82, 39/234, d=0.693, g=0.698\n",
            ">82, 40/234, d=0.692, g=0.691\n",
            ">82, 41/234, d=0.693, g=0.688\n",
            ">82, 42/234, d=0.691, g=0.691\n",
            ">82, 43/234, d=0.694, g=0.720\n",
            ">82, 44/234, d=0.687, g=0.720\n",
            ">82, 45/234, d=0.690, g=0.713\n",
            ">82, 46/234, d=0.692, g=0.689\n",
            ">82, 47/234, d=0.694, g=0.680\n",
            ">82, 48/234, d=0.695, g=0.668\n",
            ">82, 49/234, d=0.689, g=0.675\n",
            ">82, 50/234, d=0.692, g=0.693\n",
            ">82, 51/234, d=0.699, g=0.710\n",
            ">82, 52/234, d=0.690, g=0.717\n",
            ">82, 53/234, d=0.690, g=0.698\n",
            ">82, 54/234, d=0.694, g=0.692\n",
            ">82, 55/234, d=0.690, g=0.706\n",
            ">82, 56/234, d=0.691, g=0.702\n",
            ">82, 57/234, d=0.689, g=0.719\n",
            ">82, 58/234, d=0.686, g=0.728\n",
            ">82, 59/234, d=0.692, g=0.703\n",
            ">82, 60/234, d=0.699, g=0.694\n",
            ">82, 61/234, d=0.699, g=0.689\n",
            ">82, 62/234, d=0.686, g=0.689\n",
            ">82, 63/234, d=0.687, g=0.697\n",
            ">82, 64/234, d=0.692, g=0.698\n",
            ">82, 65/234, d=0.691, g=0.696\n",
            ">82, 66/234, d=0.690, g=0.684\n",
            ">82, 67/234, d=0.697, g=0.691\n",
            ">82, 68/234, d=0.693, g=0.719\n",
            ">82, 69/234, d=0.689, g=0.735\n",
            ">82, 70/234, d=0.690, g=0.740\n",
            ">82, 71/234, d=0.693, g=0.703\n",
            ">82, 72/234, d=0.692, g=0.669\n",
            ">82, 73/234, d=0.703, g=0.660\n",
            ">82, 74/234, d=0.692, g=0.669\n",
            ">82, 75/234, d=0.695, g=0.705\n",
            ">82, 76/234, d=0.689, g=0.719\n",
            ">82, 77/234, d=0.687, g=0.731\n",
            ">82, 78/234, d=0.688, g=0.724\n",
            ">82, 79/234, d=0.692, g=0.699\n",
            ">82, 80/234, d=0.687, g=0.682\n",
            ">82, 81/234, d=0.687, g=0.665\n",
            ">82, 82/234, d=0.692, g=0.687\n",
            ">82, 83/234, d=0.692, g=0.712\n",
            ">82, 84/234, d=0.692, g=0.733\n",
            ">82, 85/234, d=0.691, g=0.717\n",
            ">82, 86/234, d=0.686, g=0.689\n",
            ">82, 87/234, d=0.690, g=0.668\n",
            ">82, 88/234, d=0.687, g=0.694\n",
            ">82, 89/234, d=0.693, g=0.715\n",
            ">82, 90/234, d=0.687, g=0.717\n",
            ">82, 91/234, d=0.689, g=0.707\n",
            ">82, 92/234, d=0.694, g=0.689\n",
            ">82, 93/234, d=0.693, g=0.675\n",
            ">82, 94/234, d=0.693, g=0.687\n",
            ">82, 95/234, d=0.694, g=0.707\n",
            ">82, 96/234, d=0.686, g=0.711\n",
            ">82, 97/234, d=0.691, g=0.698\n",
            ">82, 98/234, d=0.694, g=0.699\n",
            ">82, 99/234, d=0.697, g=0.700\n",
            ">82, 100/234, d=0.693, g=0.684\n",
            ">82, 101/234, d=0.690, g=0.698\n",
            ">82, 102/234, d=0.691, g=0.703\n",
            ">82, 103/234, d=0.693, g=0.694\n",
            ">82, 104/234, d=0.690, g=0.700\n",
            ">82, 105/234, d=0.697, g=0.709\n",
            ">82, 106/234, d=0.691, g=0.704\n",
            ">82, 107/234, d=0.691, g=0.693\n",
            ">82, 108/234, d=0.687, g=0.682\n",
            ">82, 109/234, d=0.687, g=0.675\n",
            ">82, 110/234, d=0.688, g=0.693\n",
            ">82, 111/234, d=0.694, g=0.707\n",
            ">82, 112/234, d=0.692, g=0.718\n",
            ">82, 113/234, d=0.690, g=0.705\n",
            ">82, 114/234, d=0.691, g=0.689\n",
            ">82, 115/234, d=0.688, g=0.703\n",
            ">82, 116/234, d=0.690, g=0.720\n",
            ">82, 117/234, d=0.695, g=0.716\n",
            ">82, 118/234, d=0.691, g=0.706\n",
            ">82, 119/234, d=0.691, g=0.707\n",
            ">82, 120/234, d=0.693, g=0.687\n",
            ">82, 121/234, d=0.695, g=0.679\n",
            ">82, 122/234, d=0.686, g=0.676\n",
            ">82, 123/234, d=0.695, g=0.698\n",
            ">82, 124/234, d=0.691, g=0.726\n",
            ">82, 125/234, d=0.693, g=0.729\n",
            ">82, 126/234, d=0.690, g=0.701\n",
            ">82, 127/234, d=0.691, g=0.678\n",
            ">82, 128/234, d=0.696, g=0.687\n",
            ">82, 129/234, d=0.687, g=0.689\n",
            ">82, 130/234, d=0.688, g=0.714\n",
            ">82, 131/234, d=0.692, g=0.722\n",
            ">82, 132/234, d=0.689, g=0.710\n",
            ">82, 133/234, d=0.687, g=0.698\n",
            ">82, 134/234, d=0.703, g=0.704\n",
            ">82, 135/234, d=0.686, g=0.698\n",
            ">82, 136/234, d=0.695, g=0.699\n",
            ">82, 137/234, d=0.693, g=0.699\n",
            ">82, 138/234, d=0.690, g=0.685\n",
            ">82, 139/234, d=0.690, g=0.688\n",
            ">82, 140/234, d=0.689, g=0.698\n",
            ">82, 141/234, d=0.693, g=0.724\n",
            ">82, 142/234, d=0.689, g=0.724\n",
            ">82, 143/234, d=0.686, g=0.712\n",
            ">82, 144/234, d=0.685, g=0.689\n",
            ">82, 145/234, d=0.688, g=0.689\n",
            ">82, 146/234, d=0.689, g=0.699\n",
            ">82, 147/234, d=0.687, g=0.716\n",
            ">82, 148/234, d=0.690, g=0.719\n",
            ">82, 149/234, d=0.687, g=0.682\n",
            ">82, 150/234, d=0.694, g=0.664\n",
            ">82, 151/234, d=0.694, g=0.682\n",
            ">82, 152/234, d=0.694, g=0.730\n",
            ">82, 153/234, d=0.690, g=0.733\n",
            ">82, 154/234, d=0.684, g=0.723\n",
            ">82, 155/234, d=0.693, g=0.694\n",
            ">82, 156/234, d=0.696, g=0.670\n",
            ">82, 157/234, d=0.692, g=0.690\n",
            ">82, 158/234, d=0.683, g=0.707\n",
            ">82, 159/234, d=0.684, g=0.703\n",
            ">82, 160/234, d=0.691, g=0.707\n",
            ">82, 161/234, d=0.694, g=0.697\n",
            ">82, 162/234, d=0.687, g=0.692\n",
            ">82, 163/234, d=0.687, g=0.683\n",
            ">82, 164/234, d=0.686, g=0.695\n",
            ">82, 165/234, d=0.688, g=0.718\n",
            ">82, 166/234, d=0.683, g=0.721\n",
            ">82, 167/234, d=0.691, g=0.711\n",
            ">82, 168/234, d=0.690, g=0.695\n",
            ">82, 169/234, d=0.686, g=0.676\n",
            ">82, 170/234, d=0.689, g=0.677\n",
            ">82, 171/234, d=0.693, g=0.706\n",
            ">82, 172/234, d=0.687, g=0.706\n",
            ">82, 173/234, d=0.689, g=0.706\n",
            ">82, 174/234, d=0.692, g=0.696\n",
            ">82, 175/234, d=0.687, g=0.685\n",
            ">82, 176/234, d=0.691, g=0.693\n",
            ">82, 177/234, d=0.697, g=0.722\n",
            ">82, 178/234, d=0.692, g=0.739\n",
            ">82, 179/234, d=0.693, g=0.725\n",
            ">82, 180/234, d=0.691, g=0.695\n",
            ">82, 181/234, d=0.693, g=0.674\n",
            ">82, 182/234, d=0.684, g=0.668\n",
            ">82, 183/234, d=0.689, g=0.708\n",
            ">82, 184/234, d=0.689, g=0.726\n",
            ">82, 185/234, d=0.691, g=0.718\n",
            ">82, 186/234, d=0.696, g=0.704\n",
            ">82, 187/234, d=0.691, g=0.687\n",
            ">82, 188/234, d=0.687, g=0.690\n",
            ">82, 189/234, d=0.686, g=0.711\n",
            ">82, 190/234, d=0.695, g=0.703\n",
            ">82, 191/234, d=0.687, g=0.688\n",
            ">82, 192/234, d=0.694, g=0.691\n",
            ">82, 193/234, d=0.690, g=0.704\n",
            ">82, 194/234, d=0.694, g=0.717\n",
            ">82, 195/234, d=0.695, g=0.706\n",
            ">82, 196/234, d=0.694, g=0.696\n",
            ">82, 197/234, d=0.693, g=0.694\n",
            ">82, 198/234, d=0.694, g=0.690\n",
            ">82, 199/234, d=0.690, g=0.697\n",
            ">82, 200/234, d=0.690, g=0.691\n",
            ">82, 201/234, d=0.692, g=0.687\n",
            ">82, 202/234, d=0.689, g=0.694\n",
            ">82, 203/234, d=0.690, g=0.713\n",
            ">82, 204/234, d=0.690, g=0.708\n",
            ">82, 205/234, d=0.696, g=0.711\n",
            ">82, 206/234, d=0.688, g=0.695\n",
            ">82, 207/234, d=0.689, g=0.695\n",
            ">82, 208/234, d=0.686, g=0.692\n",
            ">82, 209/234, d=0.689, g=0.694\n",
            ">82, 210/234, d=0.692, g=0.699\n",
            ">82, 211/234, d=0.688, g=0.722\n",
            ">82, 212/234, d=0.692, g=0.714\n",
            ">82, 213/234, d=0.686, g=0.689\n",
            ">82, 214/234, d=0.689, g=0.671\n",
            ">82, 215/234, d=0.699, g=0.690\n",
            ">82, 216/234, d=0.690, g=0.710\n",
            ">82, 217/234, d=0.696, g=0.721\n",
            ">82, 218/234, d=0.689, g=0.695\n",
            ">82, 219/234, d=0.687, g=0.687\n",
            ">82, 220/234, d=0.693, g=0.685\n",
            ">82, 221/234, d=0.694, g=0.696\n",
            ">82, 222/234, d=0.687, g=0.709\n",
            ">82, 223/234, d=0.695, g=0.704\n",
            ">82, 224/234, d=0.700, g=0.698\n",
            ">82, 225/234, d=0.694, g=0.702\n",
            ">82, 226/234, d=0.690, g=0.715\n",
            ">82, 227/234, d=0.698, g=0.715\n",
            ">82, 228/234, d=0.695, g=0.724\n",
            ">82, 229/234, d=0.692, g=0.721\n",
            ">82, 230/234, d=0.688, g=0.701\n",
            ">82, 231/234, d=0.701, g=0.684\n",
            ">82, 232/234, d=0.694, g=0.677\n",
            ">82, 233/234, d=0.696, g=0.681\n",
            ">82, 234/234, d=0.689, g=0.699\n",
            ">83, 1/234, d=0.692, g=0.723\n",
            ">83, 2/234, d=0.690, g=0.713\n",
            ">83, 3/234, d=0.684, g=0.699\n",
            ">83, 4/234, d=0.691, g=0.700\n",
            ">83, 5/234, d=0.691, g=0.717\n",
            ">83, 6/234, d=0.693, g=0.723\n",
            ">83, 7/234, d=0.691, g=0.700\n",
            ">83, 8/234, d=0.691, g=0.681\n",
            ">83, 9/234, d=0.695, g=0.681\n",
            ">83, 10/234, d=0.695, g=0.674\n",
            ">83, 11/234, d=0.693, g=0.688\n",
            ">83, 12/234, d=0.696, g=0.712\n",
            ">83, 13/234, d=0.685, g=0.716\n",
            ">83, 14/234, d=0.698, g=0.687\n",
            ">83, 15/234, d=0.693, g=0.683\n",
            ">83, 16/234, d=0.690, g=0.681\n",
            ">83, 17/234, d=0.693, g=0.702\n",
            ">83, 18/234, d=0.691, g=0.717\n",
            ">83, 19/234, d=0.686, g=0.727\n",
            ">83, 20/234, d=0.693, g=0.712\n",
            ">83, 21/234, d=0.696, g=0.678\n",
            ">83, 22/234, d=0.687, g=0.668\n",
            ">83, 23/234, d=0.685, g=0.685\n",
            ">83, 24/234, d=0.697, g=0.714\n",
            ">83, 25/234, d=0.689, g=0.714\n",
            ">83, 26/234, d=0.693, g=0.705\n",
            ">83, 27/234, d=0.690, g=0.693\n",
            ">83, 28/234, d=0.691, g=0.698\n",
            ">83, 29/234, d=0.687, g=0.700\n",
            ">83, 30/234, d=0.685, g=0.704\n",
            ">83, 31/234, d=0.693, g=0.713\n",
            ">83, 32/234, d=0.682, g=0.707\n",
            ">83, 33/234, d=0.688, g=0.701\n",
            ">83, 34/234, d=0.696, g=0.692\n",
            ">83, 35/234, d=0.688, g=0.680\n",
            ">83, 36/234, d=0.691, g=0.682\n",
            ">83, 37/234, d=0.682, g=0.688\n",
            ">83, 38/234, d=0.687, g=0.720\n",
            ">83, 39/234, d=0.695, g=0.724\n",
            ">83, 40/234, d=0.697, g=0.701\n",
            ">83, 41/234, d=0.690, g=0.689\n",
            ">83, 42/234, d=0.691, g=0.674\n",
            ">83, 43/234, d=0.695, g=0.677\n",
            ">83, 44/234, d=0.691, g=0.685\n",
            ">83, 45/234, d=0.691, g=0.718\n",
            ">83, 46/234, d=0.697, g=0.740\n",
            ">83, 47/234, d=0.694, g=0.716\n",
            ">83, 48/234, d=0.692, g=0.688\n",
            ">83, 49/234, d=0.694, g=0.670\n",
            ">83, 50/234, d=0.693, g=0.677\n",
            ">83, 51/234, d=0.697, g=0.704\n",
            ">83, 52/234, d=0.697, g=0.737\n",
            ">83, 53/234, d=0.698, g=0.735\n",
            ">83, 54/234, d=0.695, g=0.696\n",
            ">83, 55/234, d=0.691, g=0.684\n",
            ">83, 56/234, d=0.688, g=0.675\n",
            ">83, 57/234, d=0.691, g=0.695\n",
            ">83, 58/234, d=0.688, g=0.723\n",
            ">83, 59/234, d=0.694, g=0.720\n",
            ">83, 60/234, d=0.696, g=0.704\n",
            ">83, 61/234, d=0.688, g=0.681\n",
            ">83, 62/234, d=0.692, g=0.696\n",
            ">83, 63/234, d=0.686, g=0.717\n",
            ">83, 64/234, d=0.689, g=0.706\n",
            ">83, 65/234, d=0.687, g=0.701\n",
            ">83, 66/234, d=0.685, g=0.691\n",
            ">83, 67/234, d=0.690, g=0.687\n",
            ">83, 68/234, d=0.692, g=0.706\n",
            ">83, 69/234, d=0.689, g=0.704\n",
            ">83, 70/234, d=0.688, g=0.705\n",
            ">83, 71/234, d=0.695, g=0.690\n",
            ">83, 72/234, d=0.699, g=0.686\n",
            ">83, 73/234, d=0.697, g=0.691\n",
            ">83, 74/234, d=0.693, g=0.705\n",
            ">83, 75/234, d=0.695, g=0.709\n",
            ">83, 76/234, d=0.689, g=0.702\n",
            ">83, 77/234, d=0.701, g=0.699\n",
            ">83, 78/234, d=0.699, g=0.696\n",
            ">83, 79/234, d=0.696, g=0.706\n",
            ">83, 80/234, d=0.694, g=0.691\n",
            ">83, 81/234, d=0.694, g=0.694\n",
            ">83, 82/234, d=0.688, g=0.704\n",
            ">83, 83/234, d=0.692, g=0.690\n",
            ">83, 84/234, d=0.697, g=0.699\n",
            ">83, 85/234, d=0.690, g=0.711\n",
            ">83, 86/234, d=0.689, g=0.704\n",
            ">83, 87/234, d=0.691, g=0.701\n",
            ">83, 88/234, d=0.700, g=0.695\n",
            ">83, 89/234, d=0.695, g=0.703\n",
            ">83, 90/234, d=0.692, g=0.702\n",
            ">83, 91/234, d=0.687, g=0.704\n",
            ">83, 92/234, d=0.687, g=0.694\n",
            ">83, 93/234, d=0.692, g=0.692\n",
            ">83, 94/234, d=0.697, g=0.694\n",
            ">83, 95/234, d=0.689, g=0.700\n",
            ">83, 96/234, d=0.693, g=0.708\n",
            ">83, 97/234, d=0.690, g=0.704\n",
            ">83, 98/234, d=0.696, g=0.710\n",
            ">83, 99/234, d=0.689, g=0.714\n",
            ">83, 100/234, d=0.695, g=0.718\n",
            ">83, 101/234, d=0.686, g=0.697\n",
            ">83, 102/234, d=0.688, g=0.679\n",
            ">83, 103/234, d=0.695, g=0.678\n",
            ">83, 104/234, d=0.699, g=0.695\n",
            ">83, 105/234, d=0.690, g=0.699\n",
            ">83, 106/234, d=0.687, g=0.710\n",
            ">83, 107/234, d=0.693, g=0.692\n",
            ">83, 108/234, d=0.685, g=0.685\n",
            ">83, 109/234, d=0.693, g=0.692\n",
            ">83, 110/234, d=0.690, g=0.706\n",
            ">83, 111/234, d=0.687, g=0.707\n",
            ">83, 112/234, d=0.691, g=0.704\n",
            ">83, 113/234, d=0.697, g=0.707\n",
            ">83, 114/234, d=0.683, g=0.691\n",
            ">83, 115/234, d=0.687, g=0.684\n",
            ">83, 116/234, d=0.688, g=0.710\n",
            ">83, 117/234, d=0.698, g=0.725\n",
            ">83, 118/234, d=0.683, g=0.719\n",
            ">83, 119/234, d=0.681, g=0.694\n",
            ">83, 120/234, d=0.695, g=0.693\n",
            ">83, 121/234, d=0.695, g=0.681\n",
            ">83, 122/234, d=0.690, g=0.692\n",
            ">83, 123/234, d=0.692, g=0.700\n",
            ">83, 124/234, d=0.690, g=0.701\n",
            ">83, 125/234, d=0.689, g=0.697\n",
            ">83, 126/234, d=0.692, g=0.691\n",
            ">83, 127/234, d=0.689, g=0.691\n",
            ">83, 128/234, d=0.690, g=0.709\n",
            ">83, 129/234, d=0.698, g=0.709\n",
            ">83, 130/234, d=0.697, g=0.701\n",
            ">83, 131/234, d=0.691, g=0.692\n",
            ">83, 132/234, d=0.692, g=0.685\n",
            ">83, 133/234, d=0.689, g=0.693\n",
            ">83, 134/234, d=0.683, g=0.708\n",
            ">83, 135/234, d=0.695, g=0.698\n",
            ">83, 136/234, d=0.692, g=0.693\n",
            ">83, 137/234, d=0.701, g=0.693\n",
            ">83, 138/234, d=0.694, g=0.711\n",
            ">83, 139/234, d=0.693, g=0.721\n",
            ">83, 140/234, d=0.687, g=0.719\n",
            ">83, 141/234, d=0.694, g=0.693\n",
            ">83, 142/234, d=0.695, g=0.679\n",
            ">83, 143/234, d=0.687, g=0.652\n",
            ">83, 144/234, d=0.681, g=0.675\n",
            ">83, 145/234, d=0.685, g=0.707\n",
            ">83, 146/234, d=0.691, g=0.745\n",
            ">83, 147/234, d=0.694, g=0.750\n",
            ">83, 148/234, d=0.692, g=0.724\n",
            ">83, 149/234, d=0.695, g=0.678\n",
            ">83, 150/234, d=0.689, g=0.681\n",
            ">83, 151/234, d=0.696, g=0.689\n",
            ">83, 152/234, d=0.688, g=0.695\n",
            ">83, 153/234, d=0.695, g=0.698\n",
            ">83, 154/234, d=0.691, g=0.722\n",
            ">83, 155/234, d=0.691, g=0.722\n",
            ">83, 156/234, d=0.694, g=0.699\n",
            ">83, 157/234, d=0.697, g=0.684\n",
            ">83, 158/234, d=0.691, g=0.706\n",
            ">83, 159/234, d=0.698, g=0.720\n",
            ">83, 160/234, d=0.691, g=0.704\n",
            ">83, 161/234, d=0.693, g=0.687\n",
            ">83, 162/234, d=0.695, g=0.668\n",
            ">83, 163/234, d=0.696, g=0.685\n",
            ">83, 164/234, d=0.693, g=0.695\n",
            ">83, 165/234, d=0.690, g=0.719\n",
            ">83, 166/234, d=0.685, g=0.736\n",
            ">83, 167/234, d=0.694, g=0.709\n",
            ">83, 168/234, d=0.696, g=0.685\n",
            ">83, 169/234, d=0.695, g=0.693\n",
            ">83, 170/234, d=0.695, g=0.707\n",
            ">83, 171/234, d=0.698, g=0.733\n",
            ">83, 172/234, d=0.690, g=0.715\n",
            ">83, 173/234, d=0.690, g=0.691\n",
            ">83, 174/234, d=0.686, g=0.666\n",
            ">83, 175/234, d=0.684, g=0.665\n",
            ">83, 176/234, d=0.688, g=0.697\n",
            ">83, 177/234, d=0.696, g=0.741\n",
            ">83, 178/234, d=0.695, g=0.748\n",
            ">83, 179/234, d=0.691, g=0.716\n",
            ">83, 180/234, d=0.692, g=0.686\n",
            ">83, 181/234, d=0.688, g=0.662\n",
            ">83, 182/234, d=0.692, g=0.685\n",
            ">83, 183/234, d=0.681, g=0.722\n",
            ">83, 184/234, d=0.691, g=0.743\n",
            ">83, 185/234, d=0.689, g=0.730\n",
            ">83, 186/234, d=0.698, g=0.687\n",
            ">83, 187/234, d=0.689, g=0.683\n",
            ">83, 188/234, d=0.698, g=0.691\n",
            ">83, 189/234, d=0.689, g=0.725\n",
            ">83, 190/234, d=0.689, g=0.719\n",
            ">83, 191/234, d=0.687, g=0.706\n",
            ">83, 192/234, d=0.690, g=0.687\n",
            ">83, 193/234, d=0.689, g=0.695\n",
            ">83, 194/234, d=0.695, g=0.687\n",
            ">83, 195/234, d=0.690, g=0.715\n",
            ">83, 196/234, d=0.690, g=0.726\n",
            ">83, 197/234, d=0.693, g=0.701\n",
            ">83, 198/234, d=0.688, g=0.686\n",
            ">83, 199/234, d=0.691, g=0.692\n",
            ">83, 200/234, d=0.697, g=0.711\n",
            ">83, 201/234, d=0.695, g=0.720\n",
            ">83, 202/234, d=0.691, g=0.703\n",
            ">83, 203/234, d=0.700, g=0.681\n",
            ">83, 204/234, d=0.695, g=0.690\n",
            ">83, 205/234, d=0.688, g=0.710\n",
            ">83, 206/234, d=0.690, g=0.699\n",
            ">83, 207/234, d=0.686, g=0.697\n",
            ">83, 208/234, d=0.691, g=0.685\n",
            ">83, 209/234, d=0.694, g=0.678\n",
            ">83, 210/234, d=0.690, g=0.695\n",
            ">83, 211/234, d=0.686, g=0.714\n",
            ">83, 212/234, d=0.691, g=0.721\n",
            ">83, 213/234, d=0.691, g=0.702\n",
            ">83, 214/234, d=0.691, g=0.702\n",
            ">83, 215/234, d=0.692, g=0.694\n",
            ">83, 216/234, d=0.688, g=0.700\n",
            ">83, 217/234, d=0.693, g=0.698\n",
            ">83, 218/234, d=0.694, g=0.697\n",
            ">83, 219/234, d=0.692, g=0.695\n",
            ">83, 220/234, d=0.691, g=0.698\n",
            ">83, 221/234, d=0.697, g=0.707\n",
            ">83, 222/234, d=0.695, g=0.695\n",
            ">83, 223/234, d=0.690, g=0.688\n",
            ">83, 224/234, d=0.686, g=0.710\n",
            ">83, 225/234, d=0.691, g=0.712\n",
            ">83, 226/234, d=0.695, g=0.707\n",
            ">83, 227/234, d=0.691, g=0.690\n",
            ">83, 228/234, d=0.687, g=0.686\n",
            ">83, 229/234, d=0.697, g=0.697\n",
            ">83, 230/234, d=0.689, g=0.704\n",
            ">83, 231/234, d=0.683, g=0.685\n",
            ">83, 232/234, d=0.685, g=0.703\n",
            ">83, 233/234, d=0.692, g=0.734\n",
            ">83, 234/234, d=0.694, g=0.734\n",
            ">84, 1/234, d=0.692, g=0.716\n",
            ">84, 2/234, d=0.691, g=0.682\n",
            ">84, 3/234, d=0.694, g=0.663\n",
            ">84, 4/234, d=0.699, g=0.680\n",
            ">84, 5/234, d=0.686, g=0.702\n",
            ">84, 6/234, d=0.692, g=0.725\n",
            ">84, 7/234, d=0.694, g=0.732\n",
            ">84, 8/234, d=0.691, g=0.727\n",
            ">84, 9/234, d=0.698, g=0.687\n",
            ">84, 10/234, d=0.688, g=0.682\n",
            ">84, 11/234, d=0.694, g=0.672\n",
            ">84, 12/234, d=0.698, g=0.692\n",
            ">84, 13/234, d=0.692, g=0.704\n",
            ">84, 14/234, d=0.687, g=0.728\n",
            ">84, 15/234, d=0.698, g=0.720\n",
            ">84, 16/234, d=0.703, g=0.682\n",
            ">84, 17/234, d=0.689, g=0.673\n",
            ">84, 18/234, d=0.692, g=0.692\n",
            ">84, 19/234, d=0.686, g=0.707\n",
            ">84, 20/234, d=0.685, g=0.712\n",
            ">84, 21/234, d=0.693, g=0.694\n",
            ">84, 22/234, d=0.691, g=0.688\n",
            ">84, 23/234, d=0.691, g=0.679\n",
            ">84, 24/234, d=0.694, g=0.702\n",
            ">84, 25/234, d=0.690, g=0.715\n",
            ">84, 26/234, d=0.686, g=0.700\n",
            ">84, 27/234, d=0.692, g=0.679\n",
            ">84, 28/234, d=0.690, g=0.685\n",
            ">84, 29/234, d=0.698, g=0.703\n",
            ">84, 30/234, d=0.689, g=0.726\n",
            ">84, 31/234, d=0.683, g=0.721\n",
            ">84, 32/234, d=0.685, g=0.705\n",
            ">84, 33/234, d=0.694, g=0.697\n",
            ">84, 34/234, d=0.685, g=0.690\n",
            ">84, 35/234, d=0.691, g=0.693\n",
            ">84, 36/234, d=0.687, g=0.696\n",
            ">84, 37/234, d=0.688, g=0.687\n",
            ">84, 38/234, d=0.693, g=0.687\n",
            ">84, 39/234, d=0.698, g=0.694\n",
            ">84, 40/234, d=0.687, g=0.704\n",
            ">84, 41/234, d=0.686, g=0.711\n",
            ">84, 42/234, d=0.689, g=0.707\n",
            ">84, 43/234, d=0.688, g=0.703\n",
            ">84, 44/234, d=0.684, g=0.693\n",
            ">84, 45/234, d=0.685, g=0.707\n",
            ">84, 46/234, d=0.690, g=0.693\n",
            ">84, 47/234, d=0.690, g=0.699\n",
            ">84, 48/234, d=0.690, g=0.680\n",
            ">84, 49/234, d=0.691, g=0.670\n",
            ">84, 50/234, d=0.694, g=0.683\n",
            ">84, 51/234, d=0.689, g=0.705\n",
            ">84, 52/234, d=0.686, g=0.715\n",
            ">84, 53/234, d=0.689, g=0.719\n",
            ">84, 54/234, d=0.691, g=0.699\n",
            ">84, 55/234, d=0.690, g=0.706\n",
            ">84, 56/234, d=0.687, g=0.699\n",
            ">84, 57/234, d=0.690, g=0.695\n",
            ">84, 58/234, d=0.685, g=0.702\n",
            ">84, 59/234, d=0.691, g=0.698\n",
            ">84, 60/234, d=0.690, g=0.706\n",
            ">84, 61/234, d=0.693, g=0.690\n",
            ">84, 62/234, d=0.687, g=0.688\n",
            ">84, 63/234, d=0.697, g=0.696\n",
            ">84, 64/234, d=0.689, g=0.715\n",
            ">84, 65/234, d=0.692, g=0.712\n",
            ">84, 66/234, d=0.693, g=0.699\n",
            ">84, 67/234, d=0.695, g=0.703\n",
            ">84, 68/234, d=0.689, g=0.704\n",
            ">84, 69/234, d=0.692, g=0.703\n",
            ">84, 70/234, d=0.694, g=0.709\n",
            ">84, 71/234, d=0.687, g=0.694\n",
            ">84, 72/234, d=0.691, g=0.692\n",
            ">84, 73/234, d=0.699, g=0.701\n",
            ">84, 74/234, d=0.695, g=0.699\n",
            ">84, 75/234, d=0.688, g=0.688\n",
            ">84, 76/234, d=0.687, g=0.680\n",
            ">84, 77/234, d=0.697, g=0.695\n",
            ">84, 78/234, d=0.691, g=0.732\n",
            ">84, 79/234, d=0.694, g=0.725\n",
            ">84, 80/234, d=0.691, g=0.707\n",
            ">84, 81/234, d=0.689, g=0.688\n",
            ">84, 82/234, d=0.695, g=0.680\n",
            ">84, 83/234, d=0.684, g=0.689\n",
            ">84, 84/234, d=0.694, g=0.702\n",
            ">84, 85/234, d=0.688, g=0.705\n",
            ">84, 86/234, d=0.692, g=0.703\n",
            ">84, 87/234, d=0.690, g=0.697\n",
            ">84, 88/234, d=0.692, g=0.683\n",
            ">84, 89/234, d=0.694, g=0.685\n",
            ">84, 90/234, d=0.692, g=0.702\n",
            ">84, 91/234, d=0.684, g=0.708\n",
            ">84, 92/234, d=0.692, g=0.716\n",
            ">84, 93/234, d=0.691, g=0.689\n",
            ">84, 94/234, d=0.686, g=0.688\n",
            ">84, 95/234, d=0.689, g=0.693\n",
            ">84, 96/234, d=0.695, g=0.703\n",
            ">84, 97/234, d=0.701, g=0.708\n",
            ">84, 98/234, d=0.698, g=0.702\n",
            ">84, 99/234, d=0.692, g=0.703\n",
            ">84, 100/234, d=0.688, g=0.691\n",
            ">84, 101/234, d=0.692, g=0.694\n",
            ">84, 102/234, d=0.696, g=0.698\n",
            ">84, 103/234, d=0.689, g=0.693\n",
            ">84, 104/234, d=0.697, g=0.690\n",
            ">84, 105/234, d=0.686, g=0.671\n",
            ">84, 106/234, d=0.696, g=0.685\n",
            ">84, 107/234, d=0.687, g=0.719\n",
            ">84, 108/234, d=0.694, g=0.725\n",
            ">84, 109/234, d=0.688, g=0.726\n",
            ">84, 110/234, d=0.692, g=0.701\n",
            ">84, 111/234, d=0.695, g=0.681\n",
            ">84, 112/234, d=0.692, g=0.686\n",
            ">84, 113/234, d=0.697, g=0.692\n",
            ">84, 114/234, d=0.691, g=0.716\n",
            ">84, 115/234, d=0.687, g=0.716\n",
            ">84, 116/234, d=0.686, g=0.699\n",
            ">84, 117/234, d=0.693, g=0.683\n",
            ">84, 118/234, d=0.687, g=0.688\n",
            ">84, 119/234, d=0.699, g=0.713\n",
            ">84, 120/234, d=0.682, g=0.732\n",
            ">84, 121/234, d=0.692, g=0.733\n",
            ">84, 122/234, d=0.688, g=0.702\n",
            ">84, 123/234, d=0.690, g=0.675\n",
            ">84, 124/234, d=0.696, g=0.676\n",
            ">84, 125/234, d=0.691, g=0.695\n",
            ">84, 126/234, d=0.694, g=0.710\n",
            ">84, 127/234, d=0.690, g=0.711\n",
            ">84, 128/234, d=0.686, g=0.696\n",
            ">84, 129/234, d=0.696, g=0.690\n",
            ">84, 130/234, d=0.693, g=0.701\n",
            ">84, 131/234, d=0.696, g=0.708\n",
            ">84, 132/234, d=0.691, g=0.726\n",
            ">84, 133/234, d=0.699, g=0.723\n",
            ">84, 134/234, d=0.691, g=0.687\n",
            ">84, 135/234, d=0.684, g=0.681\n",
            ">84, 136/234, d=0.688, g=0.673\n",
            ">84, 137/234, d=0.691, g=0.690\n",
            ">84, 138/234, d=0.694, g=0.698\n",
            ">84, 139/234, d=0.690, g=0.705\n",
            ">84, 140/234, d=0.685, g=0.701\n",
            ">84, 141/234, d=0.687, g=0.705\n",
            ">84, 142/234, d=0.690, g=0.693\n",
            ">84, 143/234, d=0.685, g=0.688\n",
            ">84, 144/234, d=0.690, g=0.684\n",
            ">84, 145/234, d=0.691, g=0.693\n",
            ">84, 146/234, d=0.690, g=0.699\n",
            ">84, 147/234, d=0.687, g=0.710\n",
            ">84, 148/234, d=0.692, g=0.705\n",
            ">84, 149/234, d=0.693, g=0.702\n",
            ">84, 150/234, d=0.692, g=0.696\n",
            ">84, 151/234, d=0.691, g=0.691\n",
            ">84, 152/234, d=0.692, g=0.724\n",
            ">84, 153/234, d=0.696, g=0.731\n",
            ">84, 154/234, d=0.698, g=0.701\n",
            ">84, 155/234, d=0.694, g=0.685\n",
            ">84, 156/234, d=0.691, g=0.687\n",
            ">84, 157/234, d=0.691, g=0.707\n",
            ">84, 158/234, d=0.697, g=0.728\n",
            ">84, 159/234, d=0.690, g=0.717\n",
            ">84, 160/234, d=0.697, g=0.716\n",
            ">84, 161/234, d=0.680, g=0.695\n",
            ">84, 162/234, d=0.683, g=0.683\n",
            ">84, 163/234, d=0.694, g=0.676\n",
            ">84, 164/234, d=0.687, g=0.694\n",
            ">84, 165/234, d=0.683, g=0.712\n",
            ">84, 166/234, d=0.697, g=0.710\n",
            ">84, 167/234, d=0.695, g=0.695\n",
            ">84, 168/234, d=0.689, g=0.692\n",
            ">84, 169/234, d=0.690, g=0.700\n",
            ">84, 170/234, d=0.694, g=0.698\n",
            ">84, 171/234, d=0.697, g=0.693\n",
            ">84, 172/234, d=0.696, g=0.704\n",
            ">84, 173/234, d=0.689, g=0.716\n",
            ">84, 174/234, d=0.703, g=0.705\n",
            ">84, 175/234, d=0.683, g=0.693\n",
            ">84, 176/234, d=0.684, g=0.703\n",
            ">84, 177/234, d=0.694, g=0.697\n",
            ">84, 178/234, d=0.697, g=0.699\n",
            ">84, 179/234, d=0.683, g=0.696\n",
            ">84, 180/234, d=0.692, g=0.698\n",
            ">84, 181/234, d=0.692, g=0.687\n",
            ">84, 182/234, d=0.685, g=0.688\n",
            ">84, 183/234, d=0.693, g=0.691\n",
            ">84, 184/234, d=0.697, g=0.696\n",
            ">84, 185/234, d=0.684, g=0.689\n",
            ">84, 186/234, d=0.697, g=0.677\n",
            ">84, 187/234, d=0.689, g=0.676\n",
            ">84, 188/234, d=0.697, g=0.713\n",
            ">84, 189/234, d=0.691, g=0.735\n",
            ">84, 190/234, d=0.693, g=0.732\n",
            ">84, 191/234, d=0.686, g=0.708\n",
            ">84, 192/234, d=0.696, g=0.686\n",
            ">84, 193/234, d=0.694, g=0.666\n",
            ">84, 194/234, d=0.686, g=0.685\n",
            ">84, 195/234, d=0.690, g=0.706\n",
            ">84, 196/234, d=0.698, g=0.722\n",
            ">84, 197/234, d=0.692, g=0.698\n",
            ">84, 198/234, d=0.687, g=0.697\n",
            ">84, 199/234, d=0.682, g=0.696\n",
            ">84, 200/234, d=0.698, g=0.703\n",
            ">84, 201/234, d=0.690, g=0.695\n",
            ">84, 202/234, d=0.694, g=0.703\n",
            ">84, 203/234, d=0.702, g=0.690\n",
            ">84, 204/234, d=0.687, g=0.709\n",
            ">84, 205/234, d=0.688, g=0.698\n",
            ">84, 206/234, d=0.690, g=0.692\n",
            ">84, 207/234, d=0.686, g=0.707\n",
            ">84, 208/234, d=0.690, g=0.725\n",
            ">84, 209/234, d=0.682, g=0.707\n",
            ">84, 210/234, d=0.685, g=0.701\n",
            ">84, 211/234, d=0.692, g=0.687\n",
            ">84, 212/234, d=0.696, g=0.677\n",
            ">84, 213/234, d=0.686, g=0.689\n",
            ">84, 214/234, d=0.689, g=0.680\n",
            ">84, 215/234, d=0.698, g=0.699\n",
            ">84, 216/234, d=0.691, g=0.713\n",
            ">84, 217/234, d=0.695, g=0.743\n",
            ">84, 218/234, d=0.689, g=0.737\n",
            ">84, 219/234, d=0.691, g=0.697\n",
            ">84, 220/234, d=0.690, g=0.665\n",
            ">84, 221/234, d=0.693, g=0.662\n",
            ">84, 222/234, d=0.698, g=0.688\n",
            ">84, 223/234, d=0.693, g=0.729\n",
            ">84, 224/234, d=0.688, g=0.729\n",
            ">84, 225/234, d=0.694, g=0.709\n",
            ">84, 226/234, d=0.694, g=0.691\n",
            ">84, 227/234, d=0.697, g=0.699\n",
            ">84, 228/234, d=0.691, g=0.697\n",
            ">84, 229/234, d=0.696, g=0.709\n",
            ">84, 230/234, d=0.694, g=0.709\n",
            ">84, 231/234, d=0.695, g=0.705\n",
            ">84, 232/234, d=0.698, g=0.696\n",
            ">84, 233/234, d=0.683, g=0.696\n",
            ">84, 234/234, d=0.690, g=0.710\n",
            ">85, 1/234, d=0.689, g=0.711\n",
            ">85, 2/234, d=0.705, g=0.695\n",
            ">85, 3/234, d=0.693, g=0.678\n",
            ">85, 4/234, d=0.693, g=0.693\n",
            ">85, 5/234, d=0.687, g=0.742\n",
            ">85, 6/234, d=0.689, g=0.742\n",
            ">85, 7/234, d=0.694, g=0.718\n",
            ">85, 8/234, d=0.689, g=0.693\n",
            ">85, 9/234, d=0.695, g=0.665\n",
            ">85, 10/234, d=0.692, g=0.661\n",
            ">85, 11/234, d=0.694, g=0.683\n",
            ">85, 12/234, d=0.692, g=0.717\n",
            ">85, 13/234, d=0.693, g=0.714\n",
            ">85, 14/234, d=0.684, g=0.703\n",
            ">85, 15/234, d=0.693, g=0.694\n",
            ">85, 16/234, d=0.693, g=0.699\n",
            ">85, 17/234, d=0.701, g=0.707\n",
            ">85, 18/234, d=0.694, g=0.714\n",
            ">85, 19/234, d=0.683, g=0.707\n",
            ">85, 20/234, d=0.690, g=0.696\n",
            ">85, 21/234, d=0.690, g=0.689\n",
            ">85, 22/234, d=0.686, g=0.676\n",
            ">85, 23/234, d=0.689, g=0.693\n",
            ">85, 24/234, d=0.694, g=0.716\n",
            ">85, 25/234, d=0.697, g=0.717\n",
            ">85, 26/234, d=0.693, g=0.700\n",
            ">85, 27/234, d=0.691, g=0.675\n",
            ">85, 28/234, d=0.688, g=0.690\n",
            ">85, 29/234, d=0.695, g=0.711\n",
            ">85, 30/234, d=0.693, g=0.715\n",
            ">85, 31/234, d=0.699, g=0.702\n",
            ">85, 32/234, d=0.693, g=0.709\n",
            ">85, 33/234, d=0.689, g=0.692\n",
            ">85, 34/234, d=0.690, g=0.698\n",
            ">85, 35/234, d=0.685, g=0.685\n",
            ">85, 36/234, d=0.694, g=0.698\n",
            ">85, 37/234, d=0.695, g=0.705\n",
            ">85, 38/234, d=0.697, g=0.696\n",
            ">85, 39/234, d=0.690, g=0.708\n",
            ">85, 40/234, d=0.701, g=0.707\n",
            ">85, 41/234, d=0.695, g=0.714\n",
            ">85, 42/234, d=0.693, g=0.702\n",
            ">85, 43/234, d=0.690, g=0.688\n",
            ">85, 44/234, d=0.694, g=0.684\n",
            ">85, 45/234, d=0.688, g=0.687\n",
            ">85, 46/234, d=0.694, g=0.687\n",
            ">85, 47/234, d=0.685, g=0.701\n",
            ">85, 48/234, d=0.689, g=0.712\n",
            ">85, 49/234, d=0.697, g=0.720\n",
            ">85, 50/234, d=0.693, g=0.688\n",
            ">85, 51/234, d=0.690, g=0.684\n",
            ">85, 52/234, d=0.695, g=0.687\n",
            ">85, 53/234, d=0.692, g=0.712\n",
            ">85, 54/234, d=0.692, g=0.714\n",
            ">85, 55/234, d=0.692, g=0.711\n",
            ">85, 56/234, d=0.692, g=0.688\n",
            ">85, 57/234, d=0.692, g=0.674\n",
            ">85, 58/234, d=0.696, g=0.688\n",
            ">85, 59/234, d=0.691, g=0.708\n",
            ">85, 60/234, d=0.691, g=0.715\n",
            ">85, 61/234, d=0.688, g=0.693\n",
            ">85, 62/234, d=0.687, g=0.702\n",
            ">85, 63/234, d=0.692, g=0.714\n",
            ">85, 64/234, d=0.695, g=0.704\n",
            ">85, 65/234, d=0.690, g=0.683\n",
            ">85, 66/234, d=0.693, g=0.678\n",
            ">85, 67/234, d=0.680, g=0.689\n",
            ">85, 68/234, d=0.692, g=0.699\n",
            ">85, 69/234, d=0.690, g=0.700\n",
            ">85, 70/234, d=0.696, g=0.696\n",
            ">85, 71/234, d=0.693, g=0.694\n",
            ">85, 72/234, d=0.691, g=0.695\n",
            ">85, 73/234, d=0.695, g=0.701\n",
            ">85, 74/234, d=0.690, g=0.716\n",
            ">85, 75/234, d=0.692, g=0.710\n",
            ">85, 76/234, d=0.684, g=0.695\n",
            ">85, 77/234, d=0.691, g=0.685\n",
            ">85, 78/234, d=0.693, g=0.687\n",
            ">85, 79/234, d=0.700, g=0.715\n",
            ">85, 80/234, d=0.687, g=0.716\n",
            ">85, 81/234, d=0.693, g=0.688\n",
            ">85, 82/234, d=0.695, g=0.679\n",
            ">85, 83/234, d=0.686, g=0.715\n",
            ">85, 84/234, d=0.691, g=0.718\n",
            ">85, 85/234, d=0.690, g=0.726\n",
            ">85, 86/234, d=0.697, g=0.706\n",
            ">85, 87/234, d=0.687, g=0.701\n",
            ">85, 88/234, d=0.700, g=0.676\n",
            ">85, 89/234, d=0.693, g=0.685\n",
            ">85, 90/234, d=0.689, g=0.699\n",
            ">85, 91/234, d=0.690, g=0.707\n",
            ">85, 92/234, d=0.695, g=0.707\n",
            ">85, 93/234, d=0.696, g=0.699\n",
            ">85, 94/234, d=0.688, g=0.705\n",
            ">85, 95/234, d=0.691, g=0.697\n",
            ">85, 96/234, d=0.690, g=0.696\n",
            ">85, 97/234, d=0.697, g=0.701\n",
            ">85, 98/234, d=0.686, g=0.714\n",
            ">85, 99/234, d=0.695, g=0.705\n",
            ">85, 100/234, d=0.690, g=0.712\n",
            ">85, 101/234, d=0.688, g=0.697\n",
            ">85, 102/234, d=0.698, g=0.691\n",
            ">85, 103/234, d=0.690, g=0.683\n",
            ">85, 104/234, d=0.697, g=0.682\n",
            ">85, 105/234, d=0.689, g=0.699\n",
            ">85, 106/234, d=0.691, g=0.706\n",
            ">85, 107/234, d=0.687, g=0.716\n",
            ">85, 108/234, d=0.695, g=0.712\n",
            ">85, 109/234, d=0.689, g=0.701\n",
            ">85, 110/234, d=0.694, g=0.686\n",
            ">85, 111/234, d=0.692, g=0.663\n",
            ">85, 112/234, d=0.696, g=0.681\n",
            ">85, 113/234, d=0.691, g=0.710\n",
            ">85, 114/234, d=0.690, g=0.737\n",
            ">85, 115/234, d=0.690, g=0.711\n",
            ">85, 116/234, d=0.692, g=0.693\n",
            ">85, 117/234, d=0.690, g=0.683\n",
            ">85, 118/234, d=0.700, g=0.688\n",
            ">85, 119/234, d=0.694, g=0.714\n",
            ">85, 120/234, d=0.689, g=0.724\n",
            ">85, 121/234, d=0.693, g=0.715\n",
            ">85, 122/234, d=0.690, g=0.689\n",
            ">85, 123/234, d=0.695, g=0.663\n",
            ">85, 124/234, d=0.691, g=0.681\n",
            ">85, 125/234, d=0.694, g=0.691\n",
            ">85, 126/234, d=0.691, g=0.713\n",
            ">85, 127/234, d=0.697, g=0.706\n",
            ">85, 128/234, d=0.691, g=0.701\n",
            ">85, 129/234, d=0.691, g=0.697\n",
            ">85, 130/234, d=0.690, g=0.703\n",
            ">85, 131/234, d=0.693, g=0.713\n",
            ">85, 132/234, d=0.694, g=0.715\n",
            ">85, 133/234, d=0.687, g=0.715\n",
            ">85, 134/234, d=0.686, g=0.695\n",
            ">85, 135/234, d=0.697, g=0.677\n",
            ">85, 136/234, d=0.692, g=0.669\n",
            ">85, 137/234, d=0.694, g=0.683\n",
            ">85, 138/234, d=0.690, g=0.688\n",
            ">85, 139/234, d=0.691, g=0.695\n",
            ">85, 140/234, d=0.695, g=0.713\n",
            ">85, 141/234, d=0.688, g=0.714\n",
            ">85, 142/234, d=0.694, g=0.700\n",
            ">85, 143/234, d=0.693, g=0.689\n",
            ">85, 144/234, d=0.689, g=0.699\n",
            ">85, 145/234, d=0.689, g=0.702\n",
            ">85, 146/234, d=0.688, g=0.713\n",
            ">85, 147/234, d=0.687, g=0.722\n",
            ">85, 148/234, d=0.694, g=0.707\n",
            ">85, 149/234, d=0.694, g=0.700\n",
            ">85, 150/234, d=0.700, g=0.683\n",
            ">85, 151/234, d=0.692, g=0.678\n",
            ">85, 152/234, d=0.697, g=0.699\n",
            ">85, 153/234, d=0.689, g=0.703\n",
            ">85, 154/234, d=0.691, g=0.692\n",
            ">85, 155/234, d=0.697, g=0.696\n",
            ">85, 156/234, d=0.694, g=0.683\n",
            ">85, 157/234, d=0.696, g=0.706\n",
            ">85, 158/234, d=0.697, g=0.727\n",
            ">85, 159/234, d=0.693, g=0.726\n",
            ">85, 160/234, d=0.692, g=0.702\n",
            ">85, 161/234, d=0.695, g=0.682\n",
            ">85, 162/234, d=0.693, g=0.674\n",
            ">85, 163/234, d=0.687, g=0.684\n",
            ">85, 164/234, d=0.690, g=0.704\n",
            ">85, 165/234, d=0.692, g=0.714\n",
            ">85, 166/234, d=0.693, g=0.702\n",
            ">85, 167/234, d=0.698, g=0.699\n",
            ">85, 168/234, d=0.694, g=0.706\n",
            ">85, 169/234, d=0.685, g=0.699\n",
            ">85, 170/234, d=0.692, g=0.688\n",
            ">85, 171/234, d=0.696, g=0.698\n",
            ">85, 172/234, d=0.686, g=0.695\n",
            ">85, 173/234, d=0.693, g=0.708\n",
            ">85, 174/234, d=0.688, g=0.726\n",
            ">85, 175/234, d=0.696, g=0.704\n",
            ">85, 176/234, d=0.691, g=0.706\n",
            ">85, 177/234, d=0.694, g=0.698\n",
            ">85, 178/234, d=0.691, g=0.684\n",
            ">85, 179/234, d=0.692, g=0.684\n",
            ">85, 180/234, d=0.693, g=0.690\n",
            ">85, 181/234, d=0.690, g=0.698\n",
            ">85, 182/234, d=0.693, g=0.726\n",
            ">85, 183/234, d=0.693, g=0.715\n",
            ">85, 184/234, d=0.693, g=0.700\n",
            ">85, 185/234, d=0.686, g=0.701\n",
            ">85, 186/234, d=0.694, g=0.709\n",
            ">85, 187/234, d=0.687, g=0.695\n",
            ">85, 188/234, d=0.689, g=0.715\n",
            ">85, 189/234, d=0.686, g=0.716\n",
            ">85, 190/234, d=0.697, g=0.704\n",
            ">85, 191/234, d=0.696, g=0.696\n",
            ">85, 192/234, d=0.693, g=0.680\n",
            ">85, 193/234, d=0.694, g=0.691\n",
            ">85, 194/234, d=0.695, g=0.715\n",
            ">85, 195/234, d=0.694, g=0.738\n",
            ">85, 196/234, d=0.688, g=0.714\n",
            ">85, 197/234, d=0.691, g=0.691\n",
            ">85, 198/234, d=0.688, g=0.680\n",
            ">85, 199/234, d=0.692, g=0.690\n",
            ">85, 200/234, d=0.688, g=0.710\n",
            ">85, 201/234, d=0.693, g=0.723\n",
            ">85, 202/234, d=0.688, g=0.700\n",
            ">85, 203/234, d=0.700, g=0.701\n",
            ">85, 204/234, d=0.695, g=0.704\n",
            ">85, 205/234, d=0.693, g=0.701\n",
            ">85, 206/234, d=0.687, g=0.712\n",
            ">85, 207/234, d=0.695, g=0.711\n",
            ">85, 208/234, d=0.690, g=0.708\n",
            ">85, 209/234, d=0.695, g=0.688\n",
            ">85, 210/234, d=0.692, g=0.696\n",
            ">85, 211/234, d=0.693, g=0.706\n",
            ">85, 212/234, d=0.696, g=0.719\n",
            ">85, 213/234, d=0.697, g=0.705\n",
            ">85, 214/234, d=0.692, g=0.694\n",
            ">85, 215/234, d=0.698, g=0.680\n",
            ">85, 216/234, d=0.680, g=0.683\n",
            ">85, 217/234, d=0.695, g=0.694\n",
            ">85, 218/234, d=0.696, g=0.707\n",
            ">85, 219/234, d=0.688, g=0.701\n",
            ">85, 220/234, d=0.695, g=0.693\n",
            ">85, 221/234, d=0.686, g=0.702\n",
            ">85, 222/234, d=0.695, g=0.726\n",
            ">85, 223/234, d=0.693, g=0.721\n",
            ">85, 224/234, d=0.689, g=0.697\n",
            ">85, 225/234, d=0.695, g=0.679\n",
            ">85, 226/234, d=0.692, g=0.683\n",
            ">85, 227/234, d=0.690, g=0.699\n",
            ">85, 228/234, d=0.685, g=0.709\n",
            ">85, 229/234, d=0.690, g=0.710\n",
            ">85, 230/234, d=0.693, g=0.697\n",
            ">85, 231/234, d=0.691, g=0.680\n",
            ">85, 232/234, d=0.693, g=0.692\n",
            ">85, 233/234, d=0.690, g=0.712\n",
            ">85, 234/234, d=0.700, g=0.720\n",
            ">86, 1/234, d=0.694, g=0.725\n",
            ">86, 2/234, d=0.688, g=0.705\n",
            ">86, 3/234, d=0.692, g=0.691\n",
            ">86, 4/234, d=0.696, g=0.690\n",
            ">86, 5/234, d=0.688, g=0.700\n",
            ">86, 6/234, d=0.700, g=0.695\n",
            ">86, 7/234, d=0.692, g=0.691\n",
            ">86, 8/234, d=0.692, g=0.701\n",
            ">86, 9/234, d=0.691, g=0.691\n",
            ">86, 10/234, d=0.686, g=0.694\n",
            ">86, 11/234, d=0.699, g=0.694\n",
            ">86, 12/234, d=0.698, g=0.703\n",
            ">86, 13/234, d=0.689, g=0.703\n",
            ">86, 14/234, d=0.695, g=0.703\n",
            ">86, 15/234, d=0.688, g=0.700\n",
            ">86, 16/234, d=0.694, g=0.694\n",
            ">86, 17/234, d=0.696, g=0.699\n",
            ">86, 18/234, d=0.684, g=0.712\n",
            ">86, 19/234, d=0.693, g=0.702\n",
            ">86, 20/234, d=0.701, g=0.697\n",
            ">86, 21/234, d=0.700, g=0.688\n",
            ">86, 22/234, d=0.694, g=0.692\n",
            ">86, 23/234, d=0.689, g=0.705\n",
            ">86, 24/234, d=0.692, g=0.719\n",
            ">86, 25/234, d=0.687, g=0.708\n",
            ">86, 26/234, d=0.697, g=0.677\n",
            ">86, 27/234, d=0.697, g=0.681\n",
            ">86, 28/234, d=0.692, g=0.698\n",
            ">86, 29/234, d=0.687, g=0.725\n",
            ">86, 30/234, d=0.687, g=0.727\n",
            ">86, 31/234, d=0.692, g=0.713\n",
            ">86, 32/234, d=0.685, g=0.688\n",
            ">86, 33/234, d=0.685, g=0.675\n",
            ">86, 34/234, d=0.697, g=0.706\n",
            ">86, 35/234, d=0.697, g=0.741\n",
            ">86, 36/234, d=0.689, g=0.722\n",
            ">86, 37/234, d=0.683, g=0.686\n",
            ">86, 38/234, d=0.686, g=0.675\n",
            ">86, 39/234, d=0.690, g=0.687\n",
            ">86, 40/234, d=0.684, g=0.699\n",
            ">86, 41/234, d=0.692, g=0.720\n",
            ">86, 42/234, d=0.700, g=0.711\n",
            ">86, 43/234, d=0.692, g=0.697\n",
            ">86, 44/234, d=0.697, g=0.678\n",
            ">86, 45/234, d=0.689, g=0.683\n",
            ">86, 46/234, d=0.692, g=0.716\n",
            ">86, 47/234, d=0.688, g=0.725\n",
            ">86, 48/234, d=0.689, g=0.708\n",
            ">86, 49/234, d=0.687, g=0.699\n",
            ">86, 50/234, d=0.692, g=0.690\n",
            ">86, 51/234, d=0.694, g=0.697\n",
            ">86, 52/234, d=0.698, g=0.697\n",
            ">86, 53/234, d=0.693, g=0.695\n",
            ">86, 54/234, d=0.688, g=0.684\n",
            ">86, 55/234, d=0.694, g=0.705\n",
            ">86, 56/234, d=0.690, g=0.706\n",
            ">86, 57/234, d=0.694, g=0.705\n",
            ">86, 58/234, d=0.692, g=0.698\n",
            ">86, 59/234, d=0.687, g=0.692\n",
            ">86, 60/234, d=0.690, g=0.702\n",
            ">86, 61/234, d=0.692, g=0.699\n",
            ">86, 62/234, d=0.691, g=0.697\n",
            ">86, 63/234, d=0.689, g=0.688\n",
            ">86, 64/234, d=0.693, g=0.680\n",
            ">86, 65/234, d=0.697, g=0.687\n",
            ">86, 66/234, d=0.688, g=0.713\n",
            ">86, 67/234, d=0.697, g=0.726\n",
            ">86, 68/234, d=0.694, g=0.720\n",
            ">86, 69/234, d=0.691, g=0.705\n",
            ">86, 70/234, d=0.688, g=0.689\n",
            ">86, 71/234, d=0.697, g=0.680\n",
            ">86, 72/234, d=0.687, g=0.688\n",
            ">86, 73/234, d=0.696, g=0.691\n",
            ">86, 74/234, d=0.687, g=0.707\n",
            ">86, 75/234, d=0.688, g=0.713\n",
            ">86, 76/234, d=0.688, g=0.694\n",
            ">86, 77/234, d=0.690, g=0.681\n",
            ">86, 78/234, d=0.694, g=0.685\n",
            ">86, 79/234, d=0.685, g=0.705\n",
            ">86, 80/234, d=0.692, g=0.721\n",
            ">86, 81/234, d=0.690, g=0.715\n",
            ">86, 82/234, d=0.696, g=0.687\n",
            ">86, 83/234, d=0.696, g=0.683\n",
            ">86, 84/234, d=0.680, g=0.707\n",
            ">86, 85/234, d=0.695, g=0.710\n",
            ">86, 86/234, d=0.689, g=0.716\n",
            ">86, 87/234, d=0.681, g=0.700\n",
            ">86, 88/234, d=0.697, g=0.687\n",
            ">86, 89/234, d=0.702, g=0.682\n",
            ">86, 90/234, d=0.692, g=0.688\n",
            ">86, 91/234, d=0.688, g=0.710\n",
            ">86, 92/234, d=0.693, g=0.712\n",
            ">86, 93/234, d=0.692, g=0.687\n",
            ">86, 94/234, d=0.694, g=0.683\n",
            ">86, 95/234, d=0.698, g=0.700\n",
            ">86, 96/234, d=0.692, g=0.704\n",
            ">86, 97/234, d=0.689, g=0.721\n",
            ">86, 98/234, d=0.691, g=0.715\n",
            ">86, 99/234, d=0.689, g=0.690\n",
            ">86, 100/234, d=0.691, g=0.670\n",
            ">86, 101/234, d=0.696, g=0.690\n",
            ">86, 102/234, d=0.688, g=0.708\n",
            ">86, 103/234, d=0.689, g=0.717\n",
            ">86, 104/234, d=0.693, g=0.682\n",
            ">86, 105/234, d=0.689, g=0.688\n",
            ">86, 106/234, d=0.686, g=0.694\n",
            ">86, 107/234, d=0.694, g=0.693\n",
            ">86, 108/234, d=0.691, g=0.715\n",
            ">86, 109/234, d=0.695, g=0.711\n",
            ">86, 110/234, d=0.686, g=0.702\n",
            ">86, 111/234, d=0.688, g=0.688\n",
            ">86, 112/234, d=0.690, g=0.697\n",
            ">86, 113/234, d=0.688, g=0.715\n",
            ">86, 114/234, d=0.687, g=0.716\n",
            ">86, 115/234, d=0.695, g=0.719\n",
            ">86, 116/234, d=0.689, g=0.700\n",
            ">86, 117/234, d=0.699, g=0.689\n",
            ">86, 118/234, d=0.695, g=0.683\n",
            ">86, 119/234, d=0.694, g=0.689\n",
            ">86, 120/234, d=0.686, g=0.694\n",
            ">86, 121/234, d=0.690, g=0.703\n",
            ">86, 122/234, d=0.692, g=0.696\n",
            ">86, 123/234, d=0.694, g=0.697\n",
            ">86, 124/234, d=0.695, g=0.704\n",
            ">86, 125/234, d=0.693, g=0.708\n",
            ">86, 126/234, d=0.692, g=0.701\n",
            ">86, 127/234, d=0.698, g=0.706\n",
            ">86, 128/234, d=0.704, g=0.698\n",
            ">86, 129/234, d=0.685, g=0.687\n",
            ">86, 130/234, d=0.694, g=0.679\n",
            ">86, 131/234, d=0.690, g=0.676\n",
            ">86, 132/234, d=0.690, g=0.690\n",
            ">86, 133/234, d=0.688, g=0.707\n",
            ">86, 134/234, d=0.699, g=0.725\n",
            ">86, 135/234, d=0.694, g=0.728\n",
            ">86, 136/234, d=0.687, g=0.702\n",
            ">86, 137/234, d=0.685, g=0.682\n",
            ">86, 138/234, d=0.692, g=0.679\n",
            ">86, 139/234, d=0.690, g=0.691\n",
            ">86, 140/234, d=0.686, g=0.715\n",
            ">86, 141/234, d=0.690, g=0.715\n",
            ">86, 142/234, d=0.696, g=0.703\n",
            ">86, 143/234, d=0.685, g=0.697\n",
            ">86, 144/234, d=0.695, g=0.695\n",
            ">86, 145/234, d=0.692, g=0.704\n",
            ">86, 146/234, d=0.688, g=0.725\n",
            ">86, 147/234, d=0.694, g=0.713\n",
            ">86, 148/234, d=0.690, g=0.690\n",
            ">86, 149/234, d=0.697, g=0.674\n",
            ">86, 150/234, d=0.685, g=0.687\n",
            ">86, 151/234, d=0.692, g=0.717\n",
            ">86, 152/234, d=0.699, g=0.716\n",
            ">86, 153/234, d=0.695, g=0.715\n",
            ">86, 154/234, d=0.689, g=0.693\n",
            ">86, 155/234, d=0.691, g=0.696\n",
            ">86, 156/234, d=0.691, g=0.708\n",
            ">86, 157/234, d=0.697, g=0.715\n",
            ">86, 158/234, d=0.695, g=0.704\n",
            ">86, 159/234, d=0.687, g=0.678\n",
            ">86, 160/234, d=0.687, g=0.677\n",
            ">86, 161/234, d=0.686, g=0.686\n",
            ">86, 162/234, d=0.694, g=0.704\n",
            ">86, 163/234, d=0.683, g=0.720\n",
            ">86, 164/234, d=0.691, g=0.718\n",
            ">86, 165/234, d=0.689, g=0.708\n",
            ">86, 166/234, d=0.694, g=0.692\n",
            ">86, 167/234, d=0.691, g=0.679\n",
            ">86, 168/234, d=0.686, g=0.697\n",
            ">86, 169/234, d=0.686, g=0.705\n",
            ">86, 170/234, d=0.695, g=0.723\n",
            ">86, 171/234, d=0.691, g=0.715\n",
            ">86, 172/234, d=0.690, g=0.696\n",
            ">86, 173/234, d=0.688, g=0.687\n",
            ">86, 174/234, d=0.690, g=0.686\n",
            ">86, 175/234, d=0.684, g=0.698\n",
            ">86, 176/234, d=0.685, g=0.689\n",
            ">86, 177/234, d=0.699, g=0.694\n",
            ">86, 178/234, d=0.691, g=0.700\n",
            ">86, 179/234, d=0.687, g=0.705\n",
            ">86, 180/234, d=0.691, g=0.711\n",
            ">86, 181/234, d=0.697, g=0.699\n",
            ">86, 182/234, d=0.689, g=0.689\n",
            ">86, 183/234, d=0.686, g=0.713\n",
            ">86, 184/234, d=0.691, g=0.737\n",
            ">86, 185/234, d=0.692, g=0.711\n",
            ">86, 186/234, d=0.696, g=0.681\n",
            ">86, 187/234, d=0.690, g=0.666\n",
            ">86, 188/234, d=0.691, g=0.673\n",
            ">86, 189/234, d=0.687, g=0.696\n",
            ">86, 190/234, d=0.689, g=0.708\n",
            ">86, 191/234, d=0.688, g=0.718\n",
            ">86, 192/234, d=0.689, g=0.706\n",
            ">86, 193/234, d=0.698, g=0.680\n",
            ">86, 194/234, d=0.693, g=0.694\n",
            ">86, 195/234, d=0.690, g=0.702\n",
            ">86, 196/234, d=0.691, g=0.721\n",
            ">86, 197/234, d=0.683, g=0.708\n",
            ">86, 198/234, d=0.695, g=0.699\n",
            ">86, 199/234, d=0.686, g=0.709\n",
            ">86, 200/234, d=0.690, g=0.699\n",
            ">86, 201/234, d=0.695, g=0.684\n",
            ">86, 202/234, d=0.692, g=0.690\n",
            ">86, 203/234, d=0.689, g=0.698\n",
            ">86, 204/234, d=0.689, g=0.707\n",
            ">86, 205/234, d=0.689, g=0.703\n",
            ">86, 206/234, d=0.696, g=0.706\n",
            ">86, 207/234, d=0.690, g=0.697\n",
            ">86, 208/234, d=0.687, g=0.680\n",
            ">86, 209/234, d=0.692, g=0.687\n",
            ">86, 210/234, d=0.692, g=0.694\n",
            ">86, 211/234, d=0.698, g=0.704\n",
            ">86, 212/234, d=0.689, g=0.693\n",
            ">86, 213/234, d=0.689, g=0.698\n",
            ">86, 214/234, d=0.694, g=0.709\n",
            ">86, 215/234, d=0.686, g=0.728\n",
            ">86, 216/234, d=0.689, g=0.721\n",
            ">86, 217/234, d=0.685, g=0.695\n",
            ">86, 218/234, d=0.688, g=0.692\n",
            ">86, 219/234, d=0.692, g=0.703\n",
            ">86, 220/234, d=0.692, g=0.721\n",
            ">86, 221/234, d=0.697, g=0.717\n",
            ">86, 222/234, d=0.689, g=0.691\n",
            ">86, 223/234, d=0.695, g=0.690\n",
            ">86, 224/234, d=0.689, g=0.706\n",
            ">86, 225/234, d=0.687, g=0.704\n",
            ">86, 226/234, d=0.684, g=0.701\n",
            ">86, 227/234, d=0.691, g=0.701\n",
            ">86, 228/234, d=0.692, g=0.703\n",
            ">86, 229/234, d=0.686, g=0.708\n",
            ">86, 230/234, d=0.691, g=0.717\n",
            ">86, 231/234, d=0.688, g=0.697\n",
            ">86, 232/234, d=0.691, g=0.687\n",
            ">86, 233/234, d=0.692, g=0.689\n",
            ">86, 234/234, d=0.692, g=0.695\n",
            ">87, 1/234, d=0.690, g=0.696\n",
            ">87, 2/234, d=0.690, g=0.694\n",
            ">87, 3/234, d=0.693, g=0.694\n",
            ">87, 4/234, d=0.694, g=0.700\n",
            ">87, 5/234, d=0.692, g=0.709\n",
            ">87, 6/234, d=0.691, g=0.683\n",
            ">87, 7/234, d=0.697, g=0.687\n",
            ">87, 8/234, d=0.689, g=0.713\n",
            ">87, 9/234, d=0.691, g=0.712\n",
            ">87, 10/234, d=0.690, g=0.709\n",
            ">87, 11/234, d=0.697, g=0.701\n",
            ">87, 12/234, d=0.694, g=0.692\n",
            ">87, 13/234, d=0.688, g=0.703\n",
            ">87, 14/234, d=0.689, g=0.701\n",
            ">87, 15/234, d=0.696, g=0.698\n",
            ">87, 16/234, d=0.690, g=0.690\n",
            ">87, 17/234, d=0.698, g=0.699\n",
            ">87, 18/234, d=0.693, g=0.707\n",
            ">87, 19/234, d=0.690, g=0.693\n",
            ">87, 20/234, d=0.690, g=0.689\n",
            ">87, 21/234, d=0.685, g=0.695\n",
            ">87, 22/234, d=0.690, g=0.714\n",
            ">87, 23/234, d=0.691, g=0.717\n",
            ">87, 24/234, d=0.688, g=0.710\n",
            ">87, 25/234, d=0.691, g=0.691\n",
            ">87, 26/234, d=0.688, g=0.686\n",
            ">87, 27/234, d=0.692, g=0.674\n",
            ">87, 28/234, d=0.691, g=0.692\n",
            ">87, 29/234, d=0.687, g=0.709\n",
            ">87, 30/234, d=0.690, g=0.700\n",
            ">87, 31/234, d=0.692, g=0.695\n",
            ">87, 32/234, d=0.696, g=0.704\n",
            ">87, 33/234, d=0.688, g=0.731\n",
            ">87, 34/234, d=0.696, g=0.712\n",
            ">87, 35/234, d=0.697, g=0.707\n",
            ">87, 36/234, d=0.688, g=0.691\n",
            ">87, 37/234, d=0.693, g=0.692\n",
            ">87, 38/234, d=0.685, g=0.696\n",
            ">87, 39/234, d=0.691, g=0.700\n",
            ">87, 40/234, d=0.692, g=0.707\n",
            ">87, 41/234, d=0.689, g=0.711\n",
            ">87, 42/234, d=0.693, g=0.698\n",
            ">87, 43/234, d=0.695, g=0.701\n",
            ">87, 44/234, d=0.694, g=0.693\n",
            ">87, 45/234, d=0.695, g=0.699\n",
            ">87, 46/234, d=0.688, g=0.697\n",
            ">87, 47/234, d=0.690, g=0.704\n",
            ">87, 48/234, d=0.683, g=0.713\n",
            ">87, 49/234, d=0.694, g=0.700\n",
            ">87, 50/234, d=0.685, g=0.687\n",
            ">87, 51/234, d=0.694, g=0.697\n",
            ">87, 52/234, d=0.693, g=0.707\n",
            ">87, 53/234, d=0.694, g=0.722\n",
            ">87, 54/234, d=0.685, g=0.708\n",
            ">87, 55/234, d=0.695, g=0.693\n",
            ">87, 56/234, d=0.694, g=0.687\n",
            ">87, 57/234, d=0.690, g=0.699\n",
            ">87, 58/234, d=0.692, g=0.705\n",
            ">87, 59/234, d=0.688, g=0.691\n",
            ">87, 60/234, d=0.690, g=0.696\n",
            ">87, 61/234, d=0.696, g=0.694\n",
            ">87, 62/234, d=0.687, g=0.697\n",
            ">87, 63/234, d=0.692, g=0.718\n",
            ">87, 64/234, d=0.689, g=0.719\n",
            ">87, 65/234, d=0.691, g=0.707\n",
            ">87, 66/234, d=0.695, g=0.693\n",
            ">87, 67/234, d=0.700, g=0.681\n",
            ">87, 68/234, d=0.689, g=0.685\n",
            ">87, 69/234, d=0.693, g=0.699\n",
            ">87, 70/234, d=0.686, g=0.718\n",
            ">87, 71/234, d=0.691, g=0.710\n",
            ">87, 72/234, d=0.690, g=0.702\n",
            ">87, 73/234, d=0.693, g=0.691\n",
            ">87, 74/234, d=0.692, g=0.698\n",
            ">87, 75/234, d=0.688, g=0.694\n",
            ">87, 76/234, d=0.688, g=0.695\n",
            ">87, 77/234, d=0.688, g=0.690\n",
            ">87, 78/234, d=0.695, g=0.695\n",
            ">87, 79/234, d=0.685, g=0.702\n",
            ">87, 80/234, d=0.694, g=0.713\n",
            ">87, 81/234, d=0.689, g=0.704\n",
            ">87, 82/234, d=0.688, g=0.694\n",
            ">87, 83/234, d=0.690, g=0.683\n",
            ">87, 84/234, d=0.696, g=0.681\n",
            ">87, 85/234, d=0.692, g=0.705\n",
            ">87, 86/234, d=0.693, g=0.706\n",
            ">87, 87/234, d=0.694, g=0.698\n",
            ">87, 88/234, d=0.694, g=0.698\n",
            ">87, 89/234, d=0.690, g=0.700\n",
            ">87, 90/234, d=0.691, g=0.701\n",
            ">87, 91/234, d=0.686, g=0.713\n",
            ">87, 92/234, d=0.688, g=0.710\n",
            ">87, 93/234, d=0.691, g=0.704\n",
            ">87, 94/234, d=0.690, g=0.699\n",
            ">87, 95/234, d=0.690, g=0.706\n",
            ">87, 96/234, d=0.699, g=0.701\n",
            ">87, 97/234, d=0.692, g=0.695\n",
            ">87, 98/234, d=0.692, g=0.698\n",
            ">87, 99/234, d=0.697, g=0.690\n",
            ">87, 100/234, d=0.686, g=0.687\n",
            ">87, 101/234, d=0.692, g=0.696\n",
            ">87, 102/234, d=0.682, g=0.701\n",
            ">87, 103/234, d=0.690, g=0.715\n",
            ">87, 104/234, d=0.692, g=0.709\n",
            ">87, 105/234, d=0.689, g=0.698\n",
            ">87, 106/234, d=0.693, g=0.681\n",
            ">87, 107/234, d=0.697, g=0.688\n",
            ">87, 108/234, d=0.690, g=0.704\n",
            ">87, 109/234, d=0.690, g=0.720\n",
            ">87, 110/234, d=0.693, g=0.697\n",
            ">87, 111/234, d=0.691, g=0.679\n",
            ">87, 112/234, d=0.681, g=0.690\n",
            ">87, 113/234, d=0.687, g=0.710\n",
            ">87, 114/234, d=0.693, g=0.726\n",
            ">87, 115/234, d=0.691, g=0.715\n",
            ">87, 116/234, d=0.689, g=0.694\n",
            ">87, 117/234, d=0.689, g=0.685\n",
            ">87, 118/234, d=0.686, g=0.686\n",
            ">87, 119/234, d=0.695, g=0.704\n",
            ">87, 120/234, d=0.694, g=0.714\n",
            ">87, 121/234, d=0.684, g=0.712\n",
            ">87, 122/234, d=0.698, g=0.706\n",
            ">87, 123/234, d=0.696, g=0.686\n",
            ">87, 124/234, d=0.690, g=0.703\n",
            ">87, 125/234, d=0.695, g=0.713\n",
            ">87, 126/234, d=0.692, g=0.713\n",
            ">87, 127/234, d=0.693, g=0.699\n",
            ">87, 128/234, d=0.696, g=0.674\n",
            ">87, 129/234, d=0.690, g=0.687\n",
            ">87, 130/234, d=0.705, g=0.713\n",
            ">87, 131/234, d=0.696, g=0.735\n",
            ">87, 132/234, d=0.698, g=0.733\n",
            ">87, 133/234, d=0.697, g=0.719\n",
            ">87, 134/234, d=0.692, g=0.684\n",
            ">87, 135/234, d=0.690, g=0.682\n",
            ">87, 136/234, d=0.693, g=0.693\n",
            ">87, 137/234, d=0.693, g=0.697\n",
            ">87, 138/234, d=0.694, g=0.708\n",
            ">87, 139/234, d=0.690, g=0.708\n",
            ">87, 140/234, d=0.692, g=0.702\n",
            ">87, 141/234, d=0.693, g=0.703\n",
            ">87, 142/234, d=0.694, g=0.688\n",
            ">87, 143/234, d=0.696, g=0.701\n",
            ">87, 144/234, d=0.695, g=0.709\n",
            ">87, 145/234, d=0.695, g=0.712\n",
            ">87, 146/234, d=0.682, g=0.699\n",
            ">87, 147/234, d=0.681, g=0.694\n",
            ">87, 148/234, d=0.693, g=0.684\n",
            ">87, 149/234, d=0.684, g=0.686\n",
            ">87, 150/234, d=0.693, g=0.686\n",
            ">87, 151/234, d=0.688, g=0.685\n",
            ">87, 152/234, d=0.682, g=0.695\n",
            ">87, 153/234, d=0.691, g=0.704\n",
            ">87, 154/234, d=0.693, g=0.712\n",
            ">87, 155/234, d=0.694, g=0.711\n",
            ">87, 156/234, d=0.690, g=0.708\n",
            ">87, 157/234, d=0.689, g=0.702\n",
            ">87, 158/234, d=0.690, g=0.695\n",
            ">87, 159/234, d=0.688, g=0.684\n",
            ">87, 160/234, d=0.690, g=0.698\n",
            ">87, 161/234, d=0.694, g=0.710\n",
            ">87, 162/234, d=0.698, g=0.714\n",
            ">87, 163/234, d=0.694, g=0.690\n",
            ">87, 164/234, d=0.693, g=0.688\n",
            ">87, 165/234, d=0.696, g=0.720\n",
            ">87, 166/234, d=0.693, g=0.737\n",
            ">87, 167/234, d=0.696, g=0.713\n",
            ">87, 168/234, d=0.692, g=0.685\n",
            ">87, 169/234, d=0.693, g=0.679\n",
            ">87, 170/234, d=0.687, g=0.678\n",
            ">87, 171/234, d=0.690, g=0.699\n",
            ">87, 172/234, d=0.682, g=0.713\n",
            ">87, 173/234, d=0.696, g=0.705\n",
            ">87, 174/234, d=0.692, g=0.711\n",
            ">87, 175/234, d=0.690, g=0.684\n",
            ">87, 176/234, d=0.693, g=0.679\n",
            ">87, 177/234, d=0.694, g=0.683\n",
            ">87, 178/234, d=0.696, g=0.698\n",
            ">87, 179/234, d=0.693, g=0.708\n",
            ">87, 180/234, d=0.686, g=0.710\n",
            ">87, 181/234, d=0.690, g=0.691\n",
            ">87, 182/234, d=0.699, g=0.697\n",
            ">87, 183/234, d=0.689, g=0.697\n",
            ">87, 184/234, d=0.700, g=0.709\n",
            ">87, 185/234, d=0.686, g=0.705\n",
            ">87, 186/234, d=0.702, g=0.701\n",
            ">87, 187/234, d=0.686, g=0.704\n",
            ">87, 188/234, d=0.687, g=0.682\n",
            ">87, 189/234, d=0.687, g=0.686\n",
            ">87, 190/234, d=0.689, g=0.707\n",
            ">87, 191/234, d=0.689, g=0.709\n",
            ">87, 192/234, d=0.696, g=0.719\n",
            ">87, 193/234, d=0.693, g=0.711\n",
            ">87, 194/234, d=0.683, g=0.693\n",
            ">87, 195/234, d=0.696, g=0.675\n",
            ">87, 196/234, d=0.690, g=0.683\n",
            ">87, 197/234, d=0.690, g=0.704\n",
            ">87, 198/234, d=0.693, g=0.705\n",
            ">87, 199/234, d=0.689, g=0.702\n",
            ">87, 200/234, d=0.694, g=0.704\n",
            ">87, 201/234, d=0.690, g=0.712\n",
            ">87, 202/234, d=0.691, g=0.708\n",
            ">87, 203/234, d=0.694, g=0.700\n",
            ">87, 204/234, d=0.696, g=0.694\n",
            ">87, 205/234, d=0.692, g=0.693\n",
            ">87, 206/234, d=0.690, g=0.699\n",
            ">87, 207/234, d=0.697, g=0.696\n",
            ">87, 208/234, d=0.692, g=0.690\n",
            ">87, 209/234, d=0.695, g=0.684\n",
            ">87, 210/234, d=0.693, g=0.713\n",
            ">87, 211/234, d=0.694, g=0.726\n",
            ">87, 212/234, d=0.688, g=0.741\n",
            ">87, 213/234, d=0.688, g=0.698\n",
            ">87, 214/234, d=0.696, g=0.676\n",
            ">87, 215/234, d=0.690, g=0.669\n",
            ">87, 216/234, d=0.697, g=0.692\n",
            ">87, 217/234, d=0.694, g=0.704\n",
            ">87, 218/234, d=0.690, g=0.703\n",
            ">87, 219/234, d=0.694, g=0.711\n",
            ">87, 220/234, d=0.692, g=0.709\n",
            ">87, 221/234, d=0.693, g=0.695\n",
            ">87, 222/234, d=0.696, g=0.686\n",
            ">87, 223/234, d=0.692, g=0.688\n",
            ">87, 224/234, d=0.693, g=0.717\n",
            ">87, 225/234, d=0.695, g=0.715\n",
            ">87, 226/234, d=0.700, g=0.715\n",
            ">87, 227/234, d=0.698, g=0.694\n",
            ">87, 228/234, d=0.692, g=0.687\n",
            ">87, 229/234, d=0.693, g=0.686\n",
            ">87, 230/234, d=0.687, g=0.709\n",
            ">87, 231/234, d=0.688, g=0.716\n",
            ">87, 232/234, d=0.700, g=0.686\n",
            ">87, 233/234, d=0.693, g=0.678\n",
            ">87, 234/234, d=0.689, g=0.695\n",
            ">88, 1/234, d=0.693, g=0.719\n",
            ">88, 2/234, d=0.686, g=0.719\n",
            ">88, 3/234, d=0.688, g=0.704\n",
            ">88, 4/234, d=0.686, g=0.680\n",
            ">88, 5/234, d=0.691, g=0.681\n",
            ">88, 6/234, d=0.693, g=0.701\n",
            ">88, 7/234, d=0.695, g=0.714\n",
            ">88, 8/234, d=0.692, g=0.726\n",
            ">88, 9/234, d=0.691, g=0.701\n",
            ">88, 10/234, d=0.701, g=0.680\n",
            ">88, 11/234, d=0.688, g=0.673\n",
            ">88, 12/234, d=0.689, g=0.678\n",
            ">88, 13/234, d=0.689, g=0.700\n",
            ">88, 14/234, d=0.692, g=0.722\n",
            ">88, 15/234, d=0.697, g=0.736\n",
            ">88, 16/234, d=0.690, g=0.726\n",
            ">88, 17/234, d=0.690, g=0.703\n",
            ">88, 18/234, d=0.694, g=0.676\n",
            ">88, 19/234, d=0.692, g=0.671\n",
            ">88, 20/234, d=0.695, g=0.679\n",
            ">88, 21/234, d=0.688, g=0.694\n",
            ">88, 22/234, d=0.692, g=0.730\n",
            ">88, 23/234, d=0.690, g=0.743\n",
            ">88, 24/234, d=0.697, g=0.714\n",
            ">88, 25/234, d=0.696, g=0.687\n",
            ">88, 26/234, d=0.687, g=0.684\n",
            ">88, 27/234, d=0.685, g=0.711\n",
            ">88, 28/234, d=0.686, g=0.729\n",
            ">88, 29/234, d=0.696, g=0.706\n",
            ">88, 30/234, d=0.680, g=0.692\n",
            ">88, 31/234, d=0.693, g=0.688\n",
            ">88, 32/234, d=0.684, g=0.694\n",
            ">88, 33/234, d=0.697, g=0.700\n",
            ">88, 34/234, d=0.691, g=0.698\n",
            ">88, 35/234, d=0.692, g=0.704\n",
            ">88, 36/234, d=0.695, g=0.719\n",
            ">88, 37/234, d=0.689, g=0.717\n",
            ">88, 38/234, d=0.691, g=0.712\n",
            ">88, 39/234, d=0.692, g=0.700\n",
            ">88, 40/234, d=0.686, g=0.700\n",
            ">88, 41/234, d=0.692, g=0.684\n",
            ">88, 42/234, d=0.702, g=0.686\n",
            ">88, 43/234, d=0.687, g=0.693\n",
            ">88, 44/234, d=0.691, g=0.691\n",
            ">88, 45/234, d=0.687, g=0.682\n",
            ">88, 46/234, d=0.695, g=0.683\n",
            ">88, 47/234, d=0.695, g=0.703\n",
            ">88, 48/234, d=0.681, g=0.720\n",
            ">88, 49/234, d=0.687, g=0.706\n",
            ">88, 50/234, d=0.687, g=0.686\n",
            ">88, 51/234, d=0.693, g=0.683\n",
            ">88, 52/234, d=0.698, g=0.707\n",
            ">88, 53/234, d=0.689, g=0.714\n",
            ">88, 54/234, d=0.690, g=0.696\n",
            ">88, 55/234, d=0.685, g=0.693\n",
            ">88, 56/234, d=0.691, g=0.693\n",
            ">88, 57/234, d=0.696, g=0.698\n",
            ">88, 58/234, d=0.690, g=0.714\n",
            ">88, 59/234, d=0.693, g=0.716\n",
            ">88, 60/234, d=0.690, g=0.699\n",
            ">88, 61/234, d=0.687, g=0.681\n",
            ">88, 62/234, d=0.692, g=0.687\n",
            ">88, 63/234, d=0.688, g=0.695\n",
            ">88, 64/234, d=0.691, g=0.701\n",
            ">88, 65/234, d=0.692, g=0.702\n",
            ">88, 66/234, d=0.688, g=0.687\n",
            ">88, 67/234, d=0.691, g=0.664\n",
            ">88, 68/234, d=0.688, g=0.677\n",
            ">88, 69/234, d=0.690, g=0.712\n",
            ">88, 70/234, d=0.685, g=0.708\n",
            ">88, 71/234, d=0.691, g=0.697\n",
            ">88, 72/234, d=0.691, g=0.684\n",
            ">88, 73/234, d=0.687, g=0.686\n",
            ">88, 74/234, d=0.689, g=0.715\n",
            ">88, 75/234, d=0.686, g=0.719\n",
            ">88, 76/234, d=0.696, g=0.715\n",
            ">88, 77/234, d=0.688, g=0.702\n",
            ">88, 78/234, d=0.691, g=0.669\n",
            ">88, 79/234, d=0.691, g=0.663\n",
            ">88, 80/234, d=0.691, g=0.693\n",
            ">88, 81/234, d=0.688, g=0.733\n",
            ">88, 82/234, d=0.691, g=0.748\n",
            ">88, 83/234, d=0.695, g=0.707\n",
            ">88, 84/234, d=0.690, g=0.691\n",
            ">88, 85/234, d=0.690, g=0.681\n",
            ">88, 86/234, d=0.705, g=0.677\n",
            ">88, 87/234, d=0.689, g=0.680\n",
            ">88, 88/234, d=0.689, g=0.704\n",
            ">88, 89/234, d=0.688, g=0.708\n",
            ">88, 90/234, d=0.686, g=0.711\n",
            ">88, 91/234, d=0.692, g=0.701\n",
            ">88, 92/234, d=0.686, g=0.683\n",
            ">88, 93/234, d=0.693, g=0.697\n",
            ">88, 94/234, d=0.691, g=0.704\n",
            ">88, 95/234, d=0.690, g=0.716\n",
            ">88, 96/234, d=0.684, g=0.712\n",
            ">88, 97/234, d=0.695, g=0.701\n",
            ">88, 98/234, d=0.690, g=0.697\n",
            ">88, 99/234, d=0.689, g=0.681\n",
            ">88, 100/234, d=0.692, g=0.695\n",
            ">88, 101/234, d=0.693, g=0.708\n",
            ">88, 102/234, d=0.691, g=0.703\n",
            ">88, 103/234, d=0.687, g=0.708\n",
            ">88, 104/234, d=0.685, g=0.698\n",
            ">88, 105/234, d=0.692, g=0.691\n",
            ">88, 106/234, d=0.693, g=0.690\n",
            ">88, 107/234, d=0.691, g=0.688\n",
            ">88, 108/234, d=0.695, g=0.705\n",
            ">88, 109/234, d=0.693, g=0.708\n",
            ">88, 110/234, d=0.693, g=0.716\n",
            ">88, 111/234, d=0.693, g=0.719\n",
            ">88, 112/234, d=0.692, g=0.700\n",
            ">88, 113/234, d=0.689, g=0.694\n",
            ">88, 114/234, d=0.687, g=0.685\n",
            ">88, 115/234, d=0.686, g=0.691\n",
            ">88, 116/234, d=0.696, g=0.701\n",
            ">88, 117/234, d=0.690, g=0.716\n",
            ">88, 118/234, d=0.690, g=0.701\n",
            ">88, 119/234, d=0.698, g=0.686\n",
            ">88, 120/234, d=0.692, g=0.682\n",
            ">88, 121/234, d=0.694, g=0.704\n",
            ">88, 122/234, d=0.690, g=0.716\n",
            ">88, 123/234, d=0.688, g=0.708\n",
            ">88, 124/234, d=0.689, g=0.700\n",
            ">88, 125/234, d=0.694, g=0.700\n",
            ">88, 126/234, d=0.693, g=0.710\n",
            ">88, 127/234, d=0.692, g=0.726\n",
            ">88, 128/234, d=0.697, g=0.701\n",
            ">88, 129/234, d=0.691, g=0.683\n",
            ">88, 130/234, d=0.689, g=0.683\n",
            ">88, 131/234, d=0.687, g=0.693\n",
            ">88, 132/234, d=0.695, g=0.702\n",
            ">88, 133/234, d=0.698, g=0.702\n",
            ">88, 134/234, d=0.688, g=0.697\n",
            ">88, 135/234, d=0.693, g=0.690\n",
            ">88, 136/234, d=0.698, g=0.683\n",
            ">88, 137/234, d=0.690, g=0.678\n",
            ">88, 138/234, d=0.681, g=0.706\n",
            ">88, 139/234, d=0.689, g=0.727\n",
            ">88, 140/234, d=0.692, g=0.735\n",
            ">88, 141/234, d=0.689, g=0.712\n",
            ">88, 142/234, d=0.696, g=0.676\n",
            ">88, 143/234, d=0.693, g=0.676\n",
            ">88, 144/234, d=0.696, g=0.696\n",
            ">88, 145/234, d=0.691, g=0.715\n",
            ">88, 146/234, d=0.691, g=0.718\n",
            ">88, 147/234, d=0.699, g=0.710\n",
            ">88, 148/234, d=0.685, g=0.707\n",
            ">88, 149/234, d=0.692, g=0.675\n",
            ">88, 150/234, d=0.694, g=0.698\n",
            ">88, 151/234, d=0.693, g=0.700\n",
            ">88, 152/234, d=0.697, g=0.702\n",
            ">88, 153/234, d=0.692, g=0.700\n",
            ">88, 154/234, d=0.691, g=0.690\n",
            ">88, 155/234, d=0.688, g=0.694\n",
            ">88, 156/234, d=0.693, g=0.707\n",
            ">88, 157/234, d=0.691, g=0.704\n",
            ">88, 158/234, d=0.690, g=0.710\n",
            ">88, 159/234, d=0.690, g=0.708\n",
            ">88, 160/234, d=0.696, g=0.703\n",
            ">88, 161/234, d=0.695, g=0.694\n",
            ">88, 162/234, d=0.692, g=0.696\n",
            ">88, 163/234, d=0.691, g=0.685\n",
            ">88, 164/234, d=0.691, g=0.682\n",
            ">88, 165/234, d=0.696, g=0.693\n",
            ">88, 166/234, d=0.698, g=0.701\n",
            ">88, 167/234, d=0.690, g=0.708\n",
            ">88, 168/234, d=0.687, g=0.724\n",
            ">88, 169/234, d=0.696, g=0.719\n",
            ">88, 170/234, d=0.688, g=0.702\n",
            ">88, 171/234, d=0.697, g=0.696\n",
            ">88, 172/234, d=0.689, g=0.693\n",
            ">88, 173/234, d=0.694, g=0.689\n",
            ">88, 174/234, d=0.693, g=0.700\n",
            ">88, 175/234, d=0.683, g=0.701\n",
            ">88, 176/234, d=0.684, g=0.689\n",
            ">88, 177/234, d=0.698, g=0.686\n",
            ">88, 178/234, d=0.690, g=0.695\n",
            ">88, 179/234, d=0.696, g=0.702\n",
            ">88, 180/234, d=0.692, g=0.698\n",
            ">88, 181/234, d=0.703, g=0.691\n",
            ">88, 182/234, d=0.689, g=0.703\n",
            ">88, 183/234, d=0.682, g=0.692\n",
            ">88, 184/234, d=0.689, g=0.721\n",
            ">88, 185/234, d=0.692, g=0.737\n",
            ">88, 186/234, d=0.697, g=0.715\n",
            ">88, 187/234, d=0.690, g=0.682\n",
            ">88, 188/234, d=0.687, g=0.676\n",
            ">88, 189/234, d=0.691, g=0.688\n",
            ">88, 190/234, d=0.700, g=0.706\n",
            ">88, 191/234, d=0.695, g=0.699\n",
            ">88, 192/234, d=0.695, g=0.706\n",
            ">88, 193/234, d=0.695, g=0.707\n",
            ">88, 194/234, d=0.697, g=0.708\n",
            ">88, 195/234, d=0.692, g=0.693\n",
            ">88, 196/234, d=0.696, g=0.688\n",
            ">88, 197/234, d=0.691, g=0.699\n",
            ">88, 198/234, d=0.691, g=0.712\n",
            ">88, 199/234, d=0.696, g=0.705\n",
            ">88, 200/234, d=0.691, g=0.697\n",
            ">88, 201/234, d=0.687, g=0.672\n",
            ">88, 202/234, d=0.694, g=0.664\n",
            ">88, 203/234, d=0.691, g=0.677\n",
            ">88, 204/234, d=0.694, g=0.720\n",
            ">88, 205/234, d=0.685, g=0.743\n",
            ">88, 206/234, d=0.685, g=0.739\n",
            ">88, 207/234, d=0.697, g=0.700\n",
            ">88, 208/234, d=0.690, g=0.691\n",
            ">88, 209/234, d=0.692, g=0.668\n",
            ">88, 210/234, d=0.687, g=0.664\n",
            ">88, 211/234, d=0.694, g=0.689\n",
            ">88, 212/234, d=0.696, g=0.730\n",
            ">88, 213/234, d=0.684, g=0.727\n",
            ">88, 214/234, d=0.694, g=0.709\n",
            ">88, 215/234, d=0.690, g=0.684\n",
            ">88, 216/234, d=0.692, g=0.696\n",
            ">88, 217/234, d=0.691, g=0.687\n",
            ">88, 218/234, d=0.694, g=0.698\n",
            ">88, 219/234, d=0.682, g=0.692\n",
            ">88, 220/234, d=0.695, g=0.699\n",
            ">88, 221/234, d=0.690, g=0.711\n",
            ">88, 222/234, d=0.683, g=0.715\n",
            ">88, 223/234, d=0.690, g=0.705\n",
            ">88, 224/234, d=0.692, g=0.698\n",
            ">88, 225/234, d=0.696, g=0.692\n",
            ">88, 226/234, d=0.688, g=0.697\n",
            ">88, 227/234, d=0.691, g=0.706\n",
            ">88, 228/234, d=0.695, g=0.705\n",
            ">88, 229/234, d=0.689, g=0.699\n",
            ">88, 230/234, d=0.694, g=0.688\n",
            ">88, 231/234, d=0.695, g=0.687\n",
            ">88, 232/234, d=0.686, g=0.693\n",
            ">88, 233/234, d=0.695, g=0.703\n",
            ">88, 234/234, d=0.689, g=0.706\n",
            ">89, 1/234, d=0.696, g=0.699\n",
            ">89, 2/234, d=0.697, g=0.693\n",
            ">89, 3/234, d=0.681, g=0.702\n",
            ">89, 4/234, d=0.691, g=0.706\n",
            ">89, 5/234, d=0.690, g=0.710\n",
            ">89, 6/234, d=0.691, g=0.691\n",
            ">89, 7/234, d=0.693, g=0.703\n",
            ">89, 8/234, d=0.696, g=0.699\n",
            ">89, 9/234, d=0.691, g=0.711\n",
            ">89, 10/234, d=0.692, g=0.692\n",
            ">89, 11/234, d=0.688, g=0.679\n",
            ">89, 12/234, d=0.692, g=0.678\n",
            ">89, 13/234, d=0.696, g=0.702\n",
            ">89, 14/234, d=0.696, g=0.716\n",
            ">89, 15/234, d=0.689, g=0.720\n",
            ">89, 16/234, d=0.692, g=0.713\n",
            ">89, 17/234, d=0.690, g=0.705\n",
            ">89, 18/234, d=0.693, g=0.697\n",
            ">89, 19/234, d=0.692, g=0.691\n",
            ">89, 20/234, d=0.695, g=0.672\n",
            ">89, 21/234, d=0.694, g=0.680\n",
            ">89, 22/234, d=0.688, g=0.676\n",
            ">89, 23/234, d=0.685, g=0.695\n",
            ">89, 24/234, d=0.691, g=0.717\n",
            ">89, 25/234, d=0.693, g=0.751\n",
            ">89, 26/234, d=0.686, g=0.722\n",
            ">89, 27/234, d=0.692, g=0.698\n",
            ">89, 28/234, d=0.691, g=0.679\n",
            ">89, 29/234, d=0.693, g=0.668\n",
            ">89, 30/234, d=0.691, g=0.688\n",
            ">89, 31/234, d=0.685, g=0.714\n",
            ">89, 32/234, d=0.694, g=0.723\n",
            ">89, 33/234, d=0.698, g=0.725\n",
            ">89, 34/234, d=0.695, g=0.704\n",
            ">89, 35/234, d=0.692, g=0.692\n",
            ">89, 36/234, d=0.701, g=0.705\n",
            ">89, 37/234, d=0.690, g=0.699\n",
            ">89, 38/234, d=0.692, g=0.710\n",
            ">89, 39/234, d=0.686, g=0.711\n",
            ">89, 40/234, d=0.688, g=0.717\n",
            ">89, 41/234, d=0.687, g=0.693\n",
            ">89, 42/234, d=0.683, g=0.680\n",
            ">89, 43/234, d=0.687, g=0.679\n",
            ">89, 44/234, d=0.693, g=0.699\n",
            ">89, 45/234, d=0.692, g=0.698\n",
            ">89, 46/234, d=0.692, g=0.710\n",
            ">89, 47/234, d=0.692, g=0.705\n",
            ">89, 48/234, d=0.698, g=0.722\n",
            ">89, 49/234, d=0.693, g=0.729\n",
            ">89, 50/234, d=0.695, g=0.698\n",
            ">89, 51/234, d=0.690, g=0.686\n",
            ">89, 52/234, d=0.684, g=0.677\n",
            ">89, 53/234, d=0.694, g=0.692\n",
            ">89, 54/234, d=0.707, g=0.703\n",
            ">89, 55/234, d=0.686, g=0.709\n",
            ">89, 56/234, d=0.684, g=0.710\n",
            ">89, 57/234, d=0.687, g=0.696\n",
            ">89, 58/234, d=0.693, g=0.690\n",
            ">89, 59/234, d=0.690, g=0.690\n",
            ">89, 60/234, d=0.694, g=0.693\n",
            ">89, 61/234, d=0.690, g=0.697\n",
            ">89, 62/234, d=0.688, g=0.699\n",
            ">89, 63/234, d=0.688, g=0.699\n",
            ">89, 64/234, d=0.685, g=0.719\n",
            ">89, 65/234, d=0.692, g=0.715\n",
            ">89, 66/234, d=0.692, g=0.693\n",
            ">89, 67/234, d=0.690, g=0.685\n",
            ">89, 68/234, d=0.696, g=0.678\n",
            ">89, 69/234, d=0.689, g=0.692\n",
            ">89, 70/234, d=0.692, g=0.701\n",
            ">89, 71/234, d=0.697, g=0.703\n",
            ">89, 72/234, d=0.689, g=0.711\n",
            ">89, 73/234, d=0.690, g=0.703\n",
            ">89, 74/234, d=0.696, g=0.694\n",
            ">89, 75/234, d=0.694, g=0.688\n",
            ">89, 76/234, d=0.692, g=0.694\n",
            ">89, 77/234, d=0.691, g=0.717\n",
            ">89, 78/234, d=0.693, g=0.717\n",
            ">89, 79/234, d=0.692, g=0.706\n",
            ">89, 80/234, d=0.692, g=0.681\n",
            ">89, 81/234, d=0.685, g=0.693\n",
            ">89, 82/234, d=0.696, g=0.708\n",
            ">89, 83/234, d=0.688, g=0.707\n",
            ">89, 84/234, d=0.692, g=0.693\n",
            ">89, 85/234, d=0.687, g=0.694\n",
            ">89, 86/234, d=0.687, g=0.684\n",
            ">89, 87/234, d=0.688, g=0.692\n",
            ">89, 88/234, d=0.696, g=0.696\n",
            ">89, 89/234, d=0.695, g=0.706\n",
            ">89, 90/234, d=0.686, g=0.701\n",
            ">89, 91/234, d=0.692, g=0.697\n",
            ">89, 92/234, d=0.694, g=0.697\n",
            ">89, 93/234, d=0.690, g=0.703\n",
            ">89, 94/234, d=0.691, g=0.706\n",
            ">89, 95/234, d=0.691, g=0.694\n",
            ">89, 96/234, d=0.687, g=0.704\n",
            ">89, 97/234, d=0.690, g=0.701\n",
            ">89, 98/234, d=0.691, g=0.688\n",
            ">89, 99/234, d=0.699, g=0.689\n",
            ">89, 100/234, d=0.696, g=0.696\n",
            ">89, 101/234, d=0.690, g=0.686\n",
            ">89, 102/234, d=0.690, g=0.701\n",
            ">89, 103/234, d=0.696, g=0.718\n",
            ">89, 104/234, d=0.687, g=0.703\n",
            ">89, 105/234, d=0.696, g=0.690\n",
            ">89, 106/234, d=0.690, g=0.687\n",
            ">89, 107/234, d=0.691, g=0.684\n",
            ">89, 108/234, d=0.688, g=0.705\n",
            ">89, 109/234, d=0.691, g=0.709\n",
            ">89, 110/234, d=0.699, g=0.692\n",
            ">89, 111/234, d=0.698, g=0.675\n",
            ">89, 112/234, d=0.693, g=0.700\n",
            ">89, 113/234, d=0.687, g=0.716\n",
            ">89, 114/234, d=0.691, g=0.730\n",
            ">89, 115/234, d=0.689, g=0.717\n",
            ">89, 116/234, d=0.689, g=0.693\n",
            ">89, 117/234, d=0.689, g=0.683\n",
            ">89, 118/234, d=0.686, g=0.689\n",
            ">89, 119/234, d=0.691, g=0.691\n",
            ">89, 120/234, d=0.689, g=0.706\n",
            ">89, 121/234, d=0.698, g=0.720\n",
            ">89, 122/234, d=0.696, g=0.713\n",
            ">89, 123/234, d=0.701, g=0.675\n",
            ">89, 124/234, d=0.694, g=0.674\n",
            ">89, 125/234, d=0.689, g=0.688\n",
            ">89, 126/234, d=0.697, g=0.719\n",
            ">89, 127/234, d=0.690, g=0.729\n",
            ">89, 128/234, d=0.699, g=0.704\n",
            ">89, 129/234, d=0.687, g=0.680\n",
            ">89, 130/234, d=0.695, g=0.678\n",
            ">89, 131/234, d=0.690, g=0.711\n",
            ">89, 132/234, d=0.692, g=0.728\n",
            ">89, 133/234, d=0.693, g=0.702\n",
            ">89, 134/234, d=0.697, g=0.690\n",
            ">89, 135/234, d=0.701, g=0.678\n",
            ">89, 136/234, d=0.689, g=0.695\n",
            ">89, 137/234, d=0.693, g=0.711\n",
            ">89, 138/234, d=0.704, g=0.716\n",
            ">89, 139/234, d=0.696, g=0.709\n",
            ">89, 140/234, d=0.690, g=0.685\n",
            ">89, 141/234, d=0.691, g=0.682\n",
            ">89, 142/234, d=0.699, g=0.702\n",
            ">89, 143/234, d=0.688, g=0.707\n",
            ">89, 144/234, d=0.691, g=0.721\n",
            ">89, 145/234, d=0.691, g=0.686\n",
            ">89, 146/234, d=0.690, g=0.679\n",
            ">89, 147/234, d=0.693, g=0.678\n",
            ">89, 148/234, d=0.685, g=0.700\n",
            ">89, 149/234, d=0.687, g=0.696\n",
            ">89, 150/234, d=0.686, g=0.709\n",
            ">89, 151/234, d=0.684, g=0.702\n",
            ">89, 152/234, d=0.695, g=0.697\n",
            ">89, 153/234, d=0.691, g=0.689\n",
            ">89, 154/234, d=0.688, g=0.696\n",
            ">89, 155/234, d=0.691, g=0.714\n",
            ">89, 156/234, d=0.695, g=0.718\n",
            ">89, 157/234, d=0.685, g=0.702\n",
            ">89, 158/234, d=0.692, g=0.685\n",
            ">89, 159/234, d=0.690, g=0.679\n",
            ">89, 160/234, d=0.687, g=0.704\n",
            ">89, 161/234, d=0.685, g=0.708\n",
            ">89, 162/234, d=0.695, g=0.701\n",
            ">89, 163/234, d=0.697, g=0.686\n",
            ">89, 164/234, d=0.695, g=0.685\n",
            ">89, 165/234, d=0.693, g=0.710\n",
            ">89, 166/234, d=0.692, g=0.711\n",
            ">89, 167/234, d=0.688, g=0.697\n",
            ">89, 168/234, d=0.699, g=0.689\n",
            ">89, 169/234, d=0.696, g=0.687\n",
            ">89, 170/234, d=0.689, g=0.692\n",
            ">89, 171/234, d=0.691, g=0.699\n",
            ">89, 172/234, d=0.687, g=0.694\n",
            ">89, 173/234, d=0.685, g=0.697\n",
            ">89, 174/234, d=0.695, g=0.694\n",
            ">89, 175/234, d=0.691, g=0.711\n",
            ">89, 176/234, d=0.687, g=0.702\n",
            ">89, 177/234, d=0.695, g=0.692\n",
            ">89, 178/234, d=0.695, g=0.691\n",
            ">89, 179/234, d=0.689, g=0.698\n",
            ">89, 180/234, d=0.694, g=0.698\n",
            ">89, 181/234, d=0.688, g=0.694\n",
            ">89, 182/234, d=0.695, g=0.699\n",
            ">89, 183/234, d=0.688, g=0.687\n",
            ">89, 184/234, d=0.693, g=0.692\n",
            ">89, 185/234, d=0.681, g=0.703\n",
            ">89, 186/234, d=0.692, g=0.716\n",
            ">89, 187/234, d=0.691, g=0.703\n",
            ">89, 188/234, d=0.697, g=0.702\n",
            ">89, 189/234, d=0.695, g=0.710\n",
            ">89, 190/234, d=0.686, g=0.693\n",
            ">89, 191/234, d=0.688, g=0.696\n",
            ">89, 192/234, d=0.690, g=0.699\n",
            ">89, 193/234, d=0.692, g=0.684\n",
            ">89, 194/234, d=0.688, g=0.689\n",
            ">89, 195/234, d=0.694, g=0.703\n",
            ">89, 196/234, d=0.695, g=0.712\n",
            ">89, 197/234, d=0.700, g=0.709\n",
            ">89, 198/234, d=0.691, g=0.691\n",
            ">89, 199/234, d=0.689, g=0.680\n",
            ">89, 200/234, d=0.680, g=0.693\n",
            ">89, 201/234, d=0.696, g=0.710\n",
            ">89, 202/234, d=0.692, g=0.727\n",
            ">89, 203/234, d=0.692, g=0.718\n",
            ">89, 204/234, d=0.697, g=0.695\n",
            ">89, 205/234, d=0.692, g=0.683\n",
            ">89, 206/234, d=0.686, g=0.690\n",
            ">89, 207/234, d=0.691, g=0.702\n",
            ">89, 208/234, d=0.696, g=0.692\n",
            ">89, 209/234, d=0.694, g=0.687\n",
            ">89, 210/234, d=0.699, g=0.695\n",
            ">89, 211/234, d=0.695, g=0.712\n",
            ">89, 212/234, d=0.691, g=0.708\n",
            ">89, 213/234, d=0.691, g=0.693\n",
            ">89, 214/234, d=0.686, g=0.672\n",
            ">89, 215/234, d=0.686, g=0.682\n",
            ">89, 216/234, d=0.689, g=0.708\n",
            ">89, 217/234, d=0.695, g=0.712\n",
            ">89, 218/234, d=0.694, g=0.704\n",
            ">89, 219/234, d=0.696, g=0.684\n",
            ">89, 220/234, d=0.693, g=0.692\n",
            ">89, 221/234, d=0.691, g=0.718\n",
            ">89, 222/234, d=0.685, g=0.733\n",
            ">89, 223/234, d=0.692, g=0.717\n",
            ">89, 224/234, d=0.689, g=0.681\n",
            ">89, 225/234, d=0.692, g=0.677\n",
            ">89, 226/234, d=0.690, g=0.677\n",
            ">89, 227/234, d=0.693, g=0.686\n",
            ">89, 228/234, d=0.694, g=0.718\n",
            ">89, 229/234, d=0.688, g=0.724\n",
            ">89, 230/234, d=0.685, g=0.705\n",
            ">89, 231/234, d=0.692, g=0.691\n",
            ">89, 232/234, d=0.689, g=0.688\n",
            ">89, 233/234, d=0.692, g=0.698\n",
            ">89, 234/234, d=0.694, g=0.693\n",
            ">90, 1/234, d=0.695, g=0.710\n",
            ">Accuracy real: 23%, fake: 94%\n",
            ">90, 2/234, d=0.697, g=0.698\n",
            ">Accuracy real: 16%, fake: 93%\n",
            ">90, 3/234, d=0.696, g=0.701\n",
            ">Accuracy real: 16%, fake: 93%\n",
            ">90, 4/234, d=0.692, g=0.712\n",
            ">Accuracy real: 17%, fake: 97%\n",
            ">90, 5/234, d=0.689, g=0.714\n",
            ">Accuracy real: 6%, fake: 100%\n",
            ">90, 6/234, d=0.698, g=0.710\n",
            ">Accuracy real: 11%, fake: 100%\n",
            ">90, 7/234, d=0.688, g=0.697\n",
            ">Accuracy real: 23%, fake: 92%\n",
            ">90, 8/234, d=0.686, g=0.688\n",
            ">Accuracy real: 70%, fake: 59%\n",
            ">90, 9/234, d=0.686, g=0.677\n",
            ">Accuracy real: 78%, fake: 31%\n",
            ">90, 10/234, d=0.692, g=0.685\n",
            ">Accuracy real: 53%, fake: 54%\n",
            ">90, 11/234, d=0.690, g=0.738\n",
            ">Accuracy real: 0%, fake: 100%\n",
            ">90, 12/234, d=0.682, g=0.746\n",
            ">Accuracy real: 0%, fake: 100%\n",
            ">90, 13/234, d=0.689, g=0.726\n",
            ">Accuracy real: 0%, fake: 100%\n",
            ">90, 14/234, d=0.692, g=0.699\n",
            ">Accuracy real: 33%, fake: 95%\n",
            ">90, 15/234, d=0.694, g=0.680\n",
            ">Accuracy real: 49%, fake: 64%\n",
            ">90, 16/234, d=0.686, g=0.680\n",
            ">Accuracy real: 69%, fake: 55%\n",
            ">90, 17/234, d=0.686, g=0.687\n",
            ">Accuracy real: 68%, fake: 58%\n",
            ">90, 18/234, d=0.683, g=0.704\n",
            ">Accuracy real: 28%, fake: 92%\n",
            ">90, 19/234, d=0.688, g=0.697\n",
            ">Accuracy real: 25%, fake: 92%\n",
            ">90, 20/234, d=0.687, g=0.694\n",
            ">Accuracy real: 40%, fake: 71%\n",
            ">90, 21/234, d=0.686, g=0.711\n",
            ">Accuracy real: 28%, fake: 89%\n",
            ">90, 22/234, d=0.690, g=0.717\n",
            ">Accuracy real: 6%, fake: 98%\n",
            ">90, 23/234, d=0.691, g=0.717\n",
            ">Accuracy real: 4%, fake: 97%\n",
            ">90, 24/234, d=0.692, g=0.704\n",
            ">Accuracy real: 26%, fake: 93%\n",
            ">90, 25/234, d=0.693, g=0.698\n",
            ">Accuracy real: 30%, fake: 87%\n",
            ">90, 26/234, d=0.684, g=0.694\n",
            ">Accuracy real: 47%, fake: 73%\n",
            ">90, 27/234, d=0.691, g=0.692\n",
            ">Accuracy real: 39%, fake: 81%\n",
            ">90, 28/234, d=0.691, g=0.702\n",
            ">Accuracy real: 30%, fake: 82%\n",
            ">90, 29/234, d=0.697, g=0.710\n",
            ">Accuracy real: 18%, fake: 88%\n",
            ">90, 30/234, d=0.688, g=0.698\n",
            ">Accuracy real: 28%, fake: 86%\n",
            ">90, 31/234, d=0.694, g=0.688\n",
            ">Accuracy real: 43%, fake: 76%\n",
            ">90, 32/234, d=0.688, g=0.682\n",
            ">Accuracy real: 64%, fake: 58%\n",
            ">90, 33/234, d=0.693, g=0.681\n",
            ">Accuracy real: 71%, fake: 52%\n",
            ">90, 34/234, d=0.686, g=0.692\n",
            ">Accuracy real: 33%, fake: 87%\n",
            ">90, 35/234, d=0.695, g=0.713\n",
            ">Accuracy real: 17%, fake: 96%\n",
            ">90, 36/234, d=0.688, g=0.713\n",
            ">Accuracy real: 8%, fake: 100%\n",
            ">90, 37/234, d=0.694, g=0.709\n",
            ">Accuracy real: 14%, fake: 94%\n",
            ">90, 38/234, d=0.694, g=0.686\n",
            ">Accuracy real: 52%, fake: 62%\n",
            ">90, 39/234, d=0.692, g=0.685\n",
            ">Accuracy real: 55%, fake: 73%\n",
            ">90, 40/234, d=0.689, g=0.698\n",
            ">Accuracy real: 30%, fake: 88%\n",
            ">90, 41/234, d=0.693, g=0.718\n",
            ">Accuracy real: 4%, fake: 99%\n",
            ">90, 42/234, d=0.693, g=0.714\n",
            ">Accuracy real: 6%, fake: 98%\n",
            ">90, 43/234, d=0.689, g=0.700\n",
            ">Accuracy real: 29%, fake: 92%\n",
            ">90, 44/234, d=0.689, g=0.677\n",
            ">Accuracy real: 74%, fake: 39%\n",
            ">90, 45/234, d=0.687, g=0.671\n",
            ">Accuracy real: 77%, fake: 41%\n",
            ">90, 46/234, d=0.689, g=0.688\n",
            ">Accuracy real: 45%, fake: 75%\n",
            ">90, 47/234, d=0.697, g=0.709\n",
            ">Accuracy real: 9%, fake: 97%\n",
            ">90, 48/234, d=0.691, g=0.730\n",
            ">Accuracy real: 1%, fake: 100%\n",
            ">90, 49/234, d=0.694, g=0.718\n",
            ">Accuracy real: 6%, fake: 100%\n",
            ">90, 50/234, d=0.692, g=0.702\n",
            ">Accuracy real: 35%, fake: 91%\n",
            ">90, 51/234, d=0.693, g=0.683\n",
            ">Accuracy real: 41%, fake: 69%\n",
            ">90, 52/234, d=0.689, g=0.688\n",
            ">Accuracy real: 53%, fake: 66%\n",
            ">90, 53/234, d=0.696, g=0.694\n",
            ">Accuracy real: 42%, fake: 75%\n",
            ">90, 54/234, d=0.696, g=0.689\n",
            ">Accuracy real: 37%, fake: 88%\n",
            ">90, 55/234, d=0.691, g=0.704\n",
            ">Accuracy real: 25%, fake: 86%\n",
            ">90, 56/234, d=0.689, g=0.708\n",
            ">Accuracy real: 15%, fake: 95%\n",
            ">90, 57/234, d=0.689, g=0.697\n",
            ">Accuracy real: 21%, fake: 91%\n",
            ">90, 58/234, d=0.685, g=0.702\n",
            ">Accuracy real: 37%, fake: 86%\n",
            ">90, 59/234, d=0.691, g=0.690\n",
            ">Accuracy real: 45%, fake: 78%\n",
            ">90, 60/234, d=0.690, g=0.689\n",
            ">Accuracy real: 37%, fake: 80%\n",
            ">90, 61/234, d=0.685, g=0.686\n",
            ">Accuracy real: 31%, fake: 87%\n",
            ">90, 62/234, d=0.692, g=0.689\n",
            ">Accuracy real: 44%, fake: 84%\n",
            ">90, 63/234, d=0.690, g=0.698\n",
            ">Accuracy real: 33%, fake: 89%\n",
            ">90, 64/234, d=0.687, g=0.714\n",
            ">Accuracy real: 10%, fake: 98%\n",
            ">90, 65/234, d=0.688, g=0.717\n",
            ">Accuracy real: 6%, fake: 98%\n",
            ">90, 66/234, d=0.695, g=0.707\n",
            ">Accuracy real: 18%, fake: 90%\n",
            ">90, 67/234, d=0.688, g=0.687\n",
            ">Accuracy real: 65%, fake: 63%\n",
            ">90, 68/234, d=0.696, g=0.675\n",
            ">Accuracy real: 76%, fake: 54%\n",
            ">90, 69/234, d=0.690, g=0.680\n",
            ">Accuracy real: 72%, fake: 52%\n",
            ">90, 70/234, d=0.689, g=0.696\n",
            ">Accuracy real: 22%, fake: 95%\n",
            ">90, 71/234, d=0.698, g=0.726\n",
            ">Accuracy real: 1%, fake: 100%\n",
            ">90, 72/234, d=0.694, g=0.725\n",
            ">Accuracy real: 1%, fake: 100%\n",
            ">90, 73/234, d=0.685, g=0.692\n",
            ">Accuracy real: 38%, fake: 82%\n",
            ">90, 74/234, d=0.695, g=0.686\n",
            ">Accuracy real: 70%, fake: 53%\n",
            ">90, 75/234, d=0.689, g=0.700\n",
            ">Accuracy real: 42%, fake: 78%\n",
            ">90, 76/234, d=0.694, g=0.689\n",
            ">Accuracy real: 30%, fake: 91%\n",
            ">90, 77/234, d=0.689, g=0.699\n",
            ">Accuracy real: 14%, fake: 95%\n",
            ">90, 78/234, d=0.686, g=0.707\n",
            ">Accuracy real: 17%, fake: 98%\n",
            ">90, 79/234, d=0.692, g=0.704\n",
            ">Accuracy real: 32%, fake: 85%\n",
            ">90, 80/234, d=0.696, g=0.685\n",
            ">Accuracy real: 60%, fake: 58%\n",
            ">90, 81/234, d=0.689, g=0.683\n",
            ">Accuracy real: 44%, fake: 71%\n",
            ">90, 82/234, d=0.696, g=0.704\n",
            ">Accuracy real: 15%, fake: 96%\n",
            ">90, 83/234, d=0.699, g=0.731\n",
            ">Accuracy real: 0%, fake: 100%\n",
            ">90, 84/234, d=0.686, g=0.720\n",
            ">Accuracy real: 8%, fake: 100%\n",
            ">90, 85/234, d=0.687, g=0.691\n",
            ">Accuracy real: 45%, fake: 77%\n",
            ">90, 86/234, d=0.689, g=0.679\n",
            ">Accuracy real: 77%, fake: 49%\n",
            ">90, 87/234, d=0.689, g=0.684\n",
            ">Accuracy real: 71%, fake: 42%\n",
            ">90, 88/234, d=0.691, g=0.699\n",
            ">Accuracy real: 31%, fake: 91%\n",
            ">90, 89/234, d=0.691, g=0.704\n",
            ">Accuracy real: 9%, fake: 98%\n",
            ">90, 90/234, d=0.691, g=0.713\n",
            ">Accuracy real: 16%, fake: 99%\n",
            ">90, 91/234, d=0.688, g=0.712\n",
            ">Accuracy real: 13%, fake: 94%\n",
            ">90, 92/234, d=0.695, g=0.695\n",
            ">Accuracy real: 43%, fake: 75%\n",
            ">90, 93/234, d=0.692, g=0.686\n",
            ">Accuracy real: 34%, fake: 80%\n",
            ">90, 94/234, d=0.694, g=0.702\n",
            ">Accuracy real: 16%, fake: 90%\n",
            ">90, 95/234, d=0.684, g=0.719\n",
            ">Accuracy real: 7%, fake: 97%\n",
            ">90, 96/234, d=0.691, g=0.710\n",
            ">Accuracy real: 11%, fake: 95%\n",
            ">90, 97/234, d=0.690, g=0.692\n",
            ">Accuracy real: 34%, fake: 80%\n",
            ">90, 98/234, d=0.698, g=0.687\n",
            ">Accuracy real: 50%, fake: 68%\n",
            ">90, 99/234, d=0.691, g=0.695\n",
            ">Accuracy real: 32%, fake: 82%\n",
            ">90, 100/234, d=0.692, g=0.701\n",
            ">Accuracy real: 22%, fake: 86%\n",
            ">90, 101/234, d=0.692, g=0.703\n",
            ">Accuracy real: 36%, fake: 87%\n",
            ">90, 102/234, d=0.687, g=0.680\n",
            ">Accuracy real: 53%, fake: 68%\n",
            ">90, 103/234, d=0.693, g=0.690\n",
            ">Accuracy real: 23%, fake: 87%\n",
            ">90, 104/234, d=0.691, g=0.724\n",
            ">Accuracy real: 10%, fake: 99%\n",
            ">90, 105/234, d=0.691, g=0.727\n",
            ">Accuracy real: 7%, fake: 99%\n",
            ">90, 106/234, d=0.690, g=0.714\n",
            ">Accuracy real: 15%, fake: 97%\n",
            ">90, 107/234, d=0.695, g=0.701\n",
            ">Accuracy real: 21%, fake: 89%\n",
            ">90, 108/234, d=0.688, g=0.682\n",
            ">Accuracy real: 59%, fake: 46%\n",
            ">90, 109/234, d=0.695, g=0.666\n",
            ">Accuracy real: 87%, fake: 21%\n",
            ">90, 110/234, d=0.691, g=0.684\n",
            ">Accuracy real: 70%, fake: 44%\n",
            ">90, 111/234, d=0.694, g=0.696\n",
            ">Accuracy real: 48%, fake: 65%\n",
            ">90, 112/234, d=0.689, g=0.715\n",
            ">Accuracy real: 17%, fake: 94%\n",
            ">90, 113/234, d=0.693, g=0.727\n",
            ">Accuracy real: 4%, fake: 99%\n",
            ">90, 114/234, d=0.684, g=0.722\n",
            ">Accuracy real: 3%, fake: 100%\n",
            ">90, 115/234, d=0.694, g=0.723\n",
            ">Accuracy real: 12%, fake: 96%\n",
            ">90, 116/234, d=0.688, g=0.700\n",
            ">Accuracy real: 27%, fake: 79%\n",
            ">90, 117/234, d=0.684, g=0.707\n",
            ">Accuracy real: 27%, fake: 78%\n",
            ">90, 118/234, d=0.689, g=0.691\n",
            ">Accuracy real: 38%, fake: 79%\n",
            ">90, 119/234, d=0.695, g=0.694\n",
            ">Accuracy real: 32%, fake: 68%\n",
            ">90, 120/234, d=0.692, g=0.679\n",
            ">Accuracy real: 62%, fake: 51%\n",
            ">90, 121/234, d=0.688, g=0.686\n",
            ">Accuracy real: 70%, fake: 58%\n",
            ">90, 122/234, d=0.690, g=0.698\n",
            ">Accuracy real: 33%, fake: 84%\n",
            ">90, 123/234, d=0.688, g=0.702\n",
            ">Accuracy real: 17%, fake: 93%\n",
            ">90, 124/234, d=0.694, g=0.715\n",
            ">Accuracy real: 13%, fake: 99%\n",
            ">90, 125/234, d=0.686, g=0.725\n",
            ">Accuracy real: 2%, fake: 99%\n",
            ">90, 126/234, d=0.689, g=0.714\n",
            ">Accuracy real: 8%, fake: 99%\n",
            ">90, 127/234, d=0.697, g=0.694\n",
            ">Accuracy real: 42%, fake: 82%\n",
            ">90, 128/234, d=0.693, g=0.678\n",
            ">Accuracy real: 74%, fake: 46%\n",
            ">90, 129/234, d=0.698, g=0.675\n",
            ">Accuracy real: 83%, fake: 23%\n",
            ">90, 130/234, d=0.693, g=0.707\n",
            ">Accuracy real: 13%, fake: 97%\n",
            ">90, 131/234, d=0.692, g=0.724\n",
            ">Accuracy real: 0%, fake: 100%\n",
            ">90, 132/234, d=0.684, g=0.725\n",
            ">Accuracy real: 0%, fake: 100%\n",
            ">90, 133/234, d=0.692, g=0.695\n",
            ">Accuracy real: 42%, fake: 88%\n",
            ">90, 134/234, d=0.688, g=0.667\n",
            ">Accuracy real: 73%, fake: 36%\n",
            ">90, 135/234, d=0.694, g=0.684\n",
            ">Accuracy real: 54%, fake: 64%\n",
            ">90, 136/234, d=0.687, g=0.708\n",
            ">Accuracy real: 8%, fake: 100%\n",
            ">90, 137/234, d=0.690, g=0.731\n",
            ">Accuracy real: 1%, fake: 100%\n",
            ">90, 138/234, d=0.700, g=0.709\n",
            ">Accuracy real: 10%, fake: 98%\n",
            ">90, 139/234, d=0.693, g=0.690\n",
            ">Accuracy real: 51%, fake: 69%\n",
            ">90, 140/234, d=0.697, g=0.689\n",
            ">Accuracy real: 77%, fake: 55%\n",
            ">90, 141/234, d=0.689, g=0.694\n",
            ">Accuracy real: 42%, fake: 78%\n",
            ">90, 142/234, d=0.688, g=0.707\n",
            ">Accuracy real: 11%, fake: 100%\n",
            ">90, 143/234, d=0.695, g=0.716\n",
            ">Accuracy real: 7%, fake: 98%\n",
            ">90, 144/234, d=0.692, g=0.701\n",
            ">Accuracy real: 19%, fake: 90%\n",
            ">90, 145/234, d=0.692, g=0.688\n",
            ">Accuracy real: 57%, fake: 61%\n",
            ">90, 146/234, d=0.692, g=0.697\n",
            ">Accuracy real: 39%, fake: 79%\n",
            ">90, 147/234, d=0.695, g=0.722\n",
            ">Accuracy real: 3%, fake: 100%\n",
            ">90, 148/234, d=0.691, g=0.720\n",
            ">Accuracy real: 4%, fake: 100%\n",
            ">90, 149/234, d=0.693, g=0.702\n",
            ">Accuracy real: 53%, fake: 75%\n",
            ">90, 150/234, d=0.689, g=0.680\n",
            ">Accuracy real: 61%, fake: 41%\n",
            ">90, 151/234, d=0.695, g=0.689\n",
            ">Accuracy real: 57%, fake: 55%\n",
            ">90, 152/234, d=0.692, g=0.700\n",
            ">Accuracy real: 36%, fake: 87%\n",
            ">90, 153/234, d=0.694, g=0.711\n",
            ">Accuracy real: 10%, fake: 99%\n",
            ">90, 154/234, d=0.686, g=0.712\n",
            ">Accuracy real: 4%, fake: 97%\n",
            ">90, 155/234, d=0.692, g=0.696\n",
            ">Accuracy real: 23%, fake: 85%\n",
            ">90, 156/234, d=0.687, g=0.694\n",
            ">Accuracy real: 43%, fake: 79%\n",
            ">90, 157/234, d=0.698, g=0.699\n",
            ">Accuracy real: 21%, fake: 80%\n",
            ">90, 158/234, d=0.693, g=0.689\n",
            ">Accuracy real: 24%, fake: 86%\n",
            ">90, 159/234, d=0.684, g=0.703\n",
            ">Accuracy real: 23%, fake: 94%\n",
            ">90, 160/234, d=0.691, g=0.693\n",
            ">Accuracy real: 22%, fake: 92%\n",
            ">90, 161/234, d=0.691, g=0.714\n",
            ">Accuracy real: 11%, fake: 94%\n",
            ">90, 162/234, d=0.696, g=0.716\n",
            ">Accuracy real: 2%, fake: 99%\n",
            ">90, 163/234, d=0.699, g=0.718\n",
            ">Accuracy real: 4%, fake: 99%\n",
            ">90, 164/234, d=0.697, g=0.705\n",
            ">Accuracy real: 15%, fake: 99%\n",
            ">90, 165/234, d=0.691, g=0.686\n",
            ">Accuracy real: 51%, fake: 64%\n",
            ">90, 166/234, d=0.690, g=0.681\n",
            ">Accuracy real: 64%, fake: 54%\n",
            ">90, 167/234, d=0.691, g=0.681\n",
            ">Accuracy real: 56%, fake: 67%\n",
            ">90, 168/234, d=0.695, g=0.707\n",
            ">Accuracy real: 14%, fake: 97%\n",
            ">90, 169/234, d=0.687, g=0.700\n",
            ">Accuracy real: 14%, fake: 92%\n",
            ">90, 170/234, d=0.689, g=0.699\n",
            ">Accuracy real: 21%, fake: 95%\n",
            ">90, 171/234, d=0.690, g=0.710\n",
            ">Accuracy real: 26%, fake: 92%\n",
            ">90, 172/234, d=0.680, g=0.711\n",
            ">Accuracy real: 16%, fake: 93%\n",
            ">90, 173/234, d=0.689, g=0.709\n",
            ">Accuracy real: 26%, fake: 93%\n",
            ">90, 174/234, d=0.698, g=0.692\n",
            ">Accuracy real: 32%, fake: 83%\n",
            ">90, 175/234, d=0.687, g=0.675\n",
            ">Accuracy real: 67%, fake: 54%\n",
            ">90, 176/234, d=0.693, g=0.691\n",
            ">Accuracy real: 35%, fake: 78%\n",
            ">90, 177/234, d=0.691, g=0.712\n",
            ">Accuracy real: 15%, fake: 89%\n",
            ">90, 178/234, d=0.687, g=0.705\n",
            ">Accuracy real: 4%, fake: 98%\n",
            ">90, 179/234, d=0.690, g=0.703\n",
            ">Accuracy real: 35%, fake: 87%\n",
            ">90, 180/234, d=0.693, g=0.679\n",
            ">Accuracy real: 62%, fake: 48%\n",
            ">90, 181/234, d=0.693, g=0.683\n",
            ">Accuracy real: 75%, fake: 42%\n",
            ">90, 182/234, d=0.691, g=0.692\n",
            ">Accuracy real: 30%, fake: 90%\n",
            ">90, 183/234, d=0.690, g=0.716\n",
            ">Accuracy real: 5%, fake: 98%\n",
            ">90, 184/234, d=0.684, g=0.716\n",
            ">Accuracy real: 2%, fake: 100%\n",
            ">90, 185/234, d=0.685, g=0.711\n",
            ">Accuracy real: 15%, fake: 96%\n",
            ">90, 186/234, d=0.688, g=0.688\n",
            ">Accuracy real: 52%, fake: 69%\n",
            ">90, 187/234, d=0.690, g=0.688\n",
            ">Accuracy real: 66%, fake: 58%\n",
            ">90, 188/234, d=0.685, g=0.698\n",
            ">Accuracy real: 33%, fake: 90%\n",
            ">90, 189/234, d=0.698, g=0.720\n",
            ">Accuracy real: 6%, fake: 100%\n",
            ">90, 190/234, d=0.683, g=0.719\n",
            ">Accuracy real: 10%, fake: 100%\n",
            ">90, 191/234, d=0.692, g=0.695\n",
            ">Accuracy real: 41%, fake: 85%\n",
            ">90, 192/234, d=0.689, g=0.680\n",
            ">Accuracy real: 55%, fake: 65%\n",
            ">90, 193/234, d=0.689, g=0.680\n",
            ">Accuracy real: 61%, fake: 58%\n",
            ">90, 194/234, d=0.698, g=0.699\n",
            ">Accuracy real: 37%, fake: 87%\n",
            ">90, 195/234, d=0.700, g=0.711\n",
            ">Accuracy real: 4%, fake: 99%\n",
            ">90, 196/234, d=0.692, g=0.713\n",
            ">Accuracy real: 12%, fake: 97%\n",
            ">90, 197/234, d=0.694, g=0.704\n",
            ">Accuracy real: 22%, fake: 95%\n",
            ">90, 198/234, d=0.692, g=0.683\n",
            ">Accuracy real: 65%, fake: 47%\n",
            ">90, 199/234, d=0.687, g=0.686\n",
            ">Accuracy real: 44%, fake: 78%\n",
            ">90, 200/234, d=0.692, g=0.705\n",
            ">Accuracy real: 6%, fake: 99%\n",
            ">90, 201/234, d=0.690, g=0.704\n",
            ">Accuracy real: 15%, fake: 96%\n",
            ">90, 202/234, d=0.692, g=0.701\n",
            ">Accuracy real: 26%, fake: 94%\n",
            ">90, 203/234, d=0.692, g=0.707\n",
            ">Accuracy real: 17%, fake: 97%\n",
            ">90, 204/234, d=0.692, g=0.699\n",
            ">Accuracy real: 32%, fake: 81%\n",
            ">90, 205/234, d=0.693, g=0.706\n",
            ">Accuracy real: 11%, fake: 95%\n",
            ">90, 206/234, d=0.701, g=0.711\n",
            ">Accuracy real: 10%, fake: 99%\n",
            ">90, 207/234, d=0.692, g=0.713\n",
            ">Accuracy real: 3%, fake: 99%\n",
            ">90, 208/234, d=0.692, g=0.720\n",
            ">Accuracy real: 6%, fake: 98%\n",
            ">90, 209/234, d=0.688, g=0.711\n",
            ">Accuracy real: 14%, fake: 99%\n",
            ">90, 210/234, d=0.697, g=0.682\n",
            ">Accuracy real: 56%, fake: 71%\n",
            ">90, 211/234, d=0.693, g=0.690\n",
            ">Accuracy real: 35%, fake: 83%\n",
            ">90, 212/234, d=0.692, g=0.701\n",
            ">Accuracy real: 25%, fake: 93%\n",
            ">90, 213/234, d=0.695, g=0.714\n",
            ">Accuracy real: 6%, fake: 100%\n",
            ">90, 214/234, d=0.687, g=0.707\n",
            ">Accuracy real: 15%, fake: 98%\n",
            ">90, 215/234, d=0.693, g=0.694\n",
            ">Accuracy real: 30%, fake: 80%\n",
            ">90, 216/234, d=0.691, g=0.689\n",
            ">Accuracy real: 71%, fake: 51%\n",
            ">90, 217/234, d=0.688, g=0.685\n",
            ">Accuracy real: 57%, fake: 55%\n",
            ">90, 218/234, d=0.699, g=0.691\n",
            ">Accuracy real: 43%, fake: 82%\n",
            ">90, 219/234, d=0.694, g=0.709\n",
            ">Accuracy real: 8%, fake: 99%\n",
            ">90, 220/234, d=0.694, g=0.705\n",
            ">Accuracy real: 15%, fake: 98%\n",
            ">90, 221/234, d=0.698, g=0.711\n",
            ">Accuracy real: 10%, fake: 96%\n",
            ">90, 222/234, d=0.691, g=0.699\n",
            ">Accuracy real: 44%, fake: 72%\n",
            ">90, 223/234, d=0.691, g=0.685\n",
            ">Accuracy real: 73%, fake: 56%\n",
            ">90, 224/234, d=0.695, g=0.704\n",
            ">Accuracy real: 15%, fake: 95%\n",
            ">90, 225/234, d=0.693, g=0.728\n",
            ">Accuracy real: 1%, fake: 99%\n",
            ">90, 226/234, d=0.691, g=0.729\n",
            ">Accuracy real: 4%, fake: 100%\n",
            ">90, 227/234, d=0.693, g=0.710\n",
            ">Accuracy real: 11%, fake: 98%\n",
            ">90, 228/234, d=0.686, g=0.687\n",
            ">Accuracy real: 66%, fake: 67%\n",
            ">90, 229/234, d=0.691, g=0.666\n",
            ">Accuracy real: 88%, fake: 25%\n",
            ">90, 230/234, d=0.698, g=0.692\n",
            ">Accuracy real: 48%, fake: 78%\n",
            ">90, 231/234, d=0.687, g=0.715\n",
            ">Accuracy real: 8%, fake: 99%\n",
            ">90, 232/234, d=0.695, g=0.717\n",
            ">Accuracy real: 13%, fake: 94%\n",
            ">90, 233/234, d=0.679, g=0.700\n",
            ">Accuracy real: 34%, fake: 84%\n",
            ">90, 234/234, d=0.690, g=0.693\n",
            ">Accuracy real: 58%, fake: 72%\n",
            ">91, 1/234, d=0.690, g=0.688\n",
            ">91, 2/234, d=0.688, g=0.679\n",
            ">91, 3/234, d=0.694, g=0.691\n",
            ">91, 4/234, d=0.688, g=0.701\n",
            ">91, 5/234, d=0.688, g=0.697\n",
            ">91, 6/234, d=0.696, g=0.701\n",
            ">91, 7/234, d=0.690, g=0.710\n",
            ">91, 8/234, d=0.689, g=0.719\n",
            ">91, 9/234, d=0.692, g=0.710\n",
            ">91, 10/234, d=0.687, g=0.704\n",
            ">91, 11/234, d=0.694, g=0.697\n",
            ">91, 12/234, d=0.684, g=0.691\n",
            ">91, 13/234, d=0.690, g=0.701\n",
            ">91, 14/234, d=0.689, g=0.708\n",
            ">91, 15/234, d=0.693, g=0.695\n",
            ">91, 16/234, d=0.695, g=0.703\n",
            ">91, 17/234, d=0.691, g=0.695\n",
            ">91, 18/234, d=0.693, g=0.675\n",
            ">91, 19/234, d=0.693, g=0.686\n",
            ">91, 20/234, d=0.692, g=0.716\n",
            ">91, 21/234, d=0.691, g=0.738\n",
            ">91, 22/234, d=0.691, g=0.724\n",
            ">91, 23/234, d=0.686, g=0.686\n",
            ">91, 24/234, d=0.690, g=0.663\n",
            ">91, 25/234, d=0.690, g=0.679\n",
            ">91, 26/234, d=0.692, g=0.716\n",
            ">91, 27/234, d=0.692, g=0.743\n",
            ">91, 28/234, d=0.691, g=0.722\n",
            ">91, 29/234, d=0.690, g=0.692\n",
            ">91, 30/234, d=0.690, g=0.673\n",
            ">91, 31/234, d=0.692, g=0.667\n",
            ">91, 32/234, d=0.691, g=0.688\n",
            ">91, 33/234, d=0.691, g=0.733\n",
            ">91, 34/234, d=0.684, g=0.732\n",
            ">91, 35/234, d=0.698, g=0.706\n",
            ">91, 36/234, d=0.695, g=0.667\n",
            ">91, 37/234, d=0.697, g=0.681\n",
            ">91, 38/234, d=0.693, g=0.692\n",
            ">91, 39/234, d=0.688, g=0.707\n",
            ">91, 40/234, d=0.693, g=0.699\n",
            ">91, 41/234, d=0.682, g=0.687\n",
            ">91, 42/234, d=0.688, g=0.697\n",
            ">91, 43/234, d=0.698, g=0.701\n",
            ">91, 44/234, d=0.693, g=0.699\n",
            ">91, 45/234, d=0.684, g=0.698\n",
            ">91, 46/234, d=0.694, g=0.687\n",
            ">91, 47/234, d=0.693, g=0.682\n",
            ">91, 48/234, d=0.687, g=0.691\n",
            ">91, 49/234, d=0.691, g=0.711\n",
            ">91, 50/234, d=0.688, g=0.720\n",
            ">91, 51/234, d=0.693, g=0.699\n",
            ">91, 52/234, d=0.693, g=0.686\n",
            ">91, 53/234, d=0.688, g=0.685\n",
            ">91, 54/234, d=0.692, g=0.708\n",
            ">91, 55/234, d=0.689, g=0.714\n",
            ">91, 56/234, d=0.687, g=0.707\n",
            ">91, 57/234, d=0.685, g=0.710\n",
            ">91, 58/234, d=0.692, g=0.698\n",
            ">91, 59/234, d=0.697, g=0.686\n",
            ">91, 60/234, d=0.687, g=0.701\n",
            ">91, 61/234, d=0.693, g=0.709\n",
            ">91, 62/234, d=0.692, g=0.707\n",
            ">91, 63/234, d=0.687, g=0.695\n",
            ">91, 64/234, d=0.694, g=0.692\n",
            ">91, 65/234, d=0.693, g=0.695\n",
            ">91, 66/234, d=0.697, g=0.692\n",
            ">91, 67/234, d=0.689, g=0.687\n",
            ">91, 68/234, d=0.688, g=0.687\n",
            ">91, 69/234, d=0.691, g=0.702\n",
            ">91, 70/234, d=0.691, g=0.704\n",
            ">91, 71/234, d=0.689, g=0.708\n",
            ">91, 72/234, d=0.693, g=0.710\n",
            ">91, 73/234, d=0.688, g=0.707\n",
            ">91, 74/234, d=0.686, g=0.697\n",
            ">91, 75/234, d=0.695, g=0.702\n",
            ">91, 76/234, d=0.686, g=0.683\n",
            ">91, 77/234, d=0.692, g=0.687\n",
            ">91, 78/234, d=0.690, g=0.684\n",
            ">91, 79/234, d=0.696, g=0.693\n",
            ">91, 80/234, d=0.693, g=0.699\n",
            ">91, 81/234, d=0.694, g=0.704\n",
            ">91, 82/234, d=0.688, g=0.714\n",
            ">91, 83/234, d=0.691, g=0.729\n",
            ">91, 84/234, d=0.691, g=0.727\n",
            ">91, 85/234, d=0.680, g=0.713\n",
            ">91, 86/234, d=0.690, g=0.700\n",
            ">91, 87/234, d=0.686, g=0.668\n",
            ">91, 88/234, d=0.691, g=0.668\n",
            ">91, 89/234, d=0.696, g=0.681\n",
            ">91, 90/234, d=0.690, g=0.679\n",
            ">91, 91/234, d=0.686, g=0.697\n",
            ">91, 92/234, d=0.690, g=0.700\n",
            ">91, 93/234, d=0.691, g=0.711\n",
            ">91, 94/234, d=0.697, g=0.706\n",
            ">91, 95/234, d=0.691, g=0.711\n",
            ">91, 96/234, d=0.688, g=0.702\n",
            ">91, 97/234, d=0.693, g=0.697\n",
            ">91, 98/234, d=0.685, g=0.716\n",
            ">91, 99/234, d=0.694, g=0.703\n",
            ">91, 100/234, d=0.691, g=0.697\n",
            ">91, 101/234, d=0.690, g=0.685\n",
            ">91, 102/234, d=0.687, g=0.690\n",
            ">91, 103/234, d=0.694, g=0.707\n",
            ">91, 104/234, d=0.686, g=0.703\n",
            ">91, 105/234, d=0.690, g=0.697\n",
            ">91, 106/234, d=0.696, g=0.696\n",
            ">91, 107/234, d=0.698, g=0.719\n",
            ">91, 108/234, d=0.691, g=0.715\n",
            ">91, 109/234, d=0.694, g=0.704\n",
            ">91, 110/234, d=0.691, g=0.681\n",
            ">91, 111/234, d=0.690, g=0.667\n",
            ">91, 112/234, d=0.693, g=0.691\n",
            ">91, 113/234, d=0.699, g=0.705\n",
            ">91, 114/234, d=0.687, g=0.701\n",
            ">91, 115/234, d=0.683, g=0.685\n",
            ">91, 116/234, d=0.687, g=0.673\n",
            ">91, 117/234, d=0.695, g=0.685\n",
            ">91, 118/234, d=0.699, g=0.711\n",
            ">91, 119/234, d=0.694, g=0.720\n",
            ">91, 120/234, d=0.692, g=0.696\n",
            ">91, 121/234, d=0.684, g=0.689\n",
            ">91, 122/234, d=0.688, g=0.687\n",
            ">91, 123/234, d=0.693, g=0.685\n",
            ">91, 124/234, d=0.692, g=0.698\n",
            ">91, 125/234, d=0.695, g=0.706\n",
            ">91, 126/234, d=0.694, g=0.701\n",
            ">91, 127/234, d=0.691, g=0.694\n",
            ">91, 128/234, d=0.695, g=0.690\n",
            ">91, 129/234, d=0.691, g=0.690\n",
            ">91, 130/234, d=0.689, g=0.697\n",
            ">91, 131/234, d=0.689, g=0.694\n",
            ">91, 132/234, d=0.690, g=0.694\n",
            ">91, 133/234, d=0.694, g=0.690\n",
            ">91, 134/234, d=0.688, g=0.693\n",
            ">91, 135/234, d=0.692, g=0.705\n",
            ">91, 136/234, d=0.695, g=0.712\n",
            ">91, 137/234, d=0.697, g=0.721\n",
            ">91, 138/234, d=0.695, g=0.701\n",
            ">91, 139/234, d=0.697, g=0.683\n",
            ">91, 140/234, d=0.699, g=0.681\n",
            ">91, 141/234, d=0.693, g=0.687\n",
            ">91, 142/234, d=0.697, g=0.720\n",
            ">91, 143/234, d=0.692, g=0.710\n",
            ">91, 144/234, d=0.688, g=0.711\n",
            ">91, 145/234, d=0.692, g=0.692\n",
            ">91, 146/234, d=0.687, g=0.692\n",
            ">91, 147/234, d=0.698, g=0.691\n",
            ">91, 148/234, d=0.688, g=0.682\n",
            ">91, 149/234, d=0.685, g=0.694\n",
            ">91, 150/234, d=0.687, g=0.705\n",
            ">91, 151/234, d=0.692, g=0.724\n",
            ">91, 152/234, d=0.698, g=0.715\n",
            ">91, 153/234, d=0.689, g=0.698\n",
            ">91, 154/234, d=0.696, g=0.683\n",
            ">91, 155/234, d=0.691, g=0.696\n",
            ">91, 156/234, d=0.693, g=0.680\n",
            ">91, 157/234, d=0.703, g=0.707\n",
            ">91, 158/234, d=0.693, g=0.710\n",
            ">91, 159/234, d=0.694, g=0.698\n",
            ">91, 160/234, d=0.686, g=0.675\n",
            ">91, 161/234, d=0.693, g=0.687\n",
            ">91, 162/234, d=0.687, g=0.713\n",
            ">91, 163/234, d=0.693, g=0.734\n",
            ">91, 164/234, d=0.694, g=0.721\n",
            ">91, 165/234, d=0.684, g=0.696\n",
            ">91, 166/234, d=0.687, g=0.696\n",
            ">91, 167/234, d=0.694, g=0.693\n",
            ">91, 168/234, d=0.693, g=0.710\n",
            ">91, 169/234, d=0.699, g=0.721\n",
            ">91, 170/234, d=0.690, g=0.696\n",
            ">91, 171/234, d=0.683, g=0.695\n",
            ">91, 172/234, d=0.689, g=0.701\n",
            ">91, 173/234, d=0.695, g=0.703\n",
            ">91, 174/234, d=0.691, g=0.711\n",
            ">91, 175/234, d=0.693, g=0.716\n",
            ">91, 176/234, d=0.692, g=0.696\n",
            ">91, 177/234, d=0.690, g=0.689\n",
            ">91, 178/234, d=0.691, g=0.690\n",
            ">91, 179/234, d=0.684, g=0.699\n",
            ">91, 180/234, d=0.692, g=0.708\n",
            ">91, 181/234, d=0.692, g=0.734\n",
            ">91, 182/234, d=0.698, g=0.725\n",
            ">91, 183/234, d=0.685, g=0.691\n",
            ">91, 184/234, d=0.693, g=0.684\n",
            ">91, 185/234, d=0.693, g=0.680\n",
            ">91, 186/234, d=0.696, g=0.697\n",
            ">91, 187/234, d=0.694, g=0.730\n",
            ">91, 188/234, d=0.692, g=0.709\n",
            ">91, 189/234, d=0.691, g=0.694\n",
            ">91, 190/234, d=0.691, g=0.672\n",
            ">91, 191/234, d=0.682, g=0.659\n",
            ">91, 192/234, d=0.697, g=0.696\n",
            ">91, 193/234, d=0.698, g=0.746\n",
            ">91, 194/234, d=0.694, g=0.762\n",
            ">91, 195/234, d=0.695, g=0.715\n",
            ">91, 196/234, d=0.695, g=0.674\n",
            ">91, 197/234, d=0.681, g=0.659\n",
            ">91, 198/234, d=0.697, g=0.669\n",
            ">91, 199/234, d=0.689, g=0.713\n",
            ">91, 200/234, d=0.697, g=0.743\n",
            ">91, 201/234, d=0.693, g=0.734\n",
            ">91, 202/234, d=0.699, g=0.704\n",
            ">91, 203/234, d=0.695, g=0.690\n",
            ">91, 204/234, d=0.682, g=0.680\n",
            ">91, 205/234, d=0.687, g=0.687\n",
            ">91, 206/234, d=0.696, g=0.708\n",
            ">91, 207/234, d=0.691, g=0.726\n",
            ">91, 208/234, d=0.688, g=0.700\n",
            ">91, 209/234, d=0.694, g=0.686\n",
            ">91, 210/234, d=0.692, g=0.693\n",
            ">91, 211/234, d=0.694, g=0.710\n",
            ">91, 212/234, d=0.694, g=0.702\n",
            ">91, 213/234, d=0.694, g=0.703\n",
            ">91, 214/234, d=0.698, g=0.685\n",
            ">91, 215/234, d=0.692, g=0.688\n",
            ">91, 216/234, d=0.686, g=0.702\n",
            ">91, 217/234, d=0.691, g=0.699\n",
            ">91, 218/234, d=0.694, g=0.700\n",
            ">91, 219/234, d=0.693, g=0.694\n",
            ">91, 220/234, d=0.696, g=0.700\n",
            ">91, 221/234, d=0.690, g=0.710\n",
            ">91, 222/234, d=0.690, g=0.711\n",
            ">91, 223/234, d=0.693, g=0.687\n",
            ">91, 224/234, d=0.691, g=0.699\n",
            ">91, 225/234, d=0.695, g=0.708\n",
            ">91, 226/234, d=0.694, g=0.710\n",
            ">91, 227/234, d=0.691, g=0.694\n",
            ">91, 228/234, d=0.691, g=0.682\n",
            ">91, 229/234, d=0.691, g=0.692\n",
            ">91, 230/234, d=0.692, g=0.715\n",
            ">91, 231/234, d=0.701, g=0.724\n",
            ">91, 232/234, d=0.690, g=0.720\n",
            ">91, 233/234, d=0.682, g=0.702\n",
            ">91, 234/234, d=0.687, g=0.699\n",
            ">92, 1/234, d=0.699, g=0.699\n",
            ">92, 2/234, d=0.691, g=0.701\n",
            ">92, 3/234, d=0.693, g=0.687\n",
            ">92, 4/234, d=0.697, g=0.688\n",
            ">92, 5/234, d=0.697, g=0.705\n",
            ">92, 6/234, d=0.689, g=0.718\n",
            ">92, 7/234, d=0.691, g=0.712\n",
            ">92, 8/234, d=0.693, g=0.703\n",
            ">92, 9/234, d=0.689, g=0.685\n",
            ">92, 10/234, d=0.683, g=0.695\n",
            ">92, 11/234, d=0.693, g=0.700\n",
            ">92, 12/234, d=0.692, g=0.710\n",
            ">92, 13/234, d=0.700, g=0.711\n",
            ">92, 14/234, d=0.697, g=0.707\n",
            ">92, 15/234, d=0.689, g=0.685\n",
            ">92, 16/234, d=0.691, g=0.683\n",
            ">92, 17/234, d=0.697, g=0.689\n",
            ">92, 18/234, d=0.692, g=0.701\n",
            ">92, 19/234, d=0.691, g=0.693\n",
            ">92, 20/234, d=0.687, g=0.688\n",
            ">92, 21/234, d=0.693, g=0.707\n",
            ">92, 22/234, d=0.689, g=0.692\n",
            ">92, 23/234, d=0.697, g=0.711\n",
            ">92, 24/234, d=0.690, g=0.711\n",
            ">92, 25/234, d=0.694, g=0.710\n",
            ">92, 26/234, d=0.698, g=0.711\n",
            ">92, 27/234, d=0.694, g=0.700\n",
            ">92, 28/234, d=0.694, g=0.686\n",
            ">92, 29/234, d=0.690, g=0.678\n",
            ">92, 30/234, d=0.687, g=0.697\n",
            ">92, 31/234, d=0.697, g=0.704\n",
            ">92, 32/234, d=0.692, g=0.710\n",
            ">92, 33/234, d=0.694, g=0.710\n",
            ">92, 34/234, d=0.695, g=0.697\n",
            ">92, 35/234, d=0.701, g=0.682\n",
            ">92, 36/234, d=0.701, g=0.690\n",
            ">92, 37/234, d=0.686, g=0.685\n",
            ">92, 38/234, d=0.695, g=0.699\n",
            ">92, 39/234, d=0.693, g=0.713\n",
            ">92, 40/234, d=0.689, g=0.713\n",
            ">92, 41/234, d=0.694, g=0.708\n",
            ">92, 42/234, d=0.687, g=0.696\n",
            ">92, 43/234, d=0.697, g=0.704\n",
            ">92, 44/234, d=0.692, g=0.708\n",
            ">92, 45/234, d=0.698, g=0.708\n",
            ">92, 46/234, d=0.691, g=0.696\n",
            ">92, 47/234, d=0.692, g=0.691\n",
            ">92, 48/234, d=0.690, g=0.679\n",
            ">92, 49/234, d=0.694, g=0.698\n",
            ">92, 50/234, d=0.685, g=0.697\n",
            ">92, 51/234, d=0.696, g=0.722\n",
            ">92, 52/234, d=0.699, g=0.732\n",
            ">92, 53/234, d=0.683, g=0.716\n",
            ">92, 54/234, d=0.692, g=0.688\n",
            ">92, 55/234, d=0.692, g=0.689\n",
            ">92, 56/234, d=0.691, g=0.688\n",
            ">92, 57/234, d=0.696, g=0.704\n",
            ">92, 58/234, d=0.686, g=0.699\n",
            ">92, 59/234, d=0.694, g=0.702\n",
            ">92, 60/234, d=0.688, g=0.699\n",
            ">92, 61/234, d=0.687, g=0.699\n",
            ">92, 62/234, d=0.693, g=0.710\n",
            ">92, 63/234, d=0.691, g=0.706\n",
            ">92, 64/234, d=0.700, g=0.685\n",
            ">92, 65/234, d=0.698, g=0.685\n",
            ">92, 66/234, d=0.688, g=0.710\n",
            ">92, 67/234, d=0.690, g=0.726\n",
            ">92, 68/234, d=0.692, g=0.712\n",
            ">92, 69/234, d=0.685, g=0.699\n",
            ">92, 70/234, d=0.690, g=0.691\n",
            ">92, 71/234, d=0.689, g=0.687\n",
            ">92, 72/234, d=0.693, g=0.697\n",
            ">92, 73/234, d=0.696, g=0.707\n",
            ">92, 74/234, d=0.693, g=0.716\n",
            ">92, 75/234, d=0.698, g=0.692\n",
            ">92, 76/234, d=0.688, g=0.672\n",
            ">92, 77/234, d=0.690, g=0.668\n",
            ">92, 78/234, d=0.692, g=0.685\n",
            ">92, 79/234, d=0.689, g=0.701\n",
            ">92, 80/234, d=0.697, g=0.732\n",
            ">92, 81/234, d=0.686, g=0.738\n",
            ">92, 82/234, d=0.686, g=0.730\n",
            ">92, 83/234, d=0.687, g=0.702\n",
            ">92, 84/234, d=0.689, g=0.690\n",
            ">92, 85/234, d=0.698, g=0.672\n",
            ">92, 86/234, d=0.694, g=0.672\n",
            ">92, 87/234, d=0.693, g=0.684\n",
            ">92, 88/234, d=0.696, g=0.697\n",
            ">92, 89/234, d=0.693, g=0.708\n",
            ">92, 90/234, d=0.696, g=0.721\n",
            ">92, 91/234, d=0.691, g=0.722\n",
            ">92, 92/234, d=0.691, g=0.724\n",
            ">92, 93/234, d=0.693, g=0.695\n",
            ">92, 94/234, d=0.695, g=0.684\n",
            ">92, 95/234, d=0.694, g=0.703\n",
            ">92, 96/234, d=0.690, g=0.709\n",
            ">92, 97/234, d=0.691, g=0.719\n",
            ">92, 98/234, d=0.693, g=0.704\n",
            ">92, 99/234, d=0.694, g=0.687\n",
            ">92, 100/234, d=0.689, g=0.675\n",
            ">92, 101/234, d=0.685, g=0.691\n",
            ">92, 102/234, d=0.699, g=0.715\n",
            ">92, 103/234, d=0.687, g=0.713\n",
            ">92, 104/234, d=0.695, g=0.700\n",
            ">92, 105/234, d=0.693, g=0.693\n",
            ">92, 106/234, d=0.691, g=0.709\n",
            ">92, 107/234, d=0.691, g=0.720\n",
            ">92, 108/234, d=0.692, g=0.695\n",
            ">92, 109/234, d=0.691, g=0.685\n",
            ">92, 110/234, d=0.691, g=0.679\n",
            ">92, 111/234, d=0.694, g=0.687\n",
            ">92, 112/234, d=0.684, g=0.713\n",
            ">92, 113/234, d=0.688, g=0.720\n",
            ">92, 114/234, d=0.690, g=0.720\n",
            ">92, 115/234, d=0.696, g=0.704\n",
            ">92, 116/234, d=0.691, g=0.684\n",
            ">92, 117/234, d=0.685, g=0.684\n",
            ">92, 118/234, d=0.695, g=0.696\n",
            ">92, 119/234, d=0.688, g=0.697\n",
            ">92, 120/234, d=0.689, g=0.722\n",
            ">92, 121/234, d=0.689, g=0.717\n",
            ">92, 122/234, d=0.688, g=0.714\n",
            ">92, 123/234, d=0.690, g=0.694\n",
            ">92, 124/234, d=0.690, g=0.695\n",
            ">92, 125/234, d=0.697, g=0.697\n",
            ">92, 126/234, d=0.687, g=0.718\n",
            ">92, 127/234, d=0.694, g=0.720\n",
            ">92, 128/234, d=0.691, g=0.703\n",
            ">92, 129/234, d=0.691, g=0.679\n",
            ">92, 130/234, d=0.694, g=0.682\n",
            ">92, 131/234, d=0.696, g=0.695\n",
            ">92, 132/234, d=0.688, g=0.718\n",
            ">92, 133/234, d=0.688, g=0.699\n",
            ">92, 134/234, d=0.690, g=0.677\n",
            ">92, 135/234, d=0.693, g=0.684\n",
            ">92, 136/234, d=0.692, g=0.701\n",
            ">92, 137/234, d=0.684, g=0.725\n",
            ">92, 138/234, d=0.687, g=0.732\n",
            ">92, 139/234, d=0.694, g=0.714\n",
            ">92, 140/234, d=0.693, g=0.692\n",
            ">92, 141/234, d=0.691, g=0.658\n",
            ">92, 142/234, d=0.692, g=0.660\n",
            ">92, 143/234, d=0.690, g=0.700\n",
            ">92, 144/234, d=0.694, g=0.763\n",
            ">92, 145/234, d=0.689, g=0.772\n",
            ">92, 146/234, d=0.692, g=0.735\n",
            ">92, 147/234, d=0.688, g=0.672\n",
            ">92, 148/234, d=0.695, g=0.651\n",
            ">92, 149/234, d=0.690, g=0.651\n",
            ">92, 150/234, d=0.689, g=0.667\n",
            ">92, 151/234, d=0.696, g=0.713\n",
            ">92, 152/234, d=0.695, g=0.767\n",
            ">92, 153/234, d=0.683, g=0.779\n",
            ">92, 154/234, d=0.689, g=0.738\n",
            ">92, 155/234, d=0.695, g=0.695\n",
            ">92, 156/234, d=0.695, g=0.663\n",
            ">92, 157/234, d=0.684, g=0.655\n",
            ">92, 158/234, d=0.694, g=0.680\n",
            ">92, 159/234, d=0.689, g=0.736\n",
            ">92, 160/234, d=0.695, g=0.748\n",
            ">92, 161/234, d=0.693, g=0.743\n",
            ">92, 162/234, d=0.693, g=0.718\n",
            ">92, 163/234, d=0.691, g=0.686\n",
            ">92, 164/234, d=0.686, g=0.670\n",
            ">92, 165/234, d=0.692, g=0.680\n",
            ">92, 166/234, d=0.689, g=0.694\n",
            ">92, 167/234, d=0.694, g=0.709\n",
            ">92, 168/234, d=0.692, g=0.701\n",
            ">92, 169/234, d=0.689, g=0.692\n",
            ">92, 170/234, d=0.686, g=0.680\n",
            ">92, 171/234, d=0.686, g=0.684\n",
            ">92, 172/234, d=0.691, g=0.704\n",
            ">92, 173/234, d=0.690, g=0.730\n",
            ">92, 174/234, d=0.698, g=0.733\n",
            ">92, 175/234, d=0.699, g=0.702\n",
            ">92, 176/234, d=0.695, g=0.684\n",
            ">92, 177/234, d=0.693, g=0.676\n",
            ">92, 178/234, d=0.689, g=0.695\n",
            ">92, 179/234, d=0.690, g=0.697\n",
            ">92, 180/234, d=0.694, g=0.700\n",
            ">92, 181/234, d=0.694, g=0.714\n",
            ">92, 182/234, d=0.690, g=0.703\n",
            ">92, 183/234, d=0.694, g=0.685\n",
            ">92, 184/234, d=0.693, g=0.689\n",
            ">92, 185/234, d=0.688, g=0.697\n",
            ">92, 186/234, d=0.691, g=0.707\n",
            ">92, 187/234, d=0.694, g=0.701\n",
            ">92, 188/234, d=0.691, g=0.690\n",
            ">92, 189/234, d=0.693, g=0.692\n",
            ">92, 190/234, d=0.689, g=0.679\n",
            ">92, 191/234, d=0.693, g=0.684\n",
            ">92, 192/234, d=0.689, g=0.705\n",
            ">92, 193/234, d=0.691, g=0.699\n",
            ">92, 194/234, d=0.689, g=0.686\n",
            ">92, 195/234, d=0.694, g=0.687\n",
            ">92, 196/234, d=0.688, g=0.697\n",
            ">92, 197/234, d=0.692, g=0.715\n",
            ">92, 198/234, d=0.694, g=0.702\n",
            ">92, 199/234, d=0.693, g=0.698\n",
            ">92, 200/234, d=0.698, g=0.689\n",
            ">92, 201/234, d=0.684, g=0.710\n",
            ">92, 202/234, d=0.687, g=0.718\n",
            ">92, 203/234, d=0.697, g=0.704\n",
            ">92, 204/234, d=0.687, g=0.690\n",
            ">92, 205/234, d=0.690, g=0.692\n",
            ">92, 206/234, d=0.693, g=0.690\n",
            ">92, 207/234, d=0.696, g=0.691\n",
            ">92, 208/234, d=0.684, g=0.689\n",
            ">92, 209/234, d=0.692, g=0.700\n",
            ">92, 210/234, d=0.685, g=0.710\n",
            ">92, 211/234, d=0.691, g=0.725\n",
            ">92, 212/234, d=0.684, g=0.713\n",
            ">92, 213/234, d=0.694, g=0.702\n",
            ">92, 214/234, d=0.692, g=0.704\n",
            ">92, 215/234, d=0.691, g=0.693\n",
            ">92, 216/234, d=0.687, g=0.692\n",
            ">92, 217/234, d=0.686, g=0.700\n",
            ">92, 218/234, d=0.685, g=0.705\n",
            ">92, 219/234, d=0.684, g=0.699\n",
            ">92, 220/234, d=0.687, g=0.698\n",
            ">92, 221/234, d=0.693, g=0.694\n",
            ">92, 222/234, d=0.700, g=0.704\n",
            ">92, 223/234, d=0.687, g=0.699\n",
            ">92, 224/234, d=0.694, g=0.674\n",
            ">92, 225/234, d=0.687, g=0.668\n",
            ">92, 226/234, d=0.696, g=0.705\n",
            ">92, 227/234, d=0.690, g=0.737\n",
            ">92, 228/234, d=0.694, g=0.731\n",
            ">92, 229/234, d=0.691, g=0.703\n",
            ">92, 230/234, d=0.692, g=0.662\n",
            ">92, 231/234, d=0.690, g=0.662\n",
            ">92, 232/234, d=0.684, g=0.677\n",
            ">92, 233/234, d=0.697, g=0.710\n",
            ">92, 234/234, d=0.690, g=0.750\n",
            ">93, 1/234, d=0.690, g=0.763\n",
            ">93, 2/234, d=0.689, g=0.743\n",
            ">93, 3/234, d=0.694, g=0.692\n",
            ">93, 4/234, d=0.692, g=0.658\n",
            ">93, 5/234, d=0.694, g=0.669\n",
            ">93, 6/234, d=0.690, g=0.701\n",
            ">93, 7/234, d=0.696, g=0.724\n",
            ">93, 8/234, d=0.689, g=0.737\n",
            ">93, 9/234, d=0.686, g=0.716\n",
            ">93, 10/234, d=0.691, g=0.687\n",
            ">93, 11/234, d=0.695, g=0.687\n",
            ">93, 12/234, d=0.685, g=0.704\n",
            ">93, 13/234, d=0.693, g=0.737\n",
            ">93, 14/234, d=0.684, g=0.738\n",
            ">93, 15/234, d=0.687, g=0.710\n",
            ">93, 16/234, d=0.692, g=0.681\n",
            ">93, 17/234, d=0.691, g=0.659\n",
            ">93, 18/234, d=0.690, g=0.660\n",
            ">93, 19/234, d=0.690, g=0.680\n",
            ">93, 20/234, d=0.700, g=0.732\n",
            ">93, 21/234, d=0.690, g=0.779\n",
            ">93, 22/234, d=0.685, g=0.798\n",
            ">93, 23/234, d=0.690, g=0.772\n",
            ">93, 24/234, d=0.691, g=0.703\n",
            ">93, 25/234, d=0.701, g=0.661\n",
            ">93, 26/234, d=0.688, g=0.645\n",
            ">93, 27/234, d=0.701, g=0.660\n",
            ">93, 28/234, d=0.683, g=0.689\n",
            ">93, 29/234, d=0.688, g=0.715\n",
            ">93, 30/234, d=0.689, g=0.722\n",
            ">93, 31/234, d=0.695, g=0.756\n",
            ">93, 32/234, d=0.700, g=0.771\n",
            ">93, 33/234, d=0.694, g=0.778\n",
            ">93, 34/234, d=0.678, g=0.757\n",
            ">93, 35/234, d=0.682, g=0.716\n",
            ">93, 36/234, d=0.678, g=0.674\n",
            ">93, 37/234, d=0.703, g=0.663\n",
            ">93, 38/234, d=0.696, g=0.679\n",
            ">93, 39/234, d=0.688, g=0.705\n",
            ">93, 40/234, d=0.681, g=0.723\n",
            ">93, 41/234, d=0.675, g=0.712\n",
            ">93, 42/234, d=0.689, g=0.683\n",
            ">93, 43/234, d=0.691, g=0.682\n",
            ">93, 44/234, d=0.698, g=0.703\n",
            ">93, 45/234, d=0.700, g=0.769\n",
            ">93, 46/234, d=0.684, g=0.794\n",
            ">93, 47/234, d=0.680, g=0.792\n",
            ">93, 48/234, d=0.678, g=0.772\n",
            ">93, 49/234, d=0.686, g=0.754\n",
            ">93, 50/234, d=0.692, g=0.707\n",
            ">93, 51/234, d=0.704, g=0.662\n",
            ">93, 52/234, d=0.710, g=0.645\n",
            ">93, 53/234, d=0.709, g=0.642\n",
            ">93, 54/234, d=0.686, g=0.657\n",
            ">93, 55/234, d=0.670, g=0.672\n",
            ">93, 56/234, d=0.678, g=0.705\n",
            ">93, 57/234, d=0.692, g=0.740\n",
            ">93, 58/234, d=0.699, g=0.793\n",
            ">93, 59/234, d=0.706, g=0.805\n",
            ">93, 60/234, d=0.700, g=0.793\n",
            ">93, 61/234, d=0.700, g=0.770\n",
            ">93, 62/234, d=0.679, g=0.752\n",
            ">93, 63/234, d=0.674, g=0.742\n",
            ">93, 64/234, d=0.673, g=0.721\n",
            ">93, 65/234, d=0.681, g=0.721\n",
            ">93, 66/234, d=0.694, g=0.698\n",
            ">93, 67/234, d=0.695, g=0.682\n",
            ">93, 68/234, d=0.698, g=0.698\n",
            ">93, 69/234, d=0.695, g=0.700\n",
            ">93, 70/234, d=0.681, g=0.709\n",
            ">93, 71/234, d=0.678, g=0.709\n",
            ">93, 72/234, d=0.688, g=0.699\n",
            ">93, 73/234, d=0.690, g=0.706\n",
            ">93, 74/234, d=0.698, g=0.722\n",
            ">93, 75/234, d=0.705, g=0.741\n",
            ">93, 76/234, d=0.695, g=0.749\n",
            ">93, 77/234, d=0.690, g=0.742\n",
            ">93, 78/234, d=0.696, g=0.718\n",
            ">93, 79/234, d=0.691, g=0.705\n",
            ">93, 80/234, d=0.691, g=0.692\n",
            ">93, 81/234, d=0.694, g=0.694\n",
            ">93, 82/234, d=0.698, g=0.703\n",
            ">93, 83/234, d=0.693, g=0.694\n",
            ">93, 84/234, d=0.702, g=0.688\n",
            ">93, 85/234, d=0.701, g=0.673\n",
            ">93, 86/234, d=0.690, g=0.683\n",
            ">93, 87/234, d=0.694, g=0.692\n",
            ">93, 88/234, d=0.694, g=0.694\n",
            ">93, 89/234, d=0.691, g=0.694\n",
            ">93, 90/234, d=0.687, g=0.682\n",
            ">93, 91/234, d=0.696, g=0.688\n",
            ">93, 92/234, d=0.693, g=0.700\n",
            ">93, 93/234, d=0.692, g=0.704\n",
            ">93, 94/234, d=0.700, g=0.701\n",
            ">93, 95/234, d=0.691, g=0.714\n",
            ">93, 96/234, d=0.694, g=0.712\n",
            ">93, 97/234, d=0.699, g=0.699\n",
            ">93, 98/234, d=0.693, g=0.682\n",
            ">93, 99/234, d=0.689, g=0.674\n",
            ">93, 100/234, d=0.695, g=0.679\n",
            ">93, 101/234, d=0.697, g=0.686\n",
            ">93, 102/234, d=0.695, g=0.697\n",
            ">93, 103/234, d=0.691, g=0.700\n",
            ">93, 104/234, d=0.696, g=0.692\n",
            ">93, 105/234, d=0.696, g=0.698\n",
            ">93, 106/234, d=0.692, g=0.707\n",
            ">93, 107/234, d=0.693, g=0.700\n",
            ">93, 108/234, d=0.700, g=0.697\n",
            ">93, 109/234, d=0.692, g=0.684\n",
            ">93, 110/234, d=0.693, g=0.685\n",
            ">93, 111/234, d=0.697, g=0.701\n",
            ">93, 112/234, d=0.691, g=0.702\n",
            ">93, 113/234, d=0.693, g=0.707\n",
            ">93, 114/234, d=0.692, g=0.697\n",
            ">93, 115/234, d=0.695, g=0.686\n",
            ">93, 116/234, d=0.698, g=0.686\n",
            ">93, 117/234, d=0.698, g=0.697\n",
            ">93, 118/234, d=0.690, g=0.705\n",
            ">93, 119/234, d=0.693, g=0.688\n",
            ">93, 120/234, d=0.689, g=0.690\n",
            ">93, 121/234, d=0.697, g=0.691\n",
            ">93, 122/234, d=0.694, g=0.701\n",
            ">93, 123/234, d=0.690, g=0.691\n",
            ">93, 124/234, d=0.691, g=0.693\n",
            ">93, 125/234, d=0.685, g=0.707\n",
            ">93, 126/234, d=0.693, g=0.694\n",
            ">93, 127/234, d=0.691, g=0.690\n",
            ">93, 128/234, d=0.692, g=0.709\n",
            ">93, 129/234, d=0.696, g=0.719\n",
            ">93, 130/234, d=0.692, g=0.711\n",
            ">93, 131/234, d=0.687, g=0.681\n",
            ">93, 132/234, d=0.694, g=0.691\n",
            ">93, 133/234, d=0.695, g=0.685\n",
            ">93, 134/234, d=0.692, g=0.683\n",
            ">93, 135/234, d=0.688, g=0.700\n",
            ">93, 136/234, d=0.686, g=0.710\n",
            ">93, 137/234, d=0.689, g=0.708\n",
            ">93, 138/234, d=0.688, g=0.696\n",
            ">93, 139/234, d=0.690, g=0.694\n",
            ">93, 140/234, d=0.687, g=0.693\n",
            ">93, 141/234, d=0.686, g=0.701\n",
            ">93, 142/234, d=0.690, g=0.702\n",
            ">93, 143/234, d=0.689, g=0.719\n",
            ">93, 144/234, d=0.697, g=0.713\n",
            ">93, 145/234, d=0.688, g=0.700\n",
            ">93, 146/234, d=0.693, g=0.696\n",
            ">93, 147/234, d=0.688, g=0.702\n",
            ">93, 148/234, d=0.688, g=0.696\n",
            ">93, 149/234, d=0.690, g=0.698\n",
            ">93, 150/234, d=0.690, g=0.701\n",
            ">93, 151/234, d=0.691, g=0.689\n",
            ">93, 152/234, d=0.699, g=0.694\n",
            ">93, 153/234, d=0.696, g=0.693\n",
            ">93, 154/234, d=0.695, g=0.698\n",
            ">93, 155/234, d=0.679, g=0.695\n",
            ">93, 156/234, d=0.694, g=0.704\n",
            ">93, 157/234, d=0.695, g=0.714\n",
            ">93, 158/234, d=0.691, g=0.726\n",
            ">93, 159/234, d=0.693, g=0.711\n",
            ">93, 160/234, d=0.691, g=0.697\n",
            ">93, 161/234, d=0.690, g=0.686\n",
            ">93, 162/234, d=0.694, g=0.694\n",
            ">93, 163/234, d=0.689, g=0.687\n",
            ">93, 164/234, d=0.690, g=0.694\n",
            ">93, 165/234, d=0.690, g=0.688\n",
            ">93, 166/234, d=0.688, g=0.702\n",
            ">93, 167/234, d=0.689, g=0.707\n",
            ">93, 168/234, d=0.688, g=0.703\n",
            ">93, 169/234, d=0.687, g=0.706\n",
            ">93, 170/234, d=0.686, g=0.701\n",
            ">93, 171/234, d=0.689, g=0.693\n",
            ">93, 172/234, d=0.700, g=0.697\n",
            ">93, 173/234, d=0.695, g=0.692\n",
            ">93, 174/234, d=0.688, g=0.712\n",
            ">93, 175/234, d=0.681, g=0.705\n",
            ">93, 176/234, d=0.681, g=0.703\n",
            ">93, 177/234, d=0.692, g=0.697\n",
            ">93, 178/234, d=0.696, g=0.680\n",
            ">93, 179/234, d=0.689, g=0.699\n",
            ">93, 180/234, d=0.688, g=0.712\n",
            ">93, 181/234, d=0.695, g=0.713\n",
            ">93, 182/234, d=0.695, g=0.709\n",
            ">93, 183/234, d=0.692, g=0.689\n",
            ">93, 184/234, d=0.694, g=0.679\n",
            ">93, 185/234, d=0.689, g=0.688\n",
            ">93, 186/234, d=0.690, g=0.704\n",
            ">93, 187/234, d=0.688, g=0.706\n",
            ">93, 188/234, d=0.692, g=0.712\n",
            ">93, 189/234, d=0.685, g=0.712\n",
            ">93, 190/234, d=0.684, g=0.694\n",
            ">93, 191/234, d=0.694, g=0.683\n",
            ">93, 192/234, d=0.685, g=0.696\n",
            ">93, 193/234, d=0.690, g=0.697\n",
            ">93, 194/234, d=0.699, g=0.704\n",
            ">93, 195/234, d=0.694, g=0.711\n",
            ">93, 196/234, d=0.693, g=0.708\n",
            ">93, 197/234, d=0.691, g=0.700\n",
            ">93, 198/234, d=0.692, g=0.687\n",
            ">93, 199/234, d=0.696, g=0.682\n",
            ">93, 200/234, d=0.696, g=0.695\n",
            ">93, 201/234, d=0.689, g=0.705\n",
            ">93, 202/234, d=0.696, g=0.718\n",
            ">93, 203/234, d=0.697, g=0.715\n",
            ">93, 204/234, d=0.693, g=0.703\n",
            ">93, 205/234, d=0.687, g=0.690\n",
            ">93, 206/234, d=0.690, g=0.687\n",
            ">93, 207/234, d=0.689, g=0.690\n",
            ">93, 208/234, d=0.694, g=0.713\n",
            ">93, 209/234, d=0.688, g=0.691\n",
            ">93, 210/234, d=0.689, g=0.691\n",
            ">93, 211/234, d=0.690, g=0.707\n",
            ">93, 212/234, d=0.694, g=0.724\n",
            ">93, 213/234, d=0.692, g=0.707\n",
            ">93, 214/234, d=0.687, g=0.688\n",
            ">93, 215/234, d=0.691, g=0.682\n",
            ">93, 216/234, d=0.687, g=0.689\n",
            ">93, 217/234, d=0.695, g=0.688\n",
            ">93, 218/234, d=0.690, g=0.693\n",
            ">93, 219/234, d=0.689, g=0.706\n",
            ">93, 220/234, d=0.695, g=0.715\n",
            ">93, 221/234, d=0.689, g=0.722\n",
            ">93, 222/234, d=0.687, g=0.713\n",
            ">93, 223/234, d=0.692, g=0.693\n",
            ">93, 224/234, d=0.695, g=0.680\n",
            ">93, 225/234, d=0.688, g=0.674\n",
            ">93, 226/234, d=0.695, g=0.698\n",
            ">93, 227/234, d=0.697, g=0.702\n",
            ">93, 228/234, d=0.693, g=0.695\n",
            ">93, 229/234, d=0.692, g=0.699\n",
            ">93, 230/234, d=0.689, g=0.698\n",
            ">93, 231/234, d=0.690, g=0.704\n",
            ">93, 232/234, d=0.694, g=0.722\n",
            ">93, 233/234, d=0.689, g=0.720\n",
            ">93, 234/234, d=0.697, g=0.713\n",
            ">94, 1/234, d=0.692, g=0.707\n",
            ">94, 2/234, d=0.687, g=0.690\n",
            ">94, 3/234, d=0.699, g=0.678\n",
            ">94, 4/234, d=0.695, g=0.693\n",
            ">94, 5/234, d=0.698, g=0.695\n",
            ">94, 6/234, d=0.689, g=0.709\n",
            ">94, 7/234, d=0.689, g=0.714\n",
            ">94, 8/234, d=0.693, g=0.711\n",
            ">94, 9/234, d=0.685, g=0.697\n",
            ">94, 10/234, d=0.689, g=0.689\n",
            ">94, 11/234, d=0.689, g=0.683\n",
            ">94, 12/234, d=0.691, g=0.689\n",
            ">94, 13/234, d=0.692, g=0.711\n",
            ">94, 14/234, d=0.693, g=0.717\n",
            ">94, 15/234, d=0.686, g=0.715\n",
            ">94, 16/234, d=0.696, g=0.704\n",
            ">94, 17/234, d=0.695, g=0.693\n",
            ">94, 18/234, d=0.697, g=0.690\n",
            ">94, 19/234, d=0.692, g=0.689\n",
            ">94, 20/234, d=0.692, g=0.715\n",
            ">94, 21/234, d=0.693, g=0.711\n",
            ">94, 22/234, d=0.691, g=0.688\n",
            ">94, 23/234, d=0.688, g=0.684\n",
            ">94, 24/234, d=0.689, g=0.694\n",
            ">94, 25/234, d=0.687, g=0.707\n",
            ">94, 26/234, d=0.691, g=0.707\n",
            ">94, 27/234, d=0.690, g=0.696\n",
            ">94, 28/234, d=0.692, g=0.704\n",
            ">94, 29/234, d=0.687, g=0.694\n",
            ">94, 30/234, d=0.700, g=0.705\n",
            ">94, 31/234, d=0.696, g=0.705\n",
            ">94, 32/234, d=0.691, g=0.706\n",
            ">94, 33/234, d=0.696, g=0.703\n",
            ">94, 34/234, d=0.691, g=0.712\n",
            ">94, 35/234, d=0.690, g=0.696\n",
            ">94, 36/234, d=0.692, g=0.679\n",
            ">94, 37/234, d=0.687, g=0.682\n",
            ">94, 38/234, d=0.694, g=0.690\n",
            ">94, 39/234, d=0.691, g=0.701\n",
            ">94, 40/234, d=0.697, g=0.699\n",
            ">94, 41/234, d=0.690, g=0.696\n",
            ">94, 42/234, d=0.691, g=0.692\n",
            ">94, 43/234, d=0.697, g=0.689\n",
            ">94, 44/234, d=0.690, g=0.705\n",
            ">94, 45/234, d=0.689, g=0.696\n",
            ">94, 46/234, d=0.688, g=0.698\n",
            ">94, 47/234, d=0.688, g=0.700\n",
            ">94, 48/234, d=0.689, g=0.688\n",
            ">94, 49/234, d=0.696, g=0.697\n",
            ">94, 50/234, d=0.693, g=0.692\n",
            ">94, 51/234, d=0.691, g=0.697\n",
            ">94, 52/234, d=0.694, g=0.695\n",
            ">94, 53/234, d=0.684, g=0.697\n",
            ">94, 54/234, d=0.690, g=0.704\n",
            ">94, 55/234, d=0.691, g=0.703\n",
            ">94, 56/234, d=0.691, g=0.701\n",
            ">94, 57/234, d=0.694, g=0.693\n",
            ">94, 58/234, d=0.694, g=0.689\n",
            ">94, 59/234, d=0.690, g=0.692\n",
            ">94, 60/234, d=0.691, g=0.693\n",
            ">94, 61/234, d=0.687, g=0.697\n",
            ">94, 62/234, d=0.688, g=0.707\n",
            ">94, 63/234, d=0.698, g=0.708\n",
            ">94, 64/234, d=0.694, g=0.706\n",
            ">94, 65/234, d=0.701, g=0.703\n",
            ">94, 66/234, d=0.699, g=0.690\n",
            ">94, 67/234, d=0.695, g=0.691\n",
            ">94, 68/234, d=0.691, g=0.715\n",
            ">94, 69/234, d=0.691, g=0.734\n",
            ">94, 70/234, d=0.688, g=0.712\n",
            ">94, 71/234, d=0.689, g=0.693\n",
            ">94, 72/234, d=0.697, g=0.678\n",
            ">94, 73/234, d=0.691, g=0.679\n",
            ">94, 74/234, d=0.694, g=0.694\n",
            ">94, 75/234, d=0.690, g=0.718\n",
            ">94, 76/234, d=0.693, g=0.713\n",
            ">94, 77/234, d=0.695, g=0.690\n",
            ">94, 78/234, d=0.688, g=0.677\n",
            ">94, 79/234, d=0.684, g=0.680\n",
            ">94, 80/234, d=0.686, g=0.689\n",
            ">94, 81/234, d=0.693, g=0.721\n",
            ">94, 82/234, d=0.694, g=0.743\n",
            ">94, 83/234, d=0.690, g=0.730\n",
            ">94, 84/234, d=0.690, g=0.705\n",
            ">94, 85/234, d=0.697, g=0.678\n",
            ">94, 86/234, d=0.687, g=0.665\n",
            ">94, 87/234, d=0.702, g=0.672\n",
            ">94, 88/234, d=0.689, g=0.687\n",
            ">94, 89/234, d=0.697, g=0.697\n",
            ">94, 90/234, d=0.691, g=0.717\n",
            ">94, 91/234, d=0.687, g=0.711\n",
            ">94, 92/234, d=0.684, g=0.700\n",
            ">94, 93/234, d=0.690, g=0.684\n",
            ">94, 94/234, d=0.688, g=0.702\n",
            ">94, 95/234, d=0.695, g=0.725\n",
            ">94, 96/234, d=0.691, g=0.730\n",
            ">94, 97/234, d=0.695, g=0.710\n",
            ">94, 98/234, d=0.689, g=0.688\n",
            ">94, 99/234, d=0.695, g=0.668\n",
            ">94, 100/234, d=0.693, g=0.696\n",
            ">94, 101/234, d=0.689, g=0.699\n",
            ">94, 102/234, d=0.691, g=0.707\n",
            ">94, 103/234, d=0.696, g=0.683\n",
            ">94, 104/234, d=0.689, g=0.675\n",
            ">94, 105/234, d=0.697, g=0.679\n",
            ">94, 106/234, d=0.700, g=0.697\n",
            ">94, 107/234, d=0.688, g=0.712\n",
            ">94, 108/234, d=0.698, g=0.732\n",
            ">94, 109/234, d=0.693, g=0.723\n",
            ">94, 110/234, d=0.686, g=0.705\n",
            ">94, 111/234, d=0.697, g=0.696\n",
            ">94, 112/234, d=0.689, g=0.688\n",
            ">94, 113/234, d=0.697, g=0.690\n",
            ">94, 114/234, d=0.696, g=0.696\n",
            ">94, 115/234, d=0.691, g=0.708\n",
            ">94, 116/234, d=0.693, g=0.688\n",
            ">94, 117/234, d=0.701, g=0.698\n",
            ">94, 118/234, d=0.689, g=0.716\n",
            ">94, 119/234, d=0.690, g=0.722\n",
            ">94, 120/234, d=0.693, g=0.707\n",
            ">94, 121/234, d=0.694, g=0.699\n",
            ">94, 122/234, d=0.689, g=0.694\n",
            ">94, 123/234, d=0.694, g=0.708\n",
            ">94, 124/234, d=0.688, g=0.711\n",
            ">94, 125/234, d=0.695, g=0.696\n",
            ">94, 126/234, d=0.692, g=0.689\n",
            ">94, 127/234, d=0.693, g=0.681\n",
            ">94, 128/234, d=0.693, g=0.680\n",
            ">94, 129/234, d=0.690, g=0.673\n",
            ">94, 130/234, d=0.692, g=0.691\n",
            ">94, 131/234, d=0.692, g=0.710\n",
            ">94, 132/234, d=0.693, g=0.729\n",
            ">94, 133/234, d=0.696, g=0.731\n",
            ">94, 134/234, d=0.694, g=0.711\n",
            ">94, 135/234, d=0.692, g=0.710\n",
            ">94, 136/234, d=0.692, g=0.700\n",
            ">94, 137/234, d=0.690, g=0.705\n",
            ">94, 138/234, d=0.689, g=0.690\n",
            ">94, 139/234, d=0.694, g=0.698\n",
            ">94, 140/234, d=0.687, g=0.700\n",
            ">94, 141/234, d=0.691, g=0.706\n",
            ">94, 142/234, d=0.697, g=0.721\n",
            ">94, 143/234, d=0.697, g=0.707\n",
            ">94, 144/234, d=0.693, g=0.706\n",
            ">94, 145/234, d=0.694, g=0.700\n",
            ">94, 146/234, d=0.688, g=0.695\n",
            ">94, 147/234, d=0.699, g=0.699\n",
            ">94, 148/234, d=0.691, g=0.699\n",
            ">94, 149/234, d=0.689, g=0.693\n",
            ">94, 150/234, d=0.691, g=0.693\n",
            ">94, 151/234, d=0.686, g=0.701\n",
            ">94, 152/234, d=0.692, g=0.687\n",
            ">94, 153/234, d=0.691, g=0.686\n",
            ">94, 154/234, d=0.690, g=0.692\n",
            ">94, 155/234, d=0.692, g=0.698\n",
            ">94, 156/234, d=0.691, g=0.703\n",
            ">94, 157/234, d=0.688, g=0.688\n",
            ">94, 158/234, d=0.688, g=0.697\n",
            ">94, 159/234, d=0.692, g=0.699\n",
            ">94, 160/234, d=0.685, g=0.707\n",
            ">94, 161/234, d=0.699, g=0.703\n",
            ">94, 162/234, d=0.694, g=0.699\n",
            ">94, 163/234, d=0.687, g=0.699\n",
            ">94, 164/234, d=0.692, g=0.690\n",
            ">94, 165/234, d=0.699, g=0.690\n",
            ">94, 166/234, d=0.695, g=0.691\n",
            ">94, 167/234, d=0.692, g=0.691\n",
            ">94, 168/234, d=0.691, g=0.696\n",
            ">94, 169/234, d=0.692, g=0.713\n",
            ">94, 170/234, d=0.690, g=0.717\n",
            ">94, 171/234, d=0.692, g=0.712\n",
            ">94, 172/234, d=0.693, g=0.708\n",
            ">94, 173/234, d=0.689, g=0.690\n",
            ">94, 174/234, d=0.692, g=0.682\n",
            ">94, 175/234, d=0.686, g=0.683\n",
            ">94, 176/234, d=0.692, g=0.699\n",
            ">94, 177/234, d=0.695, g=0.708\n",
            ">94, 178/234, d=0.692, g=0.695\n",
            ">94, 179/234, d=0.695, g=0.701\n",
            ">94, 180/234, d=0.691, g=0.694\n",
            ">94, 181/234, d=0.686, g=0.699\n",
            ">94, 182/234, d=0.691, g=0.701\n",
            ">94, 183/234, d=0.694, g=0.716\n",
            ">94, 184/234, d=0.689, g=0.706\n",
            ">94, 185/234, d=0.700, g=0.697\n",
            ">94, 186/234, d=0.691, g=0.693\n",
            ">94, 187/234, d=0.692, g=0.693\n",
            ">94, 188/234, d=0.685, g=0.697\n",
            ">94, 189/234, d=0.683, g=0.709\n",
            ">94, 190/234, d=0.690, g=0.705\n",
            ">94, 191/234, d=0.696, g=0.705\n",
            ">94, 192/234, d=0.685, g=0.692\n",
            ">94, 193/234, d=0.684, g=0.680\n",
            ">94, 194/234, d=0.693, g=0.695\n",
            ">94, 195/234, d=0.690, g=0.700\n",
            ">94, 196/234, d=0.693, g=0.717\n",
            ">94, 197/234, d=0.691, g=0.704\n",
            ">94, 198/234, d=0.688, g=0.688\n",
            ">94, 199/234, d=0.688, g=0.696\n",
            ">94, 200/234, d=0.689, g=0.715\n",
            ">94, 201/234, d=0.698, g=0.724\n",
            ">94, 202/234, d=0.692, g=0.721\n",
            ">94, 203/234, d=0.693, g=0.692\n",
            ">94, 204/234, d=0.692, g=0.680\n",
            ">94, 205/234, d=0.695, g=0.681\n",
            ">94, 206/234, d=0.694, g=0.676\n",
            ">94, 207/234, d=0.695, g=0.708\n",
            ">94, 208/234, d=0.701, g=0.730\n",
            ">94, 209/234, d=0.692, g=0.729\n",
            ">94, 210/234, d=0.688, g=0.695\n",
            ">94, 211/234, d=0.690, g=0.664\n",
            ">94, 212/234, d=0.693, g=0.678\n",
            ">94, 213/234, d=0.681, g=0.709\n",
            ">94, 214/234, d=0.691, g=0.722\n",
            ">94, 215/234, d=0.691, g=0.723\n",
            ">94, 216/234, d=0.692, g=0.720\n",
            ">94, 217/234, d=0.688, g=0.703\n",
            ">94, 218/234, d=0.688, g=0.673\n",
            ">94, 219/234, d=0.696, g=0.674\n",
            ">94, 220/234, d=0.690, g=0.690\n",
            ">94, 221/234, d=0.692, g=0.710\n",
            ">94, 222/234, d=0.697, g=0.707\n",
            ">94, 223/234, d=0.694, g=0.697\n",
            ">94, 224/234, d=0.693, g=0.701\n",
            ">94, 225/234, d=0.690, g=0.686\n",
            ">94, 226/234, d=0.701, g=0.691\n",
            ">94, 227/234, d=0.695, g=0.695\n",
            ">94, 228/234, d=0.682, g=0.709\n",
            ">94, 229/234, d=0.689, g=0.715\n",
            ">94, 230/234, d=0.698, g=0.733\n",
            ">94, 231/234, d=0.690, g=0.716\n",
            ">94, 232/234, d=0.691, g=0.704\n",
            ">94, 233/234, d=0.690, g=0.695\n",
            ">94, 234/234, d=0.691, g=0.675\n",
            ">95, 1/234, d=0.687, g=0.678\n",
            ">95, 2/234, d=0.698, g=0.682\n",
            ">95, 3/234, d=0.689, g=0.686\n",
            ">95, 4/234, d=0.696, g=0.722\n",
            ">95, 5/234, d=0.691, g=0.716\n",
            ">95, 6/234, d=0.695, g=0.703\n",
            ">95, 7/234, d=0.691, g=0.686\n",
            ">95, 8/234, d=0.692, g=0.677\n",
            ">95, 9/234, d=0.693, g=0.694\n",
            ">95, 10/234, d=0.693, g=0.708\n",
            ">95, 11/234, d=0.694, g=0.719\n",
            ">95, 12/234, d=0.687, g=0.690\n",
            ">95, 13/234, d=0.689, g=0.688\n",
            ">95, 14/234, d=0.686, g=0.692\n",
            ">95, 15/234, d=0.688, g=0.705\n",
            ">95, 16/234, d=0.690, g=0.700\n",
            ">95, 17/234, d=0.693, g=0.693\n",
            ">95, 18/234, d=0.689, g=0.694\n",
            ">95, 19/234, d=0.693, g=0.687\n",
            ">95, 20/234, d=0.693, g=0.693\n",
            ">95, 21/234, d=0.691, g=0.701\n",
            ">95, 22/234, d=0.689, g=0.695\n",
            ">95, 23/234, d=0.688, g=0.700\n",
            ">95, 24/234, d=0.697, g=0.688\n",
            ">95, 25/234, d=0.689, g=0.701\n",
            ">95, 26/234, d=0.695, g=0.692\n",
            ">95, 27/234, d=0.693, g=0.698\n",
            ">95, 28/234, d=0.689, g=0.696\n",
            ">95, 29/234, d=0.691, g=0.710\n",
            ">95, 30/234, d=0.692, g=0.702\n",
            ">95, 31/234, d=0.696, g=0.704\n",
            ">95, 32/234, d=0.687, g=0.694\n",
            ">95, 33/234, d=0.687, g=0.700\n",
            ">95, 34/234, d=0.691, g=0.698\n",
            ">95, 35/234, d=0.687, g=0.690\n",
            ">95, 36/234, d=0.690, g=0.698\n",
            ">95, 37/234, d=0.692, g=0.710\n",
            ">95, 38/234, d=0.693, g=0.718\n",
            ">95, 39/234, d=0.694, g=0.710\n",
            ">95, 40/234, d=0.692, g=0.697\n",
            ">95, 41/234, d=0.700, g=0.694\n",
            ">95, 42/234, d=0.692, g=0.698\n",
            ">95, 43/234, d=0.694, g=0.692\n",
            ">95, 44/234, d=0.691, g=0.703\n",
            ">95, 45/234, d=0.688, g=0.705\n",
            ">95, 46/234, d=0.688, g=0.689\n",
            ">95, 47/234, d=0.692, g=0.685\n",
            ">95, 48/234, d=0.697, g=0.705\n",
            ">95, 49/234, d=0.697, g=0.720\n",
            ">95, 50/234, d=0.696, g=0.714\n",
            ">95, 51/234, d=0.692, g=0.695\n",
            ">95, 52/234, d=0.690, g=0.695\n",
            ">95, 53/234, d=0.696, g=0.704\n",
            ">95, 54/234, d=0.686, g=0.698\n",
            ">95, 55/234, d=0.692, g=0.701\n",
            ">95, 56/234, d=0.694, g=0.700\n",
            ">95, 57/234, d=0.688, g=0.687\n",
            ">95, 58/234, d=0.689, g=0.693\n",
            ">95, 59/234, d=0.693, g=0.674\n",
            ">95, 60/234, d=0.691, g=0.690\n",
            ">95, 61/234, d=0.684, g=0.689\n",
            ">95, 62/234, d=0.690, g=0.697\n",
            ">95, 63/234, d=0.690, g=0.709\n",
            ">95, 64/234, d=0.692, g=0.708\n",
            ">95, 65/234, d=0.689, g=0.713\n",
            ">95, 66/234, d=0.692, g=0.693\n",
            ">95, 67/234, d=0.694, g=0.688\n",
            ">95, 68/234, d=0.688, g=0.696\n",
            ">95, 69/234, d=0.696, g=0.715\n",
            ">95, 70/234, d=0.691, g=0.713\n",
            ">95, 71/234, d=0.689, g=0.703\n",
            ">95, 72/234, d=0.696, g=0.680\n",
            ">95, 73/234, d=0.688, g=0.690\n",
            ">95, 74/234, d=0.691, g=0.701\n",
            ">95, 75/234, d=0.693, g=0.713\n",
            ">95, 76/234, d=0.696, g=0.699\n",
            ">95, 77/234, d=0.694, g=0.684\n",
            ">95, 78/234, d=0.690, g=0.688\n",
            ">95, 79/234, d=0.690, g=0.701\n",
            ">95, 80/234, d=0.692, g=0.706\n",
            ">95, 81/234, d=0.686, g=0.704\n",
            ">95, 82/234, d=0.693, g=0.691\n",
            ">95, 83/234, d=0.695, g=0.678\n",
            ">95, 84/234, d=0.687, g=0.698\n",
            ">95, 85/234, d=0.693, g=0.718\n",
            ">95, 86/234, d=0.693, g=0.724\n",
            ">95, 87/234, d=0.690, g=0.716\n",
            ">95, 88/234, d=0.695, g=0.688\n",
            ">95, 89/234, d=0.692, g=0.681\n",
            ">95, 90/234, d=0.692, g=0.688\n",
            ">95, 91/234, d=0.683, g=0.697\n",
            ">95, 92/234, d=0.692, g=0.708\n",
            ">95, 93/234, d=0.694, g=0.704\n",
            ">95, 94/234, d=0.694, g=0.708\n",
            ">95, 95/234, d=0.691, g=0.721\n",
            ">95, 96/234, d=0.693, g=0.704\n",
            ">95, 97/234, d=0.694, g=0.701\n",
            ">95, 98/234, d=0.698, g=0.699\n",
            ">95, 99/234, d=0.679, g=0.691\n",
            ">95, 100/234, d=0.697, g=0.683\n",
            ">95, 101/234, d=0.690, g=0.695\n",
            ">95, 102/234, d=0.698, g=0.699\n",
            ">95, 103/234, d=0.690, g=0.701\n",
            ">95, 104/234, d=0.689, g=0.704\n",
            ">95, 105/234, d=0.693, g=0.700\n",
            ">95, 106/234, d=0.692, g=0.696\n",
            ">95, 107/234, d=0.688, g=0.697\n",
            ">95, 108/234, d=0.687, g=0.699\n",
            ">95, 109/234, d=0.691, g=0.698\n",
            ">95, 110/234, d=0.688, g=0.688\n",
            ">95, 111/234, d=0.693, g=0.676\n",
            ">95, 112/234, d=0.690, g=0.685\n",
            ">95, 113/234, d=0.699, g=0.696\n",
            ">95, 114/234, d=0.689, g=0.701\n",
            ">95, 115/234, d=0.688, g=0.716\n",
            ">95, 116/234, d=0.689, g=0.713\n",
            ">95, 117/234, d=0.688, g=0.711\n",
            ">95, 118/234, d=0.693, g=0.711\n",
            ">95, 119/234, d=0.687, g=0.703\n",
            ">95, 120/234, d=0.691, g=0.688\n",
            ">95, 121/234, d=0.690, g=0.690\n",
            ">95, 122/234, d=0.695, g=0.684\n",
            ">95, 123/234, d=0.690, g=0.679\n",
            ">95, 124/234, d=0.699, g=0.682\n",
            ">95, 125/234, d=0.683, g=0.692\n",
            ">95, 126/234, d=0.691, g=0.704\n",
            ">95, 127/234, d=0.691, g=0.715\n",
            ">95, 128/234, d=0.697, g=0.718\n",
            ">95, 129/234, d=0.697, g=0.703\n",
            ">95, 130/234, d=0.688, g=0.701\n",
            ">95, 131/234, d=0.690, g=0.687\n",
            ">95, 132/234, d=0.698, g=0.695\n",
            ">95, 133/234, d=0.695, g=0.700\n",
            ">95, 134/234, d=0.695, g=0.705\n",
            ">95, 135/234, d=0.692, g=0.703\n",
            ">95, 136/234, d=0.695, g=0.700\n",
            ">95, 137/234, d=0.694, g=0.694\n",
            ">95, 138/234, d=0.686, g=0.683\n",
            ">95, 139/234, d=0.693, g=0.687\n",
            ">95, 140/234, d=0.689, g=0.697\n",
            ">95, 141/234, d=0.693, g=0.710\n",
            ">95, 142/234, d=0.688, g=0.726\n",
            ">95, 143/234, d=0.693, g=0.711\n",
            ">95, 144/234, d=0.689, g=0.696\n",
            ">95, 145/234, d=0.684, g=0.686\n",
            ">95, 146/234, d=0.697, g=0.692\n",
            ">95, 147/234, d=0.698, g=0.691\n",
            ">95, 148/234, d=0.689, g=0.692\n",
            ">95, 149/234, d=0.694, g=0.717\n",
            ">95, 150/234, d=0.690, g=0.707\n",
            ">95, 151/234, d=0.688, g=0.699\n",
            ">95, 152/234, d=0.691, g=0.687\n",
            ">95, 153/234, d=0.696, g=0.693\n",
            ">95, 154/234, d=0.689, g=0.705\n",
            ">95, 155/234, d=0.687, g=0.703\n",
            ">95, 156/234, d=0.692, g=0.707\n",
            ">95, 157/234, d=0.686, g=0.689\n",
            ">95, 158/234, d=0.687, g=0.704\n",
            ">95, 159/234, d=0.699, g=0.705\n",
            ">95, 160/234, d=0.694, g=0.712\n",
            ">95, 161/234, d=0.688, g=0.701\n",
            ">95, 162/234, d=0.691, g=0.700\n",
            ">95, 163/234, d=0.688, g=0.696\n",
            ">95, 164/234, d=0.694, g=0.686\n",
            ">95, 165/234, d=0.697, g=0.695\n",
            ">95, 166/234, d=0.694, g=0.694\n",
            ">95, 167/234, d=0.693, g=0.711\n",
            ">95, 168/234, d=0.691, g=0.705\n",
            ">95, 169/234, d=0.691, g=0.688\n",
            ">95, 170/234, d=0.697, g=0.680\n",
            ">95, 171/234, d=0.691, g=0.681\n",
            ">95, 172/234, d=0.689, g=0.697\n",
            ">95, 173/234, d=0.697, g=0.716\n",
            ">95, 174/234, d=0.692, g=0.733\n",
            ">95, 175/234, d=0.688, g=0.719\n",
            ">95, 176/234, d=0.691, g=0.703\n",
            ">95, 177/234, d=0.696, g=0.679\n",
            ">95, 178/234, d=0.686, g=0.670\n",
            ">95, 179/234, d=0.690, g=0.681\n",
            ">95, 180/234, d=0.685, g=0.690\n",
            ">95, 181/234, d=0.693, g=0.716\n",
            ">95, 182/234, d=0.687, g=0.741\n",
            ">95, 183/234, d=0.691, g=0.726\n",
            ">95, 184/234, d=0.690, g=0.710\n",
            ">95, 185/234, d=0.690, g=0.686\n",
            ">95, 186/234, d=0.690, g=0.670\n",
            ">95, 187/234, d=0.692, g=0.676\n",
            ">95, 188/234, d=0.689, g=0.690\n",
            ">95, 189/234, d=0.689, g=0.697\n",
            ">95, 190/234, d=0.689, g=0.691\n",
            ">95, 191/234, d=0.694, g=0.708\n",
            ">95, 192/234, d=0.689, g=0.716\n",
            ">95, 193/234, d=0.692, g=0.704\n",
            ">95, 194/234, d=0.685, g=0.694\n",
            ">95, 195/234, d=0.685, g=0.695\n",
            ">95, 196/234, d=0.687, g=0.692\n",
            ">95, 197/234, d=0.696, g=0.699\n",
            ">95, 198/234, d=0.695, g=0.700\n",
            ">95, 199/234, d=0.688, g=0.711\n",
            ">95, 200/234, d=0.688, g=0.698\n",
            ">95, 201/234, d=0.684, g=0.704\n",
            ">95, 202/234, d=0.693, g=0.700\n",
            ">95, 203/234, d=0.689, g=0.695\n",
            ">95, 204/234, d=0.696, g=0.693\n",
            ">95, 205/234, d=0.686, g=0.693\n",
            ">95, 206/234, d=0.688, g=0.695\n",
            ">95, 207/234, d=0.689, g=0.704\n",
            ">95, 208/234, d=0.689, g=0.718\n",
            ">95, 209/234, d=0.688, g=0.720\n",
            ">95, 210/234, d=0.694, g=0.693\n",
            ">95, 211/234, d=0.695, g=0.685\n",
            ">95, 212/234, d=0.691, g=0.671\n",
            ">95, 213/234, d=0.689, g=0.689\n",
            ">95, 214/234, d=0.698, g=0.716\n",
            ">95, 215/234, d=0.698, g=0.722\n",
            ">95, 216/234, d=0.688, g=0.700\n",
            ">95, 217/234, d=0.696, g=0.677\n",
            ">95, 218/234, d=0.690, g=0.692\n",
            ">95, 219/234, d=0.692, g=0.707\n",
            ">95, 220/234, d=0.684, g=0.718\n",
            ">95, 221/234, d=0.696, g=0.729\n",
            ">95, 222/234, d=0.693, g=0.711\n",
            ">95, 223/234, d=0.692, g=0.689\n",
            ">95, 224/234, d=0.692, g=0.678\n",
            ">95, 225/234, d=0.684, g=0.672\n",
            ">95, 226/234, d=0.694, g=0.686\n",
            ">95, 227/234, d=0.692, g=0.699\n",
            ">95, 228/234, d=0.689, g=0.718\n",
            ">95, 229/234, d=0.692, g=0.721\n",
            ">95, 230/234, d=0.697, g=0.689\n",
            ">95, 231/234, d=0.690, g=0.682\n",
            ">95, 232/234, d=0.693, g=0.687\n",
            ">95, 233/234, d=0.693, g=0.703\n",
            ">95, 234/234, d=0.695, g=0.707\n",
            ">96, 1/234, d=0.692, g=0.701\n",
            ">96, 2/234, d=0.692, g=0.694\n",
            ">96, 3/234, d=0.694, g=0.704\n",
            ">96, 4/234, d=0.691, g=0.721\n",
            ">96, 5/234, d=0.694, g=0.725\n",
            ">96, 6/234, d=0.689, g=0.711\n",
            ">96, 7/234, d=0.693, g=0.700\n",
            ">96, 8/234, d=0.694, g=0.687\n",
            ">96, 9/234, d=0.689, g=0.696\n",
            ">96, 10/234, d=0.688, g=0.693\n",
            ">96, 11/234, d=0.686, g=0.685\n",
            ">96, 12/234, d=0.696, g=0.696\n",
            ">96, 13/234, d=0.686, g=0.691\n",
            ">96, 14/234, d=0.687, g=0.686\n",
            ">96, 15/234, d=0.690, g=0.688\n",
            ">96, 16/234, d=0.691, g=0.705\n",
            ">96, 17/234, d=0.699, g=0.717\n",
            ">96, 18/234, d=0.696, g=0.726\n",
            ">96, 19/234, d=0.691, g=0.710\n",
            ">96, 20/234, d=0.700, g=0.698\n",
            ">96, 21/234, d=0.695, g=0.687\n",
            ">96, 22/234, d=0.696, g=0.693\n",
            ">96, 23/234, d=0.688, g=0.685\n",
            ">96, 24/234, d=0.693, g=0.685\n",
            ">96, 25/234, d=0.692, g=0.688\n",
            ">96, 26/234, d=0.699, g=0.704\n",
            ">96, 27/234, d=0.701, g=0.695\n",
            ">96, 28/234, d=0.694, g=0.696\n",
            ">96, 29/234, d=0.689, g=0.691\n",
            ">96, 30/234, d=0.700, g=0.697\n",
            ">96, 31/234, d=0.691, g=0.696\n",
            ">96, 32/234, d=0.696, g=0.709\n",
            ">96, 33/234, d=0.692, g=0.713\n",
            ">96, 34/234, d=0.693, g=0.711\n",
            ">96, 35/234, d=0.691, g=0.691\n",
            ">96, 36/234, d=0.690, g=0.680\n",
            ">96, 37/234, d=0.690, g=0.677\n",
            ">96, 38/234, d=0.694, g=0.693\n",
            ">96, 39/234, d=0.692, g=0.707\n",
            ">96, 40/234, d=0.691, g=0.700\n",
            ">96, 41/234, d=0.688, g=0.688\n",
            ">96, 42/234, d=0.690, g=0.700\n",
            ">96, 43/234, d=0.692, g=0.700\n",
            ">96, 44/234, d=0.697, g=0.697\n",
            ">96, 45/234, d=0.694, g=0.691\n",
            ">96, 46/234, d=0.692, g=0.680\n",
            ">96, 47/234, d=0.689, g=0.686\n",
            ">96, 48/234, d=0.694, g=0.694\n",
            ">96, 49/234, d=0.692, g=0.728\n",
            ">96, 50/234, d=0.692, g=0.743\n",
            ">96, 51/234, d=0.691, g=0.717\n",
            ">96, 52/234, d=0.692, g=0.679\n",
            ">96, 53/234, d=0.692, g=0.677\n",
            ">96, 54/234, d=0.696, g=0.689\n",
            ">96, 55/234, d=0.692, g=0.692\n",
            ">96, 56/234, d=0.690, g=0.721\n",
            ">96, 57/234, d=0.688, g=0.726\n",
            ">96, 58/234, d=0.689, g=0.710\n",
            ">96, 59/234, d=0.684, g=0.678\n",
            ">96, 60/234, d=0.693, g=0.683\n",
            ">96, 61/234, d=0.689, g=0.680\n",
            ">96, 62/234, d=0.692, g=0.698\n",
            ">96, 63/234, d=0.693, g=0.693\n",
            ">96, 64/234, d=0.687, g=0.701\n",
            ">96, 65/234, d=0.697, g=0.704\n",
            ">96, 66/234, d=0.695, g=0.704\n",
            ">96, 67/234, d=0.692, g=0.709\n",
            ">96, 68/234, d=0.691, g=0.703\n",
            ">96, 69/234, d=0.691, g=0.697\n",
            ">96, 70/234, d=0.688, g=0.694\n",
            ">96, 71/234, d=0.693, g=0.692\n",
            ">96, 72/234, d=0.691, g=0.686\n",
            ">96, 73/234, d=0.693, g=0.699\n",
            ">96, 74/234, d=0.691, g=0.707\n",
            ">96, 75/234, d=0.694, g=0.705\n",
            ">96, 76/234, d=0.691, g=0.699\n",
            ">96, 77/234, d=0.697, g=0.689\n",
            ">96, 78/234, d=0.693, g=0.686\n",
            ">96, 79/234, d=0.693, g=0.704\n",
            ">96, 80/234, d=0.691, g=0.717\n",
            ">96, 81/234, d=0.697, g=0.708\n",
            ">96, 82/234, d=0.688, g=0.697\n",
            ">96, 83/234, d=0.692, g=0.693\n",
            ">96, 84/234, d=0.691, g=0.695\n",
            ">96, 85/234, d=0.694, g=0.691\n",
            ">96, 86/234, d=0.689, g=0.690\n",
            ">96, 87/234, d=0.694, g=0.698\n",
            ">96, 88/234, d=0.691, g=0.704\n",
            ">96, 89/234, d=0.691, g=0.711\n",
            ">96, 90/234, d=0.690, g=0.712\n",
            ">96, 91/234, d=0.693, g=0.717\n",
            ">96, 92/234, d=0.693, g=0.701\n",
            ">96, 93/234, d=0.689, g=0.701\n",
            ">96, 94/234, d=0.687, g=0.694\n",
            ">96, 95/234, d=0.693, g=0.694\n",
            ">96, 96/234, d=0.692, g=0.683\n",
            ">96, 97/234, d=0.689, g=0.682\n",
            ">96, 98/234, d=0.685, g=0.685\n",
            ">96, 99/234, d=0.694, g=0.712\n",
            ">96, 100/234, d=0.692, g=0.720\n",
            ">96, 101/234, d=0.687, g=0.719\n",
            ">96, 102/234, d=0.694, g=0.705\n",
            ">96, 103/234, d=0.694, g=0.694\n",
            ">96, 104/234, d=0.679, g=0.682\n",
            ">96, 105/234, d=0.689, g=0.692\n",
            ">96, 106/234, d=0.692, g=0.697\n",
            ">96, 107/234, d=0.687, g=0.710\n",
            ">96, 108/234, d=0.688, g=0.704\n",
            ">96, 109/234, d=0.693, g=0.704\n",
            ">96, 110/234, d=0.697, g=0.706\n",
            ">96, 111/234, d=0.688, g=0.700\n",
            ">96, 112/234, d=0.692, g=0.702\n",
            ">96, 113/234, d=0.687, g=0.701\n",
            ">96, 114/234, d=0.693, g=0.700\n",
            ">96, 115/234, d=0.692, g=0.690\n",
            ">96, 116/234, d=0.691, g=0.701\n",
            ">96, 117/234, d=0.696, g=0.698\n",
            ">96, 118/234, d=0.697, g=0.703\n",
            ">96, 119/234, d=0.696, g=0.699\n",
            ">96, 120/234, d=0.693, g=0.697\n",
            ">96, 121/234, d=0.693, g=0.690\n",
            ">96, 122/234, d=0.687, g=0.685\n",
            ">96, 123/234, d=0.693, g=0.692\n",
            ">96, 124/234, d=0.691, g=0.700\n",
            ">96, 125/234, d=0.686, g=0.705\n",
            ">96, 126/234, d=0.696, g=0.703\n",
            ">96, 127/234, d=0.692, g=0.697\n",
            ">96, 128/234, d=0.690, g=0.697\n",
            ">96, 129/234, d=0.691, g=0.693\n",
            ">96, 130/234, d=0.694, g=0.690\n",
            ">96, 131/234, d=0.695, g=0.695\n",
            ">96, 132/234, d=0.695, g=0.692\n",
            ">96, 133/234, d=0.694, g=0.705\n",
            ">96, 134/234, d=0.692, g=0.714\n",
            ">96, 135/234, d=0.690, g=0.702\n",
            ">96, 136/234, d=0.694, g=0.701\n",
            ">96, 137/234, d=0.695, g=0.681\n",
            ">96, 138/234, d=0.692, g=0.689\n",
            ">96, 139/234, d=0.692, g=0.705\n",
            ">96, 140/234, d=0.693, g=0.710\n",
            ">96, 141/234, d=0.688, g=0.704\n",
            ">96, 142/234, d=0.694, g=0.701\n",
            ">96, 143/234, d=0.696, g=0.692\n",
            ">96, 144/234, d=0.691, g=0.684\n",
            ">96, 145/234, d=0.689, g=0.691\n",
            ">96, 146/234, d=0.693, g=0.711\n",
            ">96, 147/234, d=0.691, g=0.715\n",
            ">96, 148/234, d=0.690, g=0.700\n",
            ">96, 149/234, d=0.689, g=0.693\n",
            ">96, 150/234, d=0.689, g=0.684\n",
            ">96, 151/234, d=0.692, g=0.710\n",
            ">96, 152/234, d=0.691, g=0.725\n",
            ">96, 153/234, d=0.692, g=0.707\n",
            ">96, 154/234, d=0.696, g=0.688\n",
            ">96, 155/234, d=0.687, g=0.666\n",
            ">96, 156/234, d=0.688, g=0.682\n",
            ">96, 157/234, d=0.688, g=0.703\n",
            ">96, 158/234, d=0.691, g=0.716\n",
            ">96, 159/234, d=0.698, g=0.711\n",
            ">96, 160/234, d=0.692, g=0.703\n",
            ">96, 161/234, d=0.692, g=0.690\n",
            ">96, 162/234, d=0.689, g=0.675\n",
            ">96, 163/234, d=0.685, g=0.682\n",
            ">96, 164/234, d=0.693, g=0.706\n",
            ">96, 165/234, d=0.693, g=0.704\n",
            ">96, 166/234, d=0.697, g=0.698\n",
            ">96, 167/234, d=0.687, g=0.703\n",
            ">96, 168/234, d=0.684, g=0.709\n",
            ">96, 169/234, d=0.691, g=0.694\n",
            ">96, 170/234, d=0.689, g=0.689\n",
            ">96, 171/234, d=0.700, g=0.692\n",
            ">96, 172/234, d=0.691, g=0.692\n",
            ">96, 173/234, d=0.692, g=0.683\n",
            ">96, 174/234, d=0.690, g=0.669\n",
            ">96, 175/234, d=0.686, g=0.700\n",
            ">96, 176/234, d=0.691, g=0.711\n",
            ">96, 177/234, d=0.692, g=0.714\n",
            ">96, 178/234, d=0.693, g=0.708\n",
            ">96, 179/234, d=0.693, g=0.694\n",
            ">96, 180/234, d=0.696, g=0.700\n",
            ">96, 181/234, d=0.692, g=0.709\n",
            ">96, 182/234, d=0.693, g=0.719\n",
            ">96, 183/234, d=0.693, g=0.708\n",
            ">96, 184/234, d=0.687, g=0.685\n",
            ">96, 185/234, d=0.689, g=0.685\n",
            ">96, 186/234, d=0.696, g=0.695\n",
            ">96, 187/234, d=0.693, g=0.691\n",
            ">96, 188/234, d=0.692, g=0.699\n",
            ">96, 189/234, d=0.694, g=0.704\n",
            ">96, 190/234, d=0.688, g=0.708\n",
            ">96, 191/234, d=0.691, g=0.705\n",
            ">96, 192/234, d=0.689, g=0.686\n",
            ">96, 193/234, d=0.691, g=0.688\n",
            ">96, 194/234, d=0.689, g=0.681\n",
            ">96, 195/234, d=0.696, g=0.691\n",
            ">96, 196/234, d=0.689, g=0.705\n",
            ">96, 197/234, d=0.690, g=0.722\n",
            ">96, 198/234, d=0.694, g=0.730\n",
            ">96, 199/234, d=0.689, g=0.705\n",
            ">96, 200/234, d=0.690, g=0.691\n",
            ">96, 201/234, d=0.692, g=0.674\n",
            ">96, 202/234, d=0.694, g=0.672\n",
            ">96, 203/234, d=0.687, g=0.683\n",
            ">96, 204/234, d=0.695, g=0.697\n",
            ">96, 205/234, d=0.698, g=0.706\n",
            ">96, 206/234, d=0.693, g=0.706\n",
            ">96, 207/234, d=0.689, g=0.705\n",
            ">96, 208/234, d=0.695, g=0.702\n",
            ">96, 209/234, d=0.694, g=0.712\n",
            ">96, 210/234, d=0.688, g=0.702\n",
            ">96, 211/234, d=0.691, g=0.685\n",
            ">96, 212/234, d=0.698, g=0.678\n",
            ">96, 213/234, d=0.686, g=0.677\n",
            ">96, 214/234, d=0.697, g=0.695\n",
            ">96, 215/234, d=0.693, g=0.720\n",
            ">96, 216/234, d=0.693, g=0.721\n",
            ">96, 217/234, d=0.683, g=0.719\n",
            ">96, 218/234, d=0.691, g=0.713\n",
            ">96, 219/234, d=0.689, g=0.705\n",
            ">96, 220/234, d=0.693, g=0.693\n",
            ">96, 221/234, d=0.691, g=0.686\n",
            ">96, 222/234, d=0.686, g=0.676\n",
            ">96, 223/234, d=0.696, g=0.689\n",
            ">96, 224/234, d=0.689, g=0.706\n",
            ">96, 225/234, d=0.689, g=0.712\n",
            ">96, 226/234, d=0.697, g=0.699\n",
            ">96, 227/234, d=0.688, g=0.699\n",
            ">96, 228/234, d=0.689, g=0.719\n",
            ">96, 229/234, d=0.697, g=0.707\n",
            ">96, 230/234, d=0.692, g=0.702\n",
            ">96, 231/234, d=0.690, g=0.714\n",
            ">96, 232/234, d=0.688, g=0.698\n",
            ">96, 233/234, d=0.686, g=0.696\n",
            ">96, 234/234, d=0.690, g=0.683\n",
            ">97, 1/234, d=0.690, g=0.687\n",
            ">97, 2/234, d=0.690, g=0.688\n",
            ">97, 3/234, d=0.699, g=0.699\n",
            ">97, 4/234, d=0.691, g=0.713\n",
            ">97, 5/234, d=0.685, g=0.696\n",
            ">97, 6/234, d=0.688, g=0.695\n",
            ">97, 7/234, d=0.689, g=0.701\n",
            ">97, 8/234, d=0.691, g=0.698\n",
            ">97, 9/234, d=0.691, g=0.693\n",
            ">97, 10/234, d=0.690, g=0.689\n",
            ">97, 11/234, d=0.690, g=0.693\n",
            ">97, 12/234, d=0.692, g=0.695\n",
            ">97, 13/234, d=0.689, g=0.704\n",
            ">97, 14/234, d=0.694, g=0.715\n",
            ">97, 15/234, d=0.691, g=0.714\n",
            ">97, 16/234, d=0.690, g=0.692\n",
            ">97, 17/234, d=0.693, g=0.694\n",
            ">97, 18/234, d=0.689, g=0.698\n",
            ">97, 19/234, d=0.693, g=0.711\n",
            ">97, 20/234, d=0.693, g=0.713\n",
            ">97, 21/234, d=0.692, g=0.701\n",
            ">97, 22/234, d=0.697, g=0.683\n",
            ">97, 23/234, d=0.690, g=0.681\n",
            ">97, 24/234, d=0.687, g=0.695\n",
            ">97, 25/234, d=0.694, g=0.703\n",
            ">97, 26/234, d=0.691, g=0.705\n",
            ">97, 27/234, d=0.690, g=0.690\n",
            ">97, 28/234, d=0.691, g=0.711\n",
            ">97, 29/234, d=0.696, g=0.714\n",
            ">97, 30/234, d=0.692, g=0.702\n",
            ">97, 31/234, d=0.694, g=0.689\n",
            ">97, 32/234, d=0.698, g=0.686\n",
            ">97, 33/234, d=0.691, g=0.684\n",
            ">97, 34/234, d=0.688, g=0.691\n",
            ">97, 35/234, d=0.695, g=0.702\n",
            ">97, 36/234, d=0.685, g=0.702\n",
            ">97, 37/234, d=0.689, g=0.697\n",
            ">97, 38/234, d=0.699, g=0.700\n",
            ">97, 39/234, d=0.688, g=0.695\n",
            ">97, 40/234, d=0.695, g=0.693\n",
            ">97, 41/234, d=0.697, g=0.694\n",
            ">97, 42/234, d=0.688, g=0.687\n",
            ">97, 43/234, d=0.686, g=0.697\n",
            ">97, 44/234, d=0.688, g=0.701\n",
            ">97, 45/234, d=0.693, g=0.704\n",
            ">97, 46/234, d=0.696, g=0.706\n",
            ">97, 47/234, d=0.687, g=0.690\n",
            ">97, 48/234, d=0.685, g=0.692\n",
            ">97, 49/234, d=0.692, g=0.701\n",
            ">97, 50/234, d=0.689, g=0.693\n",
            ">97, 51/234, d=0.687, g=0.695\n",
            ">97, 52/234, d=0.694, g=0.695\n",
            ">97, 53/234, d=0.693, g=0.693\n",
            ">97, 54/234, d=0.690, g=0.698\n",
            ">97, 55/234, d=0.692, g=0.689\n",
            ">97, 56/234, d=0.695, g=0.704\n",
            ">97, 57/234, d=0.688, g=0.703\n",
            ">97, 58/234, d=0.690, g=0.700\n",
            ">97, 59/234, d=0.694, g=0.697\n",
            ">97, 60/234, d=0.697, g=0.707\n",
            ">97, 61/234, d=0.695, g=0.698\n",
            ">97, 62/234, d=0.700, g=0.704\n",
            ">97, 63/234, d=0.695, g=0.702\n",
            ">97, 64/234, d=0.695, g=0.697\n",
            ">97, 65/234, d=0.695, g=0.689\n",
            ">97, 66/234, d=0.688, g=0.687\n",
            ">97, 67/234, d=0.694, g=0.710\n",
            ">97, 68/234, d=0.686, g=0.715\n",
            ">97, 69/234, d=0.691, g=0.696\n",
            ">97, 70/234, d=0.692, g=0.696\n",
            ">97, 71/234, d=0.698, g=0.707\n",
            ">97, 72/234, d=0.691, g=0.716\n",
            ">97, 73/234, d=0.692, g=0.688\n",
            ">97, 74/234, d=0.692, g=0.683\n",
            ">97, 75/234, d=0.687, g=0.683\n",
            ">97, 76/234, d=0.694, g=0.689\n",
            ">97, 77/234, d=0.692, g=0.708\n",
            ">97, 78/234, d=0.691, g=0.720\n",
            ">97, 79/234, d=0.690, g=0.709\n",
            ">97, 80/234, d=0.693, g=0.691\n",
            ">97, 81/234, d=0.694, g=0.691\n",
            ">97, 82/234, d=0.697, g=0.701\n",
            ">97, 83/234, d=0.687, g=0.707\n",
            ">97, 84/234, d=0.693, g=0.709\n",
            ">97, 85/234, d=0.688, g=0.694\n",
            ">97, 86/234, d=0.695, g=0.698\n",
            ">97, 87/234, d=0.694, g=0.695\n",
            ">97, 88/234, d=0.690, g=0.694\n",
            ">97, 89/234, d=0.693, g=0.695\n",
            ">97, 90/234, d=0.690, g=0.702\n",
            ">97, 91/234, d=0.690, g=0.703\n",
            ">97, 92/234, d=0.690, g=0.708\n",
            ">97, 93/234, d=0.692, g=0.723\n",
            ">97, 94/234, d=0.692, g=0.704\n",
            ">97, 95/234, d=0.689, g=0.700\n",
            ">97, 96/234, d=0.691, g=0.691\n",
            ">97, 97/234, d=0.686, g=0.680\n",
            ">97, 98/234, d=0.692, g=0.676\n",
            ">97, 99/234, d=0.693, g=0.691\n",
            ">97, 100/234, d=0.690, g=0.720\n",
            ">97, 101/234, d=0.695, g=0.707\n",
            ">97, 102/234, d=0.692, g=0.697\n",
            ">97, 103/234, d=0.692, g=0.695\n",
            ">97, 104/234, d=0.692, g=0.709\n",
            ">97, 105/234, d=0.686, g=0.715\n",
            ">97, 106/234, d=0.692, g=0.708\n",
            ">97, 107/234, d=0.690, g=0.684\n",
            ">97, 108/234, d=0.688, g=0.668\n",
            ">97, 109/234, d=0.698, g=0.690\n",
            ">97, 110/234, d=0.688, g=0.709\n",
            ">97, 111/234, d=0.696, g=0.719\n",
            ">97, 112/234, d=0.701, g=0.713\n",
            ">97, 113/234, d=0.693, g=0.701\n",
            ">97, 114/234, d=0.700, g=0.709\n",
            ">97, 115/234, d=0.691, g=0.715\n",
            ">97, 116/234, d=0.689, g=0.715\n",
            ">97, 117/234, d=0.692, g=0.708\n",
            ">97, 118/234, d=0.689, g=0.701\n",
            ">97, 119/234, d=0.694, g=0.697\n",
            ">97, 120/234, d=0.695, g=0.694\n",
            ">97, 121/234, d=0.691, g=0.696\n",
            ">97, 122/234, d=0.690, g=0.696\n",
            ">97, 123/234, d=0.695, g=0.685\n",
            ">97, 124/234, d=0.688, g=0.684\n",
            ">97, 125/234, d=0.697, g=0.709\n",
            ">97, 126/234, d=0.692, g=0.702\n",
            ">97, 127/234, d=0.694, g=0.691\n",
            ">97, 128/234, d=0.694, g=0.689\n",
            ">97, 129/234, d=0.691, g=0.708\n",
            ">97, 130/234, d=0.692, g=0.718\n",
            ">97, 131/234, d=0.693, g=0.694\n",
            ">97, 132/234, d=0.693, g=0.692\n",
            ">97, 133/234, d=0.694, g=0.691\n",
            ">97, 134/234, d=0.690, g=0.699\n",
            ">97, 135/234, d=0.693, g=0.694\n",
            ">97, 136/234, d=0.693, g=0.688\n",
            ">97, 137/234, d=0.695, g=0.697\n",
            ">97, 138/234, d=0.695, g=0.705\n",
            ">97, 139/234, d=0.695, g=0.717\n",
            ">97, 140/234, d=0.686, g=0.699\n",
            ">97, 141/234, d=0.685, g=0.689\n",
            ">97, 142/234, d=0.691, g=0.680\n",
            ">97, 143/234, d=0.688, g=0.694\n",
            ">97, 144/234, d=0.692, g=0.704\n",
            ">97, 145/234, d=0.688, g=0.702\n",
            ">97, 146/234, d=0.690, g=0.704\n",
            ">97, 147/234, d=0.696, g=0.712\n",
            ">97, 148/234, d=0.696, g=0.709\n",
            ">97, 149/234, d=0.691, g=0.703\n",
            ">97, 150/234, d=0.692, g=0.696\n",
            ">97, 151/234, d=0.695, g=0.683\n",
            ">97, 152/234, d=0.696, g=0.675\n",
            ">97, 153/234, d=0.692, g=0.697\n",
            ">97, 154/234, d=0.691, g=0.699\n",
            ">97, 155/234, d=0.689, g=0.698\n",
            ">97, 156/234, d=0.690, g=0.698\n",
            ">97, 157/234, d=0.684, g=0.702\n",
            ">97, 158/234, d=0.694, g=0.715\n",
            ">97, 159/234, d=0.694, g=0.712\n",
            ">97, 160/234, d=0.695, g=0.703\n",
            ">97, 161/234, d=0.692, g=0.698\n",
            ">97, 162/234, d=0.700, g=0.693\n",
            ">97, 163/234, d=0.699, g=0.691\n",
            ">97, 164/234, d=0.693, g=0.696\n",
            ">97, 165/234, d=0.693, g=0.700\n",
            ">97, 166/234, d=0.692, g=0.688\n",
            ">97, 167/234, d=0.695, g=0.699\n",
            ">97, 168/234, d=0.693, g=0.718\n",
            ">97, 169/234, d=0.691, g=0.698\n",
            ">97, 170/234, d=0.687, g=0.689\n",
            ">97, 171/234, d=0.693, g=0.689\n",
            ">97, 172/234, d=0.687, g=0.687\n",
            ">97, 173/234, d=0.693, g=0.701\n",
            ">97, 174/234, d=0.696, g=0.710\n",
            ">97, 175/234, d=0.693, g=0.692\n",
            ">97, 176/234, d=0.690, g=0.677\n",
            ">97, 177/234, d=0.679, g=0.680\n",
            ">97, 178/234, d=0.694, g=0.684\n",
            ">97, 179/234, d=0.688, g=0.706\n",
            ">97, 180/234, d=0.700, g=0.732\n",
            ">97, 181/234, d=0.684, g=0.733\n",
            ">97, 182/234, d=0.697, g=0.718\n",
            ">97, 183/234, d=0.687, g=0.695\n",
            ">97, 184/234, d=0.690, g=0.677\n",
            ">97, 185/234, d=0.692, g=0.669\n",
            ">97, 186/234, d=0.690, g=0.673\n",
            ">97, 187/234, d=0.694, g=0.701\n",
            ">97, 188/234, d=0.691, g=0.730\n",
            ">97, 189/234, d=0.688, g=0.722\n",
            ">97, 190/234, d=0.692, g=0.704\n",
            ">97, 191/234, d=0.692, g=0.684\n",
            ">97, 192/234, d=0.692, g=0.690\n",
            ">97, 193/234, d=0.693, g=0.698\n",
            ">97, 194/234, d=0.691, g=0.712\n",
            ">97, 195/234, d=0.692, g=0.704\n",
            ">97, 196/234, d=0.691, g=0.690\n",
            ">97, 197/234, d=0.690, g=0.688\n",
            ">97, 198/234, d=0.690, g=0.703\n",
            ">97, 199/234, d=0.689, g=0.714\n",
            ">97, 200/234, d=0.688, g=0.716\n",
            ">97, 201/234, d=0.688, g=0.700\n",
            ">97, 202/234, d=0.686, g=0.689\n",
            ">97, 203/234, d=0.686, g=0.681\n",
            ">97, 204/234, d=0.695, g=0.689\n",
            ">97, 205/234, d=0.695, g=0.704\n",
            ">97, 206/234, d=0.696, g=0.701\n",
            ">97, 207/234, d=0.695, g=0.703\n",
            ">97, 208/234, d=0.682, g=0.695\n",
            ">97, 209/234, d=0.682, g=0.716\n",
            ">97, 210/234, d=0.686, g=0.712\n",
            ">97, 211/234, d=0.684, g=0.699\n",
            ">97, 212/234, d=0.690, g=0.683\n",
            ">97, 213/234, d=0.693, g=0.693\n",
            ">97, 214/234, d=0.679, g=0.711\n",
            ">97, 215/234, d=0.694, g=0.709\n",
            ">97, 216/234, d=0.690, g=0.722\n",
            ">97, 217/234, d=0.692, g=0.697\n",
            ">97, 218/234, d=0.694, g=0.691\n",
            ">97, 219/234, d=0.695, g=0.683\n",
            ">97, 220/234, d=0.687, g=0.688\n",
            ">97, 221/234, d=0.684, g=0.699\n",
            ">97, 222/234, d=0.690, g=0.703\n",
            ">97, 223/234, d=0.695, g=0.708\n",
            ">97, 224/234, d=0.688, g=0.703\n",
            ">97, 225/234, d=0.690, g=0.692\n",
            ">97, 226/234, d=0.690, g=0.689\n",
            ">97, 227/234, d=0.689, g=0.688\n",
            ">97, 228/234, d=0.688, g=0.693\n",
            ">97, 229/234, d=0.693, g=0.703\n",
            ">97, 230/234, d=0.693, g=0.714\n",
            ">97, 231/234, d=0.686, g=0.720\n",
            ">97, 232/234, d=0.686, g=0.710\n",
            ">97, 233/234, d=0.691, g=0.694\n",
            ">97, 234/234, d=0.694, g=0.682\n",
            ">98, 1/234, d=0.685, g=0.684\n",
            ">98, 2/234, d=0.691, g=0.696\n",
            ">98, 3/234, d=0.692, g=0.720\n",
            ">98, 4/234, d=0.694, g=0.719\n",
            ">98, 5/234, d=0.690, g=0.701\n",
            ">98, 6/234, d=0.692, g=0.687\n",
            ">98, 7/234, d=0.692, g=0.682\n",
            ">98, 8/234, d=0.701, g=0.693\n",
            ">98, 9/234, d=0.690, g=0.681\n",
            ">98, 10/234, d=0.686, g=0.706\n",
            ">98, 11/234, d=0.690, g=0.695\n",
            ">98, 12/234, d=0.692, g=0.701\n",
            ">98, 13/234, d=0.691, g=0.707\n",
            ">98, 14/234, d=0.687, g=0.700\n",
            ">98, 15/234, d=0.689, g=0.697\n",
            ">98, 16/234, d=0.683, g=0.695\n",
            ">98, 17/234, d=0.689, g=0.701\n",
            ">98, 18/234, d=0.686, g=0.685\n",
            ">98, 19/234, d=0.697, g=0.705\n",
            ">98, 20/234, d=0.694, g=0.712\n",
            ">98, 21/234, d=0.691, g=0.705\n",
            ">98, 22/234, d=0.690, g=0.704\n",
            ">98, 23/234, d=0.690, g=0.688\n",
            ">98, 24/234, d=0.686, g=0.682\n",
            ">98, 25/234, d=0.688, g=0.699\n",
            ">98, 26/234, d=0.688, g=0.706\n",
            ">98, 27/234, d=0.685, g=0.694\n",
            ">98, 28/234, d=0.690, g=0.685\n",
            ">98, 29/234, d=0.692, g=0.698\n",
            ">98, 30/234, d=0.695, g=0.722\n",
            ">98, 31/234, d=0.686, g=0.716\n",
            ">98, 32/234, d=0.686, g=0.706\n",
            ">98, 33/234, d=0.685, g=0.690\n",
            ">98, 34/234, d=0.698, g=0.702\n",
            ">98, 35/234, d=0.695, g=0.699\n",
            ">98, 36/234, d=0.688, g=0.702\n",
            ">98, 37/234, d=0.689, g=0.703\n",
            ">98, 38/234, d=0.689, g=0.691\n",
            ">98, 39/234, d=0.691, g=0.694\n",
            ">98, 40/234, d=0.690, g=0.687\n",
            ">98, 41/234, d=0.688, g=0.698\n",
            ">98, 42/234, d=0.694, g=0.698\n",
            ">98, 43/234, d=0.691, g=0.713\n",
            ">98, 44/234, d=0.687, g=0.729\n",
            ">98, 45/234, d=0.695, g=0.728\n",
            ">98, 46/234, d=0.690, g=0.696\n",
            ">98, 47/234, d=0.690, g=0.670\n",
            ">98, 48/234, d=0.694, g=0.665\n",
            ">98, 49/234, d=0.694, g=0.686\n",
            ">98, 50/234, d=0.696, g=0.728\n",
            ">98, 51/234, d=0.692, g=0.746\n",
            ">98, 52/234, d=0.693, g=0.730\n",
            ">98, 53/234, d=0.691, g=0.717\n",
            ">98, 54/234, d=0.694, g=0.678\n",
            ">98, 55/234, d=0.690, g=0.671\n",
            ">98, 56/234, d=0.690, g=0.686\n",
            ">98, 57/234, d=0.692, g=0.701\n",
            ">98, 58/234, d=0.699, g=0.709\n",
            ">98, 59/234, d=0.688, g=0.707\n",
            ">98, 60/234, d=0.689, g=0.702\n",
            ">98, 61/234, d=0.691, g=0.692\n",
            ">98, 62/234, d=0.689, g=0.687\n",
            ">98, 63/234, d=0.699, g=0.695\n",
            ">98, 64/234, d=0.690, g=0.699\n",
            ">98, 65/234, d=0.688, g=0.702\n",
            ">98, 66/234, d=0.685, g=0.704\n",
            ">98, 67/234, d=0.691, g=0.693\n",
            ">98, 68/234, d=0.691, g=0.683\n",
            ">98, 69/234, d=0.690, g=0.673\n",
            ">98, 70/234, d=0.696, g=0.691\n",
            ">98, 71/234, d=0.694, g=0.716\n",
            ">98, 72/234, d=0.691, g=0.729\n",
            ">98, 73/234, d=0.697, g=0.700\n",
            ">98, 74/234, d=0.691, g=0.694\n",
            ">98, 75/234, d=0.694, g=0.688\n",
            ">98, 76/234, d=0.692, g=0.695\n",
            ">98, 77/234, d=0.695, g=0.712\n",
            ">98, 78/234, d=0.691, g=0.716\n",
            ">98, 79/234, d=0.693, g=0.689\n",
            ">98, 80/234, d=0.693, g=0.685\n",
            ">98, 81/234, d=0.689, g=0.688\n",
            ">98, 82/234, d=0.691, g=0.697\n",
            ">98, 83/234, d=0.689, g=0.710\n",
            ">98, 84/234, d=0.691, g=0.710\n",
            ">98, 85/234, d=0.689, g=0.697\n",
            ">98, 86/234, d=0.696, g=0.686\n",
            ">98, 87/234, d=0.691, g=0.693\n",
            ">98, 88/234, d=0.690, g=0.704\n",
            ">98, 89/234, d=0.694, g=0.701\n",
            ">98, 90/234, d=0.694, g=0.686\n",
            ">98, 91/234, d=0.690, g=0.686\n",
            ">98, 92/234, d=0.689, g=0.704\n",
            ">98, 93/234, d=0.689, g=0.705\n",
            ">98, 94/234, d=0.698, g=0.699\n",
            ">98, 95/234, d=0.692, g=0.702\n",
            ">98, 96/234, d=0.695, g=0.709\n",
            ">98, 97/234, d=0.691, g=0.703\n",
            ">98, 98/234, d=0.691, g=0.705\n",
            ">98, 99/234, d=0.688, g=0.699\n",
            ">98, 100/234, d=0.689, g=0.688\n",
            ">98, 101/234, d=0.694, g=0.708\n",
            ">98, 102/234, d=0.691, g=0.708\n",
            ">98, 103/234, d=0.700, g=0.691\n",
            ">98, 104/234, d=0.702, g=0.677\n",
            ">98, 105/234, d=0.687, g=0.672\n",
            ">98, 106/234, d=0.701, g=0.707\n",
            ">98, 107/234, d=0.687, g=0.717\n",
            ">98, 108/234, d=0.697, g=0.714\n",
            ">98, 109/234, d=0.684, g=0.696\n",
            ">98, 110/234, d=0.683, g=0.693\n",
            ">98, 111/234, d=0.693, g=0.695\n",
            ">98, 112/234, d=0.697, g=0.701\n",
            ">98, 113/234, d=0.689, g=0.696\n",
            ">98, 114/234, d=0.696, g=0.699\n",
            ">98, 115/234, d=0.691, g=0.698\n",
            ">98, 116/234, d=0.690, g=0.697\n",
            ">98, 117/234, d=0.690, g=0.695\n",
            ">98, 118/234, d=0.701, g=0.706\n",
            ">98, 119/234, d=0.690, g=0.709\n",
            ">98, 120/234, d=0.688, g=0.702\n",
            ">98, 121/234, d=0.688, g=0.694\n",
            ">98, 122/234, d=0.688, g=0.672\n",
            ">98, 123/234, d=0.697, g=0.672\n",
            ">98, 124/234, d=0.693, g=0.685\n",
            ">98, 125/234, d=0.699, g=0.703\n",
            ">98, 126/234, d=0.695, g=0.727\n",
            ">98, 127/234, d=0.685, g=0.737\n",
            ">98, 128/234, d=0.689, g=0.703\n",
            ">98, 129/234, d=0.686, g=0.681\n",
            ">98, 130/234, d=0.693, g=0.669\n",
            ">98, 131/234, d=0.682, g=0.681\n",
            ">98, 132/234, d=0.687, g=0.682\n",
            ">98, 133/234, d=0.696, g=0.695\n",
            ">98, 134/234, d=0.693, g=0.707\n",
            ">98, 135/234, d=0.684, g=0.691\n",
            ">98, 136/234, d=0.685, g=0.697\n",
            ">98, 137/234, d=0.686, g=0.688\n",
            ">98, 138/234, d=0.697, g=0.715\n",
            ">98, 139/234, d=0.694, g=0.719\n",
            ">98, 140/234, d=0.692, g=0.719\n",
            ">98, 141/234, d=0.696, g=0.704\n",
            ">98, 142/234, d=0.686, g=0.685\n",
            ">98, 143/234, d=0.695, g=0.690\n",
            ">98, 144/234, d=0.701, g=0.695\n",
            ">98, 145/234, d=0.696, g=0.708\n",
            ">98, 146/234, d=0.692, g=0.705\n",
            ">98, 147/234, d=0.691, g=0.693\n",
            ">98, 148/234, d=0.691, g=0.694\n",
            ">98, 149/234, d=0.690, g=0.698\n",
            ">98, 150/234, d=0.700, g=0.702\n",
            ">98, 151/234, d=0.690, g=0.706\n",
            ">98, 152/234, d=0.691, g=0.699\n",
            ">98, 153/234, d=0.696, g=0.690\n",
            ">98, 154/234, d=0.689, g=0.699\n",
            ">98, 155/234, d=0.694, g=0.689\n",
            ">98, 156/234, d=0.694, g=0.703\n",
            ">98, 157/234, d=0.693, g=0.695\n",
            ">98, 158/234, d=0.686, g=0.701\n",
            ">98, 159/234, d=0.697, g=0.682\n",
            ">98, 160/234, d=0.693, g=0.694\n",
            ">98, 161/234, d=0.691, g=0.697\n",
            ">98, 162/234, d=0.692, g=0.703\n",
            ">98, 163/234, d=0.690, g=0.710\n",
            ">98, 164/234, d=0.687, g=0.716\n",
            ">98, 165/234, d=0.691, g=0.707\n",
            ">98, 166/234, d=0.691, g=0.692\n",
            ">98, 167/234, d=0.696, g=0.677\n",
            ">98, 168/234, d=0.690, g=0.671\n",
            ">98, 169/234, d=0.695, g=0.677\n",
            ">98, 170/234, d=0.692, g=0.697\n",
            ">98, 171/234, d=0.700, g=0.724\n",
            ">98, 172/234, d=0.692, g=0.726\n",
            ">98, 173/234, d=0.694, g=0.706\n",
            ">98, 174/234, d=0.697, g=0.692\n",
            ">98, 175/234, d=0.686, g=0.688\n",
            ">98, 176/234, d=0.692, g=0.695\n",
            ">98, 177/234, d=0.684, g=0.702\n",
            ">98, 178/234, d=0.691, g=0.695\n",
            ">98, 179/234, d=0.694, g=0.698\n",
            ">98, 180/234, d=0.690, g=0.699\n",
            ">98, 181/234, d=0.693, g=0.699\n",
            ">98, 182/234, d=0.689, g=0.700\n",
            ">98, 183/234, d=0.690, g=0.700\n",
            ">98, 184/234, d=0.692, g=0.692\n",
            ">98, 185/234, d=0.692, g=0.689\n",
            ">98, 186/234, d=0.694, g=0.689\n",
            ">98, 187/234, d=0.686, g=0.691\n",
            ">98, 188/234, d=0.691, g=0.703\n",
            ">98, 189/234, d=0.687, g=0.715\n",
            ">98, 190/234, d=0.692, g=0.719\n",
            ">98, 191/234, d=0.692, g=0.684\n",
            ">98, 192/234, d=0.683, g=0.684\n",
            ">98, 193/234, d=0.697, g=0.700\n",
            ">98, 194/234, d=0.688, g=0.708\n",
            ">98, 195/234, d=0.689, g=0.688\n",
            ">98, 196/234, d=0.685, g=0.682\n",
            ">98, 197/234, d=0.701, g=0.691\n",
            ">98, 198/234, d=0.691, g=0.707\n",
            ">98, 199/234, d=0.694, g=0.699\n",
            ">98, 200/234, d=0.696, g=0.710\n",
            ">98, 201/234, d=0.691, g=0.704\n",
            ">98, 202/234, d=0.690, g=0.708\n",
            ">98, 203/234, d=0.695, g=0.709\n",
            ">98, 204/234, d=0.691, g=0.696\n",
            ">98, 205/234, d=0.687, g=0.691\n",
            ">98, 206/234, d=0.688, g=0.697\n",
            ">98, 207/234, d=0.692, g=0.695\n",
            ">98, 208/234, d=0.689, g=0.716\n",
            ">98, 209/234, d=0.690, g=0.715\n",
            ">98, 210/234, d=0.689, g=0.694\n",
            ">98, 211/234, d=0.693, g=0.686\n",
            ">98, 212/234, d=0.695, g=0.680\n",
            ">98, 213/234, d=0.690, g=0.696\n",
            ">98, 214/234, d=0.690, g=0.712\n",
            ">98, 215/234, d=0.689, g=0.709\n",
            ">98, 216/234, d=0.688, g=0.700\n",
            ">98, 217/234, d=0.694, g=0.689\n",
            ">98, 218/234, d=0.689, g=0.683\n",
            ">98, 219/234, d=0.693, g=0.697\n",
            ">98, 220/234, d=0.691, g=0.701\n",
            ">98, 221/234, d=0.684, g=0.709\n",
            ">98, 222/234, d=0.689, g=0.695\n",
            ">98, 223/234, d=0.688, g=0.693\n",
            ">98, 224/234, d=0.697, g=0.700\n",
            ">98, 225/234, d=0.694, g=0.711\n",
            ">98, 226/234, d=0.690, g=0.709\n",
            ">98, 227/234, d=0.689, g=0.693\n",
            ">98, 228/234, d=0.690, g=0.719\n",
            ">98, 229/234, d=0.689, g=0.717\n",
            ">98, 230/234, d=0.693, g=0.695\n",
            ">98, 231/234, d=0.693, g=0.685\n",
            ">98, 232/234, d=0.695, g=0.719\n",
            ">98, 233/234, d=0.692, g=0.739\n",
            ">98, 234/234, d=0.692, g=0.723\n",
            ">99, 1/234, d=0.690, g=0.699\n",
            ">99, 2/234, d=0.689, g=0.676\n",
            ">99, 3/234, d=0.693, g=0.666\n",
            ">99, 4/234, d=0.684, g=0.672\n",
            ">99, 5/234, d=0.689, g=0.683\n",
            ">99, 6/234, d=0.684, g=0.694\n",
            ">99, 7/234, d=0.687, g=0.703\n",
            ">99, 8/234, d=0.695, g=0.698\n",
            ">99, 9/234, d=0.689, g=0.694\n",
            ">99, 10/234, d=0.685, g=0.697\n",
            ">99, 11/234, d=0.694, g=0.712\n",
            ">99, 12/234, d=0.698, g=0.714\n",
            ">99, 13/234, d=0.691, g=0.723\n",
            ">99, 14/234, d=0.696, g=0.707\n",
            ">99, 15/234, d=0.688, g=0.694\n",
            ">99, 16/234, d=0.690, g=0.692\n",
            ">99, 17/234, d=0.691, g=0.693\n",
            ">99, 18/234, d=0.690, g=0.698\n",
            ">99, 19/234, d=0.697, g=0.707\n",
            ">99, 20/234, d=0.694, g=0.702\n",
            ">99, 21/234, d=0.684, g=0.692\n",
            ">99, 22/234, d=0.691, g=0.697\n",
            ">99, 23/234, d=0.693, g=0.697\n",
            ">99, 24/234, d=0.683, g=0.703\n",
            ">99, 25/234, d=0.691, g=0.697\n",
            ">99, 26/234, d=0.690, g=0.690\n",
            ">99, 27/234, d=0.688, g=0.686\n",
            ">99, 28/234, d=0.690, g=0.701\n",
            ">99, 29/234, d=0.692, g=0.711\n",
            ">99, 30/234, d=0.694, g=0.712\n",
            ">99, 31/234, d=0.685, g=0.702\n",
            ">99, 32/234, d=0.694, g=0.676\n",
            ">99, 33/234, d=0.688, g=0.678\n",
            ">99, 34/234, d=0.703, g=0.698\n",
            ">99, 35/234, d=0.692, g=0.721\n",
            ">99, 36/234, d=0.692, g=0.726\n",
            ">99, 37/234, d=0.690, g=0.697\n",
            ">99, 38/234, d=0.690, g=0.701\n",
            ">99, 39/234, d=0.693, g=0.712\n",
            ">99, 40/234, d=0.701, g=0.704\n",
            ">99, 41/234, d=0.700, g=0.702\n",
            ">99, 42/234, d=0.691, g=0.704\n",
            ">99, 43/234, d=0.700, g=0.716\n",
            ">99, 44/234, d=0.695, g=0.708\n",
            ">99, 45/234, d=0.695, g=0.694\n",
            ">99, 46/234, d=0.695, g=0.676\n",
            ">99, 47/234, d=0.692, g=0.691\n",
            ">99, 48/234, d=0.695, g=0.700\n",
            ">99, 49/234, d=0.694, g=0.708\n",
            ">99, 50/234, d=0.690, g=0.703\n",
            ">99, 51/234, d=0.693, g=0.700\n",
            ">99, 52/234, d=0.694, g=0.691\n",
            ">99, 53/234, d=0.696, g=0.683\n",
            ">99, 54/234, d=0.681, g=0.683\n",
            ">99, 55/234, d=0.694, g=0.682\n",
            ">99, 56/234, d=0.695, g=0.687\n",
            ">99, 57/234, d=0.699, g=0.709\n",
            ">99, 58/234, d=0.697, g=0.711\n",
            ">99, 59/234, d=0.692, g=0.707\n",
            ">99, 60/234, d=0.694, g=0.722\n",
            ">99, 61/234, d=0.691, g=0.705\n",
            ">99, 62/234, d=0.692, g=0.691\n",
            ">99, 63/234, d=0.691, g=0.686\n",
            ">99, 64/234, d=0.688, g=0.704\n",
            ">99, 65/234, d=0.692, g=0.696\n",
            ">99, 66/234, d=0.695, g=0.706\n",
            ">99, 67/234, d=0.696, g=0.720\n",
            ">99, 68/234, d=0.695, g=0.706\n",
            ">99, 69/234, d=0.698, g=0.700\n",
            ">99, 70/234, d=0.696, g=0.690\n",
            ">99, 71/234, d=0.688, g=0.683\n",
            ">99, 72/234, d=0.685, g=0.669\n",
            ">99, 73/234, d=0.692, g=0.690\n",
            ">99, 74/234, d=0.695, g=0.701\n",
            ">99, 75/234, d=0.693, g=0.707\n",
            ">99, 76/234, d=0.696, g=0.693\n",
            ">99, 77/234, d=0.692, g=0.692\n",
            ">99, 78/234, d=0.686, g=0.694\n",
            ">99, 79/234, d=0.693, g=0.709\n",
            ">99, 80/234, d=0.688, g=0.733\n",
            ">99, 81/234, d=0.692, g=0.729\n",
            ">99, 82/234, d=0.693, g=0.698\n",
            ">99, 83/234, d=0.691, g=0.687\n",
            ">99, 84/234, d=0.691, g=0.676\n",
            ">99, 85/234, d=0.688, g=0.688\n",
            ">99, 86/234, d=0.696, g=0.701\n",
            ">99, 87/234, d=0.691, g=0.693\n",
            ">99, 88/234, d=0.690, g=0.696\n",
            ">99, 89/234, d=0.693, g=0.700\n",
            ">99, 90/234, d=0.700, g=0.716\n",
            ">99, 91/234, d=0.693, g=0.727\n",
            ">99, 92/234, d=0.692, g=0.706\n",
            ">99, 93/234, d=0.693, g=0.674\n",
            ">99, 94/234, d=0.691, g=0.662\n",
            ">99, 95/234, d=0.696, g=0.682\n",
            ">99, 96/234, d=0.692, g=0.703\n",
            ">99, 97/234, d=0.694, g=0.713\n",
            ">99, 98/234, d=0.694, g=0.705\n",
            ">99, 99/234, d=0.690, g=0.685\n",
            ">99, 100/234, d=0.691, g=0.686\n",
            ">99, 101/234, d=0.694, g=0.694\n",
            ">99, 102/234, d=0.683, g=0.693\n",
            ">99, 103/234, d=0.691, g=0.713\n",
            ">99, 104/234, d=0.690, g=0.725\n",
            ">99, 105/234, d=0.691, g=0.720\n",
            ">99, 106/234, d=0.689, g=0.705\n",
            ">99, 107/234, d=0.694, g=0.693\n",
            ">99, 108/234, d=0.692, g=0.677\n",
            ">99, 109/234, d=0.692, g=0.679\n",
            ">99, 110/234, d=0.691, g=0.688\n",
            ">99, 111/234, d=0.690, g=0.702\n",
            ">99, 112/234, d=0.687, g=0.705\n",
            ">99, 113/234, d=0.691, g=0.705\n",
            ">99, 114/234, d=0.690, g=0.700\n",
            ">99, 115/234, d=0.686, g=0.698\n",
            ">99, 116/234, d=0.697, g=0.710\n",
            ">99, 117/234, d=0.685, g=0.709\n",
            ">99, 118/234, d=0.688, g=0.712\n",
            ">99, 119/234, d=0.693, g=0.695\n",
            ">99, 120/234, d=0.692, g=0.698\n",
            ">99, 121/234, d=0.687, g=0.697\n",
            ">99, 122/234, d=0.691, g=0.702\n",
            ">99, 123/234, d=0.687, g=0.702\n",
            ">99, 124/234, d=0.697, g=0.711\n",
            ">99, 125/234, d=0.687, g=0.687\n",
            ">99, 126/234, d=0.693, g=0.695\n",
            ">99, 127/234, d=0.689, g=0.715\n",
            ">99, 128/234, d=0.694, g=0.721\n",
            ">99, 129/234, d=0.695, g=0.712\n",
            ">99, 130/234, d=0.689, g=0.699\n",
            ">99, 131/234, d=0.685, g=0.694\n",
            ">99, 132/234, d=0.688, g=0.688\n",
            ">99, 133/234, d=0.690, g=0.691\n",
            ">99, 134/234, d=0.693, g=0.684\n",
            ">99, 135/234, d=0.691, g=0.704\n",
            ">99, 136/234, d=0.698, g=0.716\n",
            ">99, 137/234, d=0.688, g=0.718\n",
            ">99, 138/234, d=0.690, g=0.685\n",
            ">99, 139/234, d=0.691, g=0.673\n",
            ">99, 140/234, d=0.687, g=0.710\n",
            ">99, 141/234, d=0.691, g=0.708\n",
            ">99, 142/234, d=0.695, g=0.696\n",
            ">99, 143/234, d=0.691, g=0.684\n",
            ">99, 144/234, d=0.691, g=0.702\n",
            ">99, 145/234, d=0.679, g=0.715\n",
            ">99, 146/234, d=0.684, g=0.711\n",
            ">99, 147/234, d=0.691, g=0.698\n",
            ">99, 148/234, d=0.695, g=0.678\n",
            ">99, 149/234, d=0.692, g=0.674\n",
            ">99, 150/234, d=0.687, g=0.713\n",
            ">99, 151/234, d=0.689, g=0.738\n",
            ">99, 152/234, d=0.693, g=0.736\n",
            ">99, 153/234, d=0.690, g=0.705\n",
            ">99, 154/234, d=0.692, g=0.679\n",
            ">99, 155/234, d=0.692, g=0.677\n",
            ">99, 156/234, d=0.700, g=0.680\n",
            ">99, 157/234, d=0.698, g=0.704\n",
            ">99, 158/234, d=0.697, g=0.715\n",
            ">99, 159/234, d=0.691, g=0.704\n",
            ">99, 160/234, d=0.693, g=0.687\n",
            ">99, 161/234, d=0.694, g=0.703\n",
            ">99, 162/234, d=0.689, g=0.705\n",
            ">99, 163/234, d=0.691, g=0.715\n",
            ">99, 164/234, d=0.694, g=0.699\n",
            ">99, 165/234, d=0.693, g=0.694\n",
            ">99, 166/234, d=0.695, g=0.700\n",
            ">99, 167/234, d=0.694, g=0.699\n",
            ">99, 168/234, d=0.686, g=0.704\n",
            ">99, 169/234, d=0.688, g=0.705\n",
            ">99, 170/234, d=0.695, g=0.702\n",
            ">99, 171/234, d=0.694, g=0.689\n",
            ">99, 172/234, d=0.694, g=0.690\n",
            ">99, 173/234, d=0.695, g=0.707\n",
            ">99, 174/234, d=0.701, g=0.716\n",
            ">99, 175/234, d=0.694, g=0.704\n",
            ">99, 176/234, d=0.696, g=0.702\n",
            ">99, 177/234, d=0.690, g=0.686\n",
            ">99, 178/234, d=0.692, g=0.699\n",
            ">99, 179/234, d=0.690, g=0.724\n",
            ">99, 180/234, d=0.692, g=0.705\n",
            ">99, 181/234, d=0.692, g=0.703\n",
            ">99, 182/234, d=0.692, g=0.688\n",
            ">99, 183/234, d=0.688, g=0.661\n",
            ">99, 184/234, d=0.692, g=0.671\n",
            ">99, 185/234, d=0.684, g=0.703\n",
            ">99, 186/234, d=0.688, g=0.724\n",
            ">99, 187/234, d=0.687, g=0.748\n",
            ">99, 188/234, d=0.691, g=0.733\n",
            ">99, 189/234, d=0.689, g=0.704\n",
            ">99, 190/234, d=0.695, g=0.687\n",
            ">99, 191/234, d=0.691, g=0.685\n",
            ">99, 192/234, d=0.687, g=0.682\n",
            ">99, 193/234, d=0.689, g=0.684\n",
            ">99, 194/234, d=0.689, g=0.705\n",
            ">99, 195/234, d=0.684, g=0.708\n",
            ">99, 196/234, d=0.696, g=0.698\n",
            ">99, 197/234, d=0.692, g=0.707\n",
            ">99, 198/234, d=0.695, g=0.704\n",
            ">99, 199/234, d=0.691, g=0.705\n",
            ">99, 200/234, d=0.696, g=0.705\n",
            ">99, 201/234, d=0.689, g=0.713\n",
            ">99, 202/234, d=0.689, g=0.698\n",
            ">99, 203/234, d=0.694, g=0.693\n",
            ">99, 204/234, d=0.697, g=0.676\n",
            ">99, 205/234, d=0.694, g=0.676\n",
            ">99, 206/234, d=0.690, g=0.693\n",
            ">99, 207/234, d=0.692, g=0.722\n",
            ">99, 208/234, d=0.694, g=0.726\n",
            ">99, 209/234, d=0.689, g=0.723\n",
            ">99, 210/234, d=0.693, g=0.703\n",
            ">99, 211/234, d=0.694, g=0.688\n",
            ">99, 212/234, d=0.689, g=0.675\n",
            ">99, 213/234, d=0.691, g=0.674\n",
            ">99, 214/234, d=0.688, g=0.683\n",
            ">99, 215/234, d=0.693, g=0.697\n",
            ">99, 216/234, d=0.693, g=0.709\n",
            ">99, 217/234, d=0.699, g=0.701\n",
            ">99, 218/234, d=0.690, g=0.700\n",
            ">99, 219/234, d=0.698, g=0.710\n",
            ">99, 220/234, d=0.694, g=0.717\n",
            ">99, 221/234, d=0.692, g=0.702\n",
            ">99, 222/234, d=0.700, g=0.709\n",
            ">99, 223/234, d=0.695, g=0.700\n",
            ">99, 224/234, d=0.693, g=0.692\n",
            ">99, 225/234, d=0.684, g=0.691\n",
            ">99, 226/234, d=0.682, g=0.683\n",
            ">99, 227/234, d=0.685, g=0.702\n",
            ">99, 228/234, d=0.691, g=0.709\n",
            ">99, 229/234, d=0.688, g=0.723\n",
            ">99, 230/234, d=0.688, g=0.692\n",
            ">99, 231/234, d=0.693, g=0.681\n",
            ">99, 232/234, d=0.695, g=0.687\n",
            ">99, 233/234, d=0.691, g=0.701\n",
            ">99, 234/234, d=0.695, g=0.714\n",
            ">100, 1/234, d=0.696, g=0.720\n",
            ">Accuracy real: 6%, fake: 99%\n",
            ">100, 2/234, d=0.690, g=0.700\n",
            ">Accuracy real: 28%, fake: 96%\n",
            ">100, 3/234, d=0.686, g=0.684\n",
            ">Accuracy real: 37%, fake: 78%\n",
            ">100, 4/234, d=0.693, g=0.688\n",
            ">Accuracy real: 35%, fake: 72%\n",
            ">100, 5/234, d=0.697, g=0.717\n",
            ">Accuracy real: 9%, fake: 98%\n",
            ">100, 6/234, d=0.687, g=0.713\n",
            ">Accuracy real: 11%, fake: 97%\n",
            ">100, 7/234, d=0.692, g=0.706\n",
            ">Accuracy real: 18%, fake: 97%\n",
            ">100, 8/234, d=0.689, g=0.689\n",
            ">Accuracy real: 58%, fake: 67%\n",
            ">100, 9/234, d=0.698, g=0.702\n",
            ">Accuracy real: 31%, fake: 85%\n",
            ">100, 10/234, d=0.696, g=0.717\n",
            ">Accuracy real: 11%, fake: 96%\n",
            ">100, 11/234, d=0.688, g=0.716\n",
            ">Accuracy real: 8%, fake: 99%\n",
            ">100, 12/234, d=0.694, g=0.697\n",
            ">Accuracy real: 36%, fake: 88%\n",
            ">100, 13/234, d=0.693, g=0.683\n",
            ">Accuracy real: 53%, fake: 50%\n",
            ">100, 14/234, d=0.692, g=0.678\n",
            ">Accuracy real: 64%, fake: 40%\n",
            ">100, 15/234, d=0.688, g=0.671\n",
            ">Accuracy real: 67%, fake: 40%\n",
            ">100, 16/234, d=0.691, g=0.682\n",
            ">Accuracy real: 57%, fake: 46%\n",
            ">100, 17/234, d=0.692, g=0.694\n",
            ">Accuracy real: 41%, fake: 71%\n",
            ">100, 18/234, d=0.695, g=0.706\n",
            ">Accuracy real: 11%, fake: 94%\n",
            ">100, 19/234, d=0.697, g=0.710\n",
            ">Accuracy real: 4%, fake: 98%\n",
            ">100, 20/234, d=0.691, g=0.704\n",
            ">Accuracy real: 19%, fake: 94%\n",
            ">100, 21/234, d=0.695, g=0.694\n",
            ">Accuracy real: 33%, fake: 79%\n",
            ">100, 22/234, d=0.694, g=0.701\n",
            ">Accuracy real: 33%, fake: 90%\n",
            ">100, 23/234, d=0.694, g=0.702\n",
            ">Accuracy real: 34%, fake: 94%\n",
            ">100, 24/234, d=0.692, g=0.713\n",
            ">Accuracy real: 5%, fake: 97%\n",
            ">100, 25/234, d=0.684, g=0.710\n",
            ">Accuracy real: 13%, fake: 98%\n",
            ">100, 26/234, d=0.691, g=0.683\n",
            ">Accuracy real: 67%, fake: 45%\n",
            ">100, 27/234, d=0.691, g=0.684\n",
            ">Accuracy real: 59%, fake: 78%\n",
            ">100, 28/234, d=0.690, g=0.708\n",
            ">Accuracy real: 16%, fake: 96%\n",
            ">100, 29/234, d=0.690, g=0.705\n",
            ">Accuracy real: 14%, fake: 93%\n",
            ">100, 30/234, d=0.688, g=0.699\n",
            ">Accuracy real: 38%, fake: 76%\n",
            ">100, 31/234, d=0.692, g=0.686\n",
            ">Accuracy real: 59%, fake: 75%\n",
            ">100, 32/234, d=0.689, g=0.688\n",
            ">Accuracy real: 68%, fake: 54%\n",
            ">100, 33/234, d=0.701, g=0.692\n",
            ">Accuracy real: 48%, fake: 76%\n",
            ">100, 34/234, d=0.691, g=0.713\n",
            ">Accuracy real: 13%, fake: 97%\n",
            ">100, 35/234, d=0.692, g=0.723\n",
            ">Accuracy real: 5%, fake: 99%\n",
            ">100, 36/234, d=0.692, g=0.707\n",
            ">Accuracy real: 18%, fake: 88%\n",
            ">100, 37/234, d=0.689, g=0.690\n",
            ">Accuracy real: 45%, fake: 59%\n",
            ">100, 38/234, d=0.692, g=0.697\n",
            ">Accuracy real: 28%, fake: 81%\n",
            ">100, 39/234, d=0.681, g=0.714\n",
            ">Accuracy real: 7%, fake: 100%\n",
            ">100, 40/234, d=0.693, g=0.713\n",
            ">Accuracy real: 11%, fake: 100%\n",
            ">100, 41/234, d=0.700, g=0.706\n",
            ">Accuracy real: 23%, fake: 99%\n",
            ">100, 42/234, d=0.696, g=0.699\n",
            ">Accuracy real: 49%, fake: 70%\n",
            ">100, 43/234, d=0.696, g=0.691\n",
            ">Accuracy real: 46%, fake: 73%\n",
            ">100, 44/234, d=0.694, g=0.700\n",
            ">Accuracy real: 32%, fake: 86%\n",
            ">100, 45/234, d=0.689, g=0.713\n",
            ">Accuracy real: 9%, fake: 97%\n",
            ">100, 46/234, d=0.687, g=0.694\n",
            ">Accuracy real: 22%, fake: 88%\n",
            ">100, 47/234, d=0.690, g=0.699\n",
            ">Accuracy real: 32%, fake: 93%\n",
            ">100, 48/234, d=0.692, g=0.707\n",
            ">Accuracy real: 29%, fake: 78%\n",
            ">100, 49/234, d=0.689, g=0.688\n",
            ">Accuracy real: 47%, fake: 76%\n",
            ">100, 50/234, d=0.692, g=0.697\n",
            ">Accuracy real: 30%, fake: 86%\n",
            ">100, 51/234, d=0.697, g=0.710\n",
            ">Accuracy real: 10%, fake: 98%\n",
            ">100, 52/234, d=0.692, g=0.715\n",
            ">Accuracy real: 19%, fake: 97%\n",
            ">100, 53/234, d=0.692, g=0.687\n",
            ">Accuracy real: 44%, fake: 72%\n",
            ">100, 54/234, d=0.691, g=0.689\n",
            ">Accuracy real: 45%, fake: 72%\n",
            ">100, 55/234, d=0.697, g=0.686\n",
            ">Accuracy real: 40%, fake: 64%\n",
            ">100, 56/234, d=0.693, g=0.689\n",
            ">Accuracy real: 43%, fake: 76%\n",
            ">100, 57/234, d=0.687, g=0.695\n",
            ">Accuracy real: 53%, fake: 60%\n",
            ">100, 58/234, d=0.692, g=0.678\n",
            ">Accuracy real: 56%, fake: 67%\n",
            ">100, 59/234, d=0.696, g=0.692\n",
            ">Accuracy real: 53%, fake: 81%\n",
            ">100, 60/234, d=0.691, g=0.705\n",
            ">Accuracy real: 6%, fake: 94%\n",
            ">100, 61/234, d=0.689, g=0.729\n",
            ">Accuracy real: 3%, fake: 100%\n",
            ">100, 62/234, d=0.687, g=0.713\n",
            ">Accuracy real: 9%, fake: 97%\n",
            ">100, 63/234, d=0.695, g=0.689\n",
            ">Accuracy real: 37%, fake: 87%\n",
            ">100, 64/234, d=0.686, g=0.694\n",
            ">Accuracy real: 49%, fake: 64%\n",
            ">100, 65/234, d=0.691, g=0.683\n",
            ">Accuracy real: 57%, fake: 61%\n",
            ">100, 66/234, d=0.693, g=0.699\n",
            ">Accuracy real: 33%, fake: 87%\n",
            ">100, 67/234, d=0.693, g=0.702\n",
            ">Accuracy real: 20%, fake: 96%\n",
            ">100, 68/234, d=0.690, g=0.718\n",
            ">Accuracy real: 12%, fake: 99%\n",
            ">100, 69/234, d=0.698, g=0.693\n",
            ">Accuracy real: 32%, fake: 83%\n",
            ">100, 70/234, d=0.688, g=0.676\n",
            ">Accuracy real: 75%, fake: 50%\n",
            ">100, 71/234, d=0.686, g=0.680\n",
            ">Accuracy real: 76%, fake: 52%\n",
            ">100, 72/234, d=0.688, g=0.689\n",
            ">Accuracy real: 49%, fake: 82%\n",
            ">100, 73/234, d=0.684, g=0.705\n",
            ">Accuracy real: 14%, fake: 91%\n",
            ">100, 74/234, d=0.691, g=0.713\n",
            ">Accuracy real: 18%, fake: 95%\n",
            ">100, 75/234, d=0.692, g=0.706\n",
            ">Accuracy real: 9%, fake: 96%\n",
            ">100, 76/234, d=0.684, g=0.706\n",
            ">Accuracy real: 23%, fake: 93%\n",
            ">100, 77/234, d=0.697, g=0.697\n",
            ">Accuracy real: 27%, fake: 86%\n",
            ">100, 78/234, d=0.694, g=0.705\n",
            ">Accuracy real: 12%, fake: 92%\n",
            ">100, 79/234, d=0.690, g=0.717\n",
            ">Accuracy real: 10%, fake: 97%\n",
            ">100, 80/234, d=0.689, g=0.711\n",
            ">Accuracy real: 8%, fake: 95%\n",
            ">100, 81/234, d=0.699, g=0.695\n",
            ">Accuracy real: 39%, fake: 81%\n",
            ">100, 82/234, d=0.689, g=0.690\n",
            ">Accuracy real: 56%, fake: 63%\n",
            ">100, 83/234, d=0.689, g=0.678\n",
            ">Accuracy real: 65%, fake: 50%\n",
            ">100, 84/234, d=0.684, g=0.693\n",
            ">Accuracy real: 53%, fake: 74%\n",
            ">100, 85/234, d=0.690, g=0.696\n",
            ">Accuracy real: 36%, fake: 80%\n",
            ">100, 86/234, d=0.687, g=0.715\n",
            ">Accuracy real: 8%, fake: 97%\n",
            ">100, 87/234, d=0.690, g=0.697\n",
            ">Accuracy real: 29%, fake: 88%\n",
            ">100, 88/234, d=0.695, g=0.689\n",
            ">Accuracy real: 33%, fake: 84%\n",
            ">100, 89/234, d=0.690, g=0.689\n",
            ">Accuracy real: 41%, fake: 68%\n",
            ">100, 90/234, d=0.687, g=0.693\n",
            ">Accuracy real: 35%, fake: 80%\n",
            ">100, 91/234, d=0.688, g=0.698\n",
            ">Accuracy real: 30%, fake: 81%\n",
            ">100, 92/234, d=0.692, g=0.687\n",
            ">Accuracy real: 32%, fake: 78%\n",
            ">100, 93/234, d=0.698, g=0.690\n",
            ">Accuracy real: 41%, fake: 81%\n",
            ">100, 94/234, d=0.694, g=0.691\n",
            ">Accuracy real: 44%, fake: 73%\n",
            ">100, 95/234, d=0.693, g=0.697\n",
            ">Accuracy real: 19%, fake: 85%\n",
            ">100, 96/234, d=0.688, g=0.701\n",
            ">Accuracy real: 24%, fake: 89%\n",
            ">100, 97/234, d=0.689, g=0.709\n",
            ">Accuracy real: 13%, fake: 94%\n",
            ">100, 98/234, d=0.696, g=0.700\n",
            ">Accuracy real: 40%, fake: 81%\n",
            ">100, 99/234, d=0.692, g=0.682\n",
            ">Accuracy real: 54%, fake: 57%\n",
            ">100, 100/234, d=0.692, g=0.687\n",
            ">Accuracy real: 47%, fake: 77%\n",
            ">100, 101/234, d=0.695, g=0.704\n",
            ">Accuracy real: 13%, fake: 93%\n",
            ">100, 102/234, d=0.693, g=0.713\n",
            ">Accuracy real: 10%, fake: 94%\n",
            ">100, 103/234, d=0.692, g=0.694\n",
            ">Accuracy real: 26%, fake: 80%\n",
            ">100, 104/234, d=0.690, g=0.687\n",
            ">Accuracy real: 49%, fake: 76%\n",
            ">100, 105/234, d=0.688, g=0.689\n",
            ">Accuracy real: 46%, fake: 69%\n",
            ">100, 106/234, d=0.689, g=0.688\n",
            ">Accuracy real: 39%, fake: 85%\n",
            ">100, 107/234, d=0.688, g=0.714\n",
            ">Accuracy real: 5%, fake: 97%\n",
            ">100, 108/234, d=0.694, g=0.700\n",
            ">Accuracy real: 21%, fake: 94%\n",
            ">100, 109/234, d=0.697, g=0.692\n",
            ">Accuracy real: 43%, fake: 84%\n",
            ">100, 110/234, d=0.693, g=0.684\n",
            ">Accuracy real: 63%, fake: 67%\n",
            ">100, 111/234, d=0.687, g=0.677\n",
            ">Accuracy real: 69%, fake: 44%\n",
            ">100, 112/234, d=0.689, g=0.694\n",
            ">Accuracy real: 38%, fake: 80%\n",
            ">100, 113/234, d=0.691, g=0.698\n",
            ">Accuracy real: 30%, fake: 78%\n",
            ">100, 114/234, d=0.695, g=0.709\n",
            ">Accuracy real: 11%, fake: 95%\n",
            ">100, 115/234, d=0.693, g=0.725\n",
            ">Accuracy real: 4%, fake: 100%\n",
            ">100, 116/234, d=0.689, g=0.706\n",
            ">Accuracy real: 10%, fake: 94%\n",
            ">100, 117/234, d=0.695, g=0.696\n",
            ">Accuracy real: 28%, fake: 87%\n",
            ">100, 118/234, d=0.692, g=0.670\n",
            ">Accuracy real: 78%, fake: 47%\n",
            ">100, 119/234, d=0.697, g=0.689\n",
            ">Accuracy real: 47%, fake: 60%\n",
            ">100, 120/234, d=0.688, g=0.722\n",
            ">Accuracy real: 5%, fake: 99%\n",
            ">100, 121/234, d=0.691, g=0.718\n",
            ">Accuracy real: 6%, fake: 98%\n",
            ">100, 122/234, d=0.693, g=0.713\n",
            ">Accuracy real: 14%, fake: 99%\n",
            ">100, 123/234, d=0.694, g=0.694\n",
            ">Accuracy real: 37%, fake: 84%\n",
            ">100, 124/234, d=0.695, g=0.697\n",
            ">Accuracy real: 46%, fake: 73%\n",
            ">100, 125/234, d=0.697, g=0.693\n",
            ">Accuracy real: 29%, fake: 85%\n",
            ">100, 126/234, d=0.698, g=0.690\n",
            ">Accuracy real: 26%, fake: 78%\n",
            ">100, 127/234, d=0.695, g=0.711\n",
            ">Accuracy real: 5%, fake: 99%\n",
            ">100, 128/234, d=0.691, g=0.705\n",
            ">Accuracy real: 16%, fake: 92%\n",
            ">100, 129/234, d=0.688, g=0.698\n",
            ">Accuracy real: 50%, fake: 78%\n",
            ">100, 130/234, d=0.694, g=0.692\n",
            ">Accuracy real: 54%, fake: 62%\n",
            ">100, 131/234, d=0.695, g=0.691\n",
            ">Accuracy real: 55%, fake: 67%\n",
            ">100, 132/234, d=0.693, g=0.699\n",
            ">Accuracy real: 21%, fake: 97%\n",
            ">100, 133/234, d=0.699, g=0.710\n",
            ">Accuracy real: 6%, fake: 98%\n",
            ">100, 134/234, d=0.690, g=0.707\n",
            ">Accuracy real: 15%, fake: 95%\n",
            ">100, 135/234, d=0.697, g=0.711\n",
            ">Accuracy real: 16%, fake: 95%\n",
            ">100, 136/234, d=0.691, g=0.706\n",
            ">Accuracy real: 16%, fake: 96%\n",
            ">100, 137/234, d=0.689, g=0.705\n",
            ">Accuracy real: 16%, fake: 97%\n",
            ">100, 138/234, d=0.697, g=0.694\n",
            ">Accuracy real: 32%, fake: 85%\n",
            ">100, 139/234, d=0.686, g=0.687\n",
            ">Accuracy real: 57%, fake: 72%\n",
            ">100, 140/234, d=0.689, g=0.689\n",
            ">Accuracy real: 50%, fake: 55%\n",
            ">100, 141/234, d=0.687, g=0.702\n",
            ">Accuracy real: 22%, fake: 97%\n",
            ">100, 142/234, d=0.688, g=0.718\n",
            ">Accuracy real: 6%, fake: 100%\n",
            ">100, 143/234, d=0.685, g=0.708\n",
            ">Accuracy real: 14%, fake: 95%\n",
            ">100, 144/234, d=0.695, g=0.686\n",
            ">Accuracy real: 63%, fake: 55%\n",
            ">100, 145/234, d=0.695, g=0.682\n",
            ">Accuracy real: 69%, fake: 39%\n",
            ">100, 146/234, d=0.695, g=0.704\n",
            ">Accuracy real: 18%, fake: 92%\n",
            ">100, 147/234, d=0.685, g=0.708\n",
            ">Accuracy real: 12%, fake: 98%\n",
            ">100, 148/234, d=0.692, g=0.699\n",
            ">Accuracy real: 32%, fake: 90%\n",
            ">100, 149/234, d=0.688, g=0.671\n",
            ">Accuracy real: 84%, fake: 28%\n",
            ">100, 150/234, d=0.685, g=0.668\n",
            ">Accuracy real: 88%, fake: 15%\n",
            ">100, 151/234, d=0.694, g=0.695\n",
            ">Accuracy real: 37%, fake: 83%\n",
            ">100, 152/234, d=0.697, g=0.734\n",
            ">Accuracy real: 0%, fake: 100%\n",
            ">100, 153/234, d=0.694, g=0.727\n",
            ">Accuracy real: 3%, fake: 100%\n",
            ">100, 154/234, d=0.687, g=0.705\n",
            ">Accuracy real: 29%, fake: 84%\n",
            ">100, 155/234, d=0.694, g=0.666\n",
            ">Accuracy real: 91%, fake: 16%\n",
            ">100, 156/234, d=0.692, g=0.669\n",
            ">Accuracy real: 83%, fake: 33%\n",
            ">100, 157/234, d=0.689, g=0.686\n",
            ">Accuracy real: 48%, fake: 68%\n",
            ">100, 158/234, d=0.696, g=0.693\n",
            ">Accuracy real: 28%, fake: 85%\n",
            ">100, 159/234, d=0.691, g=0.715\n",
            ">Accuracy real: 12%, fake: 98%\n",
            ">100, 160/234, d=0.688, g=0.722\n",
            ">Accuracy real: 10%, fake: 97%\n",
            ">100, 161/234, d=0.692, g=0.704\n",
            ">Accuracy real: 20%, fake: 85%\n",
            ">100, 162/234, d=0.690, g=0.675\n",
            ">Accuracy real: 74%, fake: 38%\n",
            ">100, 163/234, d=0.693, g=0.687\n",
            ">Accuracy real: 39%, fake: 67%\n",
            ">100, 164/234, d=0.691, g=0.718\n",
            ">Accuracy real: 6%, fake: 100%\n",
            ">100, 165/234, d=0.692, g=0.733\n",
            ">Accuracy real: 2%, fake: 98%\n",
            ">100, 166/234, d=0.695, g=0.707\n",
            ">Accuracy real: 7%, fake: 96%\n",
            ">100, 167/234, d=0.691, g=0.692\n",
            ">Accuracy real: 34%, fake: 80%\n",
            ">100, 168/234, d=0.687, g=0.686\n",
            ">Accuracy real: 55%, fake: 63%\n",
            ">100, 169/234, d=0.692, g=0.691\n",
            ">Accuracy real: 41%, fake: 76%\n",
            ">100, 170/234, d=0.690, g=0.688\n",
            ">Accuracy real: 57%, fake: 61%\n",
            ">100, 171/234, d=0.693, g=0.674\n",
            ">Accuracy real: 54%, fake: 56%\n",
            ">100, 172/234, d=0.691, g=0.701\n",
            ">Accuracy real: 16%, fake: 95%\n",
            ">100, 173/234, d=0.685, g=0.715\n",
            ">Accuracy real: 9%, fake: 99%\n",
            ">100, 174/234, d=0.691, g=0.708\n",
            ">Accuracy real: 16%, fake: 98%\n",
            ">100, 175/234, d=0.688, g=0.694\n",
            ">Accuracy real: 45%, fake: 74%\n",
            ">100, 176/234, d=0.694, g=0.676\n",
            ">Accuracy real: 75%, fake: 33%\n",
            ">100, 177/234, d=0.696, g=0.684\n",
            ">Accuracy real: 45%, fake: 73%\n",
            ">100, 178/234, d=0.694, g=0.708\n",
            ">Accuracy real: 18%, fake: 96%\n",
            ">100, 179/234, d=0.690, g=0.723\n",
            ">Accuracy real: 4%, fake: 100%\n",
            ">100, 180/234, d=0.695, g=0.712\n",
            ">Accuracy real: 11%, fake: 97%\n",
            ">100, 181/234, d=0.700, g=0.677\n",
            ">Accuracy real: 65%, fake: 51%\n",
            ">100, 182/234, d=0.688, g=0.675\n",
            ">Accuracy real: 66%, fake: 44%\n",
            ">100, 183/234, d=0.693, g=0.686\n",
            ">Accuracy real: 44%, fake: 73%\n",
            ">100, 184/234, d=0.696, g=0.711\n",
            ">Accuracy real: 4%, fake: 99%\n",
            ">100, 185/234, d=0.687, g=0.712\n",
            ">Accuracy real: 6%, fake: 98%\n",
            ">100, 186/234, d=0.688, g=0.691\n",
            ">Accuracy real: 25%, fake: 77%\n",
            ">100, 187/234, d=0.698, g=0.703\n",
            ">Accuracy real: 23%, fake: 88%\n",
            ">100, 188/234, d=0.690, g=0.698\n",
            ">Accuracy real: 23%, fake: 86%\n",
            ">100, 189/234, d=0.689, g=0.700\n",
            ">Accuracy real: 18%, fake: 90%\n",
            ">100, 190/234, d=0.691, g=0.712\n",
            ">Accuracy real: 8%, fake: 98%\n",
            ">100, 191/234, d=0.693, g=0.699\n",
            ">Accuracy real: 25%, fake: 94%\n",
            ">100, 192/234, d=0.688, g=0.686\n",
            ">Accuracy real: 59%, fake: 67%\n",
            ">100, 193/234, d=0.694, g=0.685\n",
            ">Accuracy real: 62%, fake: 57%\n",
            ">100, 194/234, d=0.690, g=0.699\n",
            ">Accuracy real: 23%, fake: 88%\n",
            ">100, 195/234, d=0.688, g=0.719\n",
            ">Accuracy real: 8%, fake: 99%\n",
            ">100, 196/234, d=0.690, g=0.715\n",
            ">Accuracy real: 10%, fake: 97%\n",
            ">100, 197/234, d=0.691, g=0.691\n",
            ">Accuracy real: 41%, fake: 72%\n",
            ">100, 198/234, d=0.697, g=0.680\n",
            ">Accuracy real: 61%, fake: 50%\n",
            ">100, 199/234, d=0.690, g=0.689\n",
            ">Accuracy real: 35%, fake: 72%\n",
            ">100, 200/234, d=0.695, g=0.705\n",
            ">Accuracy real: 18%, fake: 97%\n",
            ">100, 201/234, d=0.694, g=0.710\n",
            ">Accuracy real: 16%, fake: 97%\n",
            ">100, 202/234, d=0.696, g=0.682\n",
            ">Accuracy real: 48%, fake: 68%\n",
            ">100, 203/234, d=0.696, g=0.677\n",
            ">Accuracy real: 67%, fake: 48%\n",
            ">100, 204/234, d=0.694, g=0.687\n",
            ">Accuracy real: 27%, fake: 82%\n",
            ">100, 205/234, d=0.690, g=0.714\n",
            ">Accuracy real: 5%, fake: 100%\n",
            ">100, 206/234, d=0.688, g=0.716\n",
            ">Accuracy real: 4%, fake: 99%\n",
            ">100, 207/234, d=0.694, g=0.717\n",
            ">Accuracy real: 3%, fake: 96%\n",
            ">100, 208/234, d=0.693, g=0.699\n",
            ">Accuracy real: 25%, fake: 92%\n",
            ">100, 209/234, d=0.691, g=0.696\n",
            ">Accuracy real: 35%, fake: 85%\n",
            ">100, 210/234, d=0.695, g=0.702\n",
            ">Accuracy real: 25%, fake: 95%\n",
            ">100, 211/234, d=0.692, g=0.692\n",
            ">Accuracy real: 34%, fake: 76%\n",
            ">100, 212/234, d=0.690, g=0.676\n",
            ">Accuracy real: 74%, fake: 42%\n",
            ">100, 213/234, d=0.694, g=0.699\n",
            ">Accuracy real: 29%, fake: 84%\n",
            ">100, 214/234, d=0.691, g=0.726\n",
            ">Accuracy real: 1%, fake: 100%\n",
            ">100, 215/234, d=0.686, g=0.722\n",
            ">Accuracy real: 1%, fake: 99%\n",
            ">100, 216/234, d=0.692, g=0.702\n",
            ">Accuracy real: 29%, fake: 88%\n",
            ">100, 217/234, d=0.686, g=0.666\n",
            ">Accuracy real: 100%, fake: 10%\n",
            ">100, 218/234, d=0.689, g=0.659\n",
            ">Accuracy real: 99%, fake: 5%\n",
            ">100, 219/234, d=0.692, g=0.682\n",
            ">Accuracy real: 55%, fake: 45%\n",
            ">100, 220/234, d=0.687, g=0.716\n",
            ">Accuracy real: 2%, fake: 100%\n",
            ">100, 221/234, d=0.689, g=0.751\n",
            ">Accuracy real: 1%, fake: 100%\n",
            ">100, 222/234, d=0.687, g=0.740\n",
            ">Accuracy real: 2%, fake: 100%\n",
            ">100, 223/234, d=0.691, g=0.694\n",
            ">Accuracy real: 31%, fake: 80%\n",
            ">100, 224/234, d=0.695, g=0.675\n",
            ">Accuracy real: 86%, fake: 22%\n",
            ">100, 225/234, d=0.690, g=0.674\n",
            ">Accuracy real: 69%, fake: 41%\n",
            ">100, 226/234, d=0.690, g=0.712\n",
            ">Accuracy real: 20%, fake: 95%\n",
            ">100, 227/234, d=0.687, g=0.719\n",
            ">Accuracy real: 6%, fake: 100%\n",
            ">100, 228/234, d=0.689, g=0.719\n",
            ">Accuracy real: 4%, fake: 100%\n",
            ">100, 229/234, d=0.693, g=0.711\n",
            ">Accuracy real: 25%, fake: 93%\n",
            ">100, 230/234, d=0.697, g=0.684\n",
            ">Accuracy real: 73%, fake: 52%\n",
            ">100, 231/234, d=0.679, g=0.671\n",
            ">Accuracy real: 83%, fake: 31%\n",
            ">100, 232/234, d=0.692, g=0.685\n",
            ">Accuracy real: 48%, fake: 70%\n",
            ">100, 233/234, d=0.692, g=0.705\n",
            ">Accuracy real: 13%, fake: 98%\n",
            ">100, 234/234, d=0.688, g=0.720\n",
            ">Accuracy real: 9%, fake: 97%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxibainHRrAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'generator_model_100.h5'\n",
        "g_model.save(filename)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPjxye6BRBQI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "c756ddd1-5a6d-4b2c-b4fe-920442a57e11"
      },
      "source": [
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn x_input\n",
        " \n",
        "# create and save a plot of generated images (reversed grayscale)\n",
        "def save_plot(examples, n):\n",
        "\t# plot images\n",
        "\tfor i in range(n * n):\n",
        "\t\t# define subplot\n",
        "\t\tpyplot.subplot(n, n, 1 + i)\n",
        "\t\t# turn off axis\n",
        "\t\tpyplot.axis('off')\n",
        "\t\t# plot raw pixel data\n",
        "\t\tpyplot.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
        "\tpyplot.show()\n",
        " \n",
        "# load model\n",
        "model = load_model('generator_model_100.h5')\n",
        "# generate images\n",
        "latent_points = generate_latent_points(100, 25)\n",
        "# generate images\n",
        "X = model.predict(latent_points)\n",
        "# plot the result\n",
        "save_plot(X, 5)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}